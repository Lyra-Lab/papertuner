{
  "id": "http://arxiv.org/abs/cmp-lg/9507003v2",
  "title": "Robust Processing of Natural Language",
  "authors": [
    "Wolfgang Menzel"
  ],
  "abstract": "Previous approaches to robustness in natural language processing usually\ntreat deviant input by relaxing grammatical constraints whenever a successful\nanalysis cannot be provided by ``normal'' means. This schema implies, that\nerror detection always comes prior to error handling, a behaviour which hardly\ncan compete with its human model, where many erroneous situations are treated\nwithout even noticing them.\n  The paper analyses the necessary preconditions for achieving a higher degree\nof robustness in natural language processing and suggests a quite different\napproach based on a procedure for structural disambiguation. It not only offers\nthe possibility to cope with robustness issues in a more natural way but\neventually might be suited to accommodate quite different aspects of robust\nbehaviour within a single framework.",
  "text": "arXiv:cmp-lg/9507003v2  14 Jul 1995\nRobust Processing of Natural Language\nWolfgang Menzel\nFachbereich Informatik, Universit¨at Hamburg\nVogt-K¨olln-Straße 30, 22527 Hamburg, Germany\nAbstract. Previous approaches to robustness in natural language pro-\ncessing usually treat deviant input by relaxing grammatical constraints\nwhenever a successful analysis cannot be provided by “normal” means.\nThis schema implies, that error detection always comes prior to error\nhandling, a behaviour which hardly can compete with its human model,\nwhere many erroneous situations are treated without even noticing them.\nThe paper analyses the necessary preconditions for achieving a higher\ndegree of robustness in natural language processing and suggests a quite\ndiﬀerent approach based on a procedure for structural disambiguation.\nIt not only oﬀers the possibility to cope with robustness issues in a more\nnatural way but eventually might be suited to accommodate quite dif-\nferent aspects of robust behaviour within a single framework.\n1\nRobustness in Natural Language Processing\nThe notion of robustness in natural language processing is a rather broad one\nand lacks a precise deﬁnition. Usually, it is taken to describe a kind of mono-\ntonic behaviour, which should be guaranteed whenever a system is exposed to\nsome sort of non-standard input data: A comparatively small deviation from a\npredeﬁned ideal should lead to no or only minor disturbances in the system’s\nresponse, whereas a total failure might only be accepted for suﬃciently distorted\ninput.\nUnder this informal notion robustness may well be interpreted as a system’s\nindiﬀerence to a wide range of external disruptive factors including\n– the inherent uncertainty of real world input, e.g. speech or hand writing,\n– noisy environments,\n– the variance between speakers, for instance idiolectal, dialectal or sociolectal,\n– “erroneous” input with respect to some normative standard,\n– an insuﬃcient competence of the processing system, if e.g. exposed to a\nnon-native language or new terminology,\n– highly varying speech rates and\n– resource limitations due to the parallel execution of several mental activities.\nOne of the most impressive features of human language processing is the\nability to retain its basic capabilities even if it is exposed to a combination of\nadverse factors. Technical solutions, on the other hand, are likely to have serious\nproblems if confronted with only a single type of distortion, apart from the\nfundamental diﬃculties to supply the desired monotonic behaviour at all.\nAccordingly, problems of robustness in NLP have almost never been consid-\nered from a unifying perspective so far. A number of very speciﬁc techniques for\nsome of those diﬀerent aspects has been developed, which hardly can be related\nto each other.\nRobustness, for instance, is a key issue in speech recognition, where reliable\nrecognition results for a variety of speakers and speaking conditions are desired.\nTwo basic technologies attempt to support this goal\n– robust stochastic modelling techniques which are able to capture generaliza-\ntions across the individual variety1 and\n– sophisticated search procedures which select among huge amounts of compet-\ning recognition hypotheses by comparing probability estimations for signal\nsegments of increasing length.\nSpecial signal enhancement techniques are used to suppress stationary environ-\nmental noise. There are other aspects of robustness which even have not been\ntreated at all, including the ﬂexible adaptation to external time constraints or\ninternal resource limitations.\nTraditionally the notion of robustness has been strongly connected to the\nprocessing of ill-formed input2, where ill-formedness can be deﬁned both, in\nterms of human standards of grammaticality or in terms of unexpected input.\nMost of the work has been concerned with the problem from a purely syntactic\npoint of view and usually relied on two basic techniques: error anticipation and\nconstraint relaxation.\nError anticipation identiﬁes a number of common mistakes and tries to in-\ntegrate them into the existing grammar by devising dedicated extensions to its\ncoverage. Therefore, the method is limited to a few selected types of deviant\nconstructions which are notorious and therefore predictable, namely\n– stereotypical spelling mistakes (*comittee, *rigth, etc.),\n– performance phenomena in spoken language, like restarts (cf. [6]) and\n– interference-based competence errors in early phases of second language\nlearning (cf. [1]).\nObviously, the complete “innovative” potential and the individual creativity for\nproducing ill-formed input cannot be adequately captured by such means alone.\nOn the other hand, constraint relaxation techniques rely on a systematic vari-\nation of existing grammar rules written for standard input. Initially, the idea\nwas restricted to the stepwise retraction of e.g. agreement conditions in syntac-\ntic rules. It can easily be extended to incorporate arbitrary rule transformations\nin order to allow for the insertion, deletion, substitution and transposition of\n1 The diﬃculties with a straightforward generalization of this approach to e.g. syntactic\nor semantic anomalies are obvious: It would require huge amounts of suﬃciently de-\nviant utterances being available as training data. This renders the approach technically\ninfeasible and cognitively implausible.\nFor similar reasons connectionist approaches are not considered here: At the moment\nthey seem to be limited to approximate solutions for ﬂat representations (cf. [27]).\n2 For a good overview see [25].\nelements. The diﬀerence vanishes completely within modern constraint-based\nformalisms [26] [2], where a transposition of constituents can be interpreted\nequally well as a relaxation of linear precedence constraints. Furthermore, con-\nstraints can be annotated by their degree of vulnerability, hence allowing to\ninclude aspects of error anticipation into the relaxation framework.\nSince both, error anticipation and constraint relaxation considerably enlarge\nthe generative capacity of the original grammar they will lead to spurious am-\nbiguities and serious search problems. This restricts their application to a kind\nof post mortem analysis3. Only if a failure of the standard analysis procedure\nindicates the presence of non-standard input, error rules or relaxation techniques\nare activated to integrate the fragmentary results obtained so far.\nEven a superﬁcial comparison with human processing principles shows the\nfundamental deﬁcit of these approaches. A human reader or listener accepts ill-\nformed input to a wide degree, often without noticing an error at all. This is\nparticularly true if strong expectations concerning the content of the utterance\nare involved or if heavy time constraints restrict the processing depth.\nObviously, there is a fundamental parallelism between robustness issues and\ntime considerations, which syntactically oriented solutions lack so far. Robust-\nness in human language processing does not amount to an additional eﬀort,\nbut instead facilitates both, insensitivity to ill-formed input as well as a ﬂexible\nadaptation to temporal restrictions.\nThis basic pattern is much better modelled by semantically oriented ap-\nproaches based on the slot-and-ﬁller-principle. Here, highly domain speciﬁc ex-\npectations are coded by means of frame-like structures and checked against the\ninput for satisfaction. The schema can be successfully extended to a kind of skim-\nming understanding bringing together the question of robustness against syn-\ntactically ill-formed input and some simple considerations concerning resource\nlimitations.\nThis advantage of a semantically guided analysis, however, is won by the\ncost of excluding another important robustness feature, namely the ability to\ncope with unexpected input (e.g. a change of topic beyond the narrow limita-\ntions of the domain or the violation of selectional restrictions in metaphorical\nexpressions).\n2\nObservations from Human Language Processing\nPsycholinguistic evidence provides a contradictory picture of human language\nprocessing. Some observations clearly support a rather strong modular organi-\nzation with processing units of great autonomy like syntax and semantics [4]\n[5]. On the other hand there is a considerable semantic inﬂuence on the assign-\nment of syntactic structure [20] which suggests a highly integrated processing\narchitecture.\n3 There are exceptions to every rule: For language learning purposes [17] propose an\ninitial analysis based on a moderately weak grammar and followed by a more rigid\nsecond pass.\nRobust behaviour in natural language understanding seems to require both,\n– the autonomy between parallel lines of processing which embodies redun-\ndancy and allows to compensate partial insuﬃciencies and\n– the interactive nature of informational exchange which allows to relate par-\ntial structures on diﬀerent levels of granularity.\nFunctional autonomy undoubtedly is of fundamental importance for robustness.\nIt allows to yield an at least vague interpretation even in cases of extremely\ndistorted input:\n1. A semantically almost empty sentence can be analysed quite well by syn-\ntactic means alone, delivering a hypothetical interpretation in terms of a\npossible world with highly underspeciﬁed referential object descriptions and\npossibly ambiguous thematic roles.\n“... und grausig gutzt der Golz.”[23]\n(1)\n2. Syntactically ill-formed utterances are interpreted based on semantic and\nbackground knowledge even if subcategorization regularities or other gram-\nmatical constraints are violated.\nAlthough both processing units are – at least partially – able to generate some\nuseful interpretation independently of the other one, best results, of course, are\nto be expected if they combine their eﬀorts in a systematic way.\nParallel and autonomous structures in language processing have not only\nevolved between syntactic and semantic aspects of language. They can be ob-\nserved equally well at the level of speech comprehension where auditory (hearing)\nand visual (lip-reading) clues are usually combined to achieve a reliable recogni-\ntion result. Again, both systems – in principle – are able to work independently,\nbut synergy occurs if both are activated concurrently.\nA second group of observations related to the question of robustness concerns\nthe expectation-driven nature of human language understanding. Here, expecta-\ntions come to play at two diﬀerent dimensions:\n– Syntactic, semantic and pragmatic predictions about future input derived\nfrom previous parts of the utterance or dialogue.\n– Expectations exchanged between parallel and autonomous processing struc-\ntures for syntax and semantics.\nThe role of dynamic expectations has mostly been investigated from the view-\npoint of a possible search space reduction in prediction based parsing strategies\n(namely left or head corner algorithms). If used to select between competing\nhypotheses in speech recognition the predictive capacity of a grammar can con-\ntribute additionally to an enhanced robustness of the overall system [10] [7].\nAlthough the importance of predictions for robustness is beyond question,\nhere the second type of expectations shall be examined as a matter of prior-\nity, since they are expected to establish the attempted informational coupling\nbetween parallel processing units. As the simple examples above have shown,\nno predeﬁned direction for this exchange of information can be assumed. Cer-\ntain syntactic constructions may trigger speciﬁc semantic interpretations, a view\nwhich is strongly supported by the traditional perspective on the relation be-\ntween syntax and semantics. In the opposite direction, semantic relations, e.g.\nderived from background knowledge, can not only be used to disambiguate be-\ntween preestablished syntactic readings, but moreover are able to actively pro-\npose suitable syntactic structures. This bidirectionality of interaction seems to\nbe of great importance for the ability to provide the mutual compensation nec-\nessary to treat deviant constructions of diﬀerent kind.\nOf course, the expectation-based nature of natural language processing can-\nnot guarantee a failure-proof performance under all circumstances4. There cer-\ntainly are situations in which strong expectations may override even sensory\ndata. Such a situation can easily be studied in everyday conversation when-\never e.g. pragmatic expectations are predominant. A similar problem occurs in\nexperimental settings using intentionally desynchronised video input, where lip\nreading information sometimes overrides even the auditory stimulus. The prob-\nlem is witnessed as well by the diﬃculties usually encountered in proof-reading\none’s own text: Extremely strong expectations concerning the content usually\ncause minor mistakes to be passed unnoticed.\nTypically, expectations are contradictory and will be of diﬀerent impact on\nthe progress of the analysis procedure. Hence, there is a third principle of ro-\nbust language processing upon which the human model builds. It concerns the\npreference-based selection between both, competing interpretations as well as\ndiﬀerent expectations [12]. Expectations have to be ranked according to their\nparticular strength and weighted against each other.\nRecently linguistic research has shown a remarkable trend towards the de-\nvelopment of integrated models of language structure. One of the more popular\nexamples surely is Head-Driven Phrase Structure Grammar (HPSG [24]), where\nsyntactic and semantic descriptions are uniquely related to each other by coref-\nerential pointers within the framework of typed feature structures. The strong\ncoupling on the level of representation and on the level of processing (i.e. within\nuniﬁcation) completely lacks autonomy. The construction of a logical form is\nalways mediated by syntactic descriptions taken e.g. from subcategorization in-\nformation. Since syntactic and semantic restrictions are conjunctively combined\nthe overall vulnerability against arbitrary impairment of the input utterances\neven increases: An analysis may now fail due to syntactic as well as due to\nsemantic reasons.\nA quite similar conclusion can be drawn for construction grammar [3], an-\nother integrated approach. It combines syntactic, semantic and even pragmatic\ninformation in a single representation named construction. Again, autonomy of\nindividual description levels is missing and even if constructions are supplied\nwith preferential weightings derived from their frequency of use (as realized in\n4 Note that perfect performance is not necessarily covered by the informal notion of\nrobustness introduced earlier.\nSAL [13]) robustness does not increase.\nA clearcut separation of representational levels has actually been realized in\nthe cognitively motivated parser COMPERE [11] [18]. The system aims at mod-\nelling error recovery techniques for garden-path sentences. It uses an arbitration\nmechanism to decide in case of a conﬂict situation which alternative reading\nshould be backed up. This allows to combine early commitment decisions with\nthe possibility to switch to another interpretation if necessary later on. Although\nthe parser is guided in its decisions by diﬀerent kinds of preferences, the map-\nping between syntactic and semantic representations seems to be a strict one.\nAccordingly, it does not provide the necessary means for conﬂict resolution in all\nthose cases of non-standard input for which no interpretation can be established.\nIn particular, three diﬀerent cases can be distinguished\n1. failure on a single level (syntax or semantics)\n2. failure on both levels (syntax and semantics)\n3. no consistent mapping between levels\nWhereas the ﬁrst case might be easily accommodated by the arbitration mech-\nanism the latter two require the abandonment of the strict mapping and its\nreplacement by a preference-based module interaction.\n3\nDisambiguation by Constraint Propagation\nA suitable combination of the three principles discussed above might in fact\nprovide the foundation for an eﬀective use of redundancy in parallel processing\nstructures\n– autonomy guarantees a fall-back behaviour for failures of a single module\n– expectancy-oriented analysis facilitates the informational exchange and\n– preference-based processing guides the analysis towards a promising interpre-\ntation and establishes a loose coupling between modules.\nThese principles, even if taken together, do not explain the almost unconscious\ntreatment of errors in everyday communication. To simulate a similar behaviour\na selective constraint invocation strategy will become necessary. Then, parsing\nis understood as a disambiguation procedure, which activates only speciﬁc parts\nof the grammar, if this is deemed to be unavoidable for solving a particular\ndisambiguation problem. The procedure can be terminated if a suﬃciently reli-\nable disambiguation has been achieved even if certain conditions of the grammar\nhave never been checked so far. Robustness is not introduced by a post mortem\nretraction of constraints but rather by their careful invocation.\nAlong these lines a rudimentary kind of robustness has been achieved in the\nConstraint Grammar framework [15], a system for parsing large amounts of un-\nrestricted text. Constraint Grammar (CG) attempts to establish a dependency\ndescription which is underspeciﬁed with respect to the precise identity of modi-\nﬁees. Initially, it assigns a set of morphologically justiﬁed syntactic labels to each\nword form in the input sentence. Possible labels among others are\n@+FMAINV\nthe ﬁnite verb of a clause\n@SUBJ\na grammatical subject\n@OBJ\na direct object\n@DN>\na determiner modifying a noun to the right\n@NN>\na noun modifying a noun to the right\nThe initial set of labels is successively reduced by applying compatibility\nand surface ordering constraints until a unique interpretation has been reached\nor the set of available constraints is exhausted. In the latter case, a total dis-\nambiguation cannot be achieved by purely syntactic means, as in the following\nattachment example:\nBill\nsaw\nthe\nlittle\ndog\nin\nthe\npark\n@SUBJ\n@+FMAINV\n@DN>\n@AN>\n@OBJ\n@<NOM\n@DN>\n@<P\n@<ADVL\nIn contrast to traditional grammars of the phrase structure type which li-\ncense well-formed structures according to their rule system, constraint grammar\nrather happens to be an eliminative approach5. Instead of imposing a normative\ndescription on the input data it takes them as starting point and tries to ﬁnd a\nplausible interpretation for them.\nThis proceeding is motivated by the ﬁnding that language is an open-ended\nsystem and so grammar formalisms based on a “rigid and idealized conception\nof grammatical correctness are bound to leak” [14, p. 37]. Parsing, if understood\nas a disambiguation procedure, is put down to the principle of parsimony: The\nmore eﬀort is spent the better disambiguation results can be expected and [14,\np. 39] points to the important psycholinguistic parallel:\n“Mental eﬀort is needed for achieving clarity, precision and maximal\ninformation. Less eﬀorts imply (retention of) unclarity and ambiguity,\ni.e. information decrease. In several types of parsers, rule applications\ncreate rather than discard ambiguities: the more processing, the less\nunambiguous information.”\nParsing as disambiguation can well be extended to deal with fully speciﬁed de-\npendency structures without loosing its promising characteristics. A complete\ndisambiguation of structural descriptions has ﬁrst been described for Constraint\nDependency Grammar (CDG [21]) and simply requires to replace the monadic\ncategories of CG by pairs consisting of the relation name and the exactly speci-\nﬁed modiﬁee. The mutual compatibility of modifying relations is checked against\na set of constraints and thus the set of possible modiﬁcations is successively\nreduced by a constraint propagation mechanism. Further extensions of the ap-\nproach concern the inclusion of feature descriptions, valency speciﬁcations and\nvalency saturation conditions [8].\n5 With this respect a strong parallel between the eliminative nature of disambiguation\nand cohort modelling ideas for spoken word recognition [19] becomes visible.\nThough, a closer inspection of the kind of robustness feature introduced by\nthe eliminative mode of operation reveals that its nature is quite accidental so\nfar. Which types of deviation can be tolerated indeed, strongly depends on the\nrather arbitrary sequence of constraint applications. This shortcoming seems to\nbe closely connected to the fact that both formalisms lack the notion of prefer-\nence so far and therefore do not have the possibility to model the “quality” of\na constraint6. Hence, adding a preference-based selection strategy will be one of\nthe most pressing needs for further improvement. Such an extension will be pro-\nposed in section 5. Before we turn to this topic section 4 introduces a modular\nrepresentation schema along the traditional syntax-semantics distinction. It sup-\nports the desired functional autonomy as well as a highly interactive exchange\nof expectations between the two layers.\n4\nRepresentation Layers\nWhereas Constraint Grammar restricts itself to purely syntactic means, an in-\ntegration of simple semantic criteria into Constraint Dependency Grammar has\nbeen proposed recently [9]. It takes into account sortal restrictions only, attach-\ning them to surface syntactic relations without aiming at modularity and au-\ntonomous behavior. In order to facilitate functional independence it will become\nnecessary to establish separate layers for structural description and constraint\npropagation\n– a syntactic layer relating word forms according to functional surface struc-\nture notions e.g (subject-of, dir-object-of, prep-modifier-of, etc.)\nand using constraints on ordering, agreement, valency, and valency satura-\ntion to select among competing structural conﬁguration and\n– a semantic layer building sentence structures by means of thematic roles (like\nagent-of, instrument-of, time-of, etc.) thereby relying upon the argu-\nment structure of semantic predicates and their corresponding selectional\nrestrictions.7\nThe following small and rather rigid sample grammar illustrates the diﬀerent\ntypes of constraints needed:\n1. licensing conditions for modiﬁcation relations\nsy1: cat(dep(X))=N\n→cat(synmod(X))=V ∧synlab(X)∈{SUBJ,OBJ}\n8\nA noun can modify a verb either as a subject or as a direct object.\n6 CG at least includes heuristic constraints, which may be activated at a particular stage\nof the disambiguation procedure\n7 The proposed separation quite closely corresponds with the one chosen in [11]\n8 dep(X) refers to the modiﬁer of a relation, syndom(X) and semdom(X) to its modi-\nﬁees. synlab(X) and semlab(X) are the respective relation names. cat(X), num(X),\nsemprop(X), . . . denote properties of the corresponding node.\n2. agreement conditions\nsy2: synlab(X)=SUBJ →num(dep(X))=num(syndom(X))\nA subject agrees with its modiﬁee with respect to number.\n3. linear ordering constraints\nsy3: synlab(X)=SUBJ →pos(dep(X))<pos(syndom(X))\nThe subject precedes the ﬁnite verb.\n4. compatibility constraints9\nsy4: syndom(X)=syndom(Y) →synlab(X) ̸= synlab(Y)\nA word form\ncannot be modiﬁed twice by the same relation.\nsy1 through sy3 are unary constraints, sy4 is a binary one. Note that constraints\nrefer to modifying relations instead of word forms. Therefore they are able to\nexpress admissibility conditions on local conﬁgurations consisting of up to three\nnodes. Note as well that sy3 – even for German main clauses – has a strong\nheuristic appearance and simply states a preference condition which additionally\nrequires a suitable exception handling mechanism.\nIn a very similar fashion semantic constraints comprise\n1. licensing conditions\nse1: cat(dep(X))=N →cat(semdom(X))=V ∧semlab(X)∈{AG,PAT}\n2. selectional restrictions\nse2: word(semdom(X))=fressen ∧semprop(mod(X))=animal\n→semlab(X)=AG\nAnimals do eat.\nse3: word(semdom(X))=fressen ∧semprop(mod(X))=plant\n→semlab(X)=PAT\nPlants are to be eaten.\n3. compatibility constraints10\nse4: semdom(X)=semdom(Y) →semlab(X)̸=semlab(Y)\nAdhering to the principle of autonomy both layers are designed in a way\nwhich allows them to propagate constraints in a completely independent man-\nner. Each modiﬁer is speciﬁed for two possibly diﬀerent modiﬁees and no cross-\nreference between the layers has been used so far.\nIn order to ﬁnally mediate the interaction between layers, a set of mapping\nconstraints has to be provided which sets up bidirectional correspondences\n9 In fact there is another general compatibility constraint implicitly built into the decision\nprocedure and excluding ambiguous modifying relations from being consistent:\nsygen:\n¬ (syndom(X)=syndom(Y) ↔dep(X)=dep(Y))\n10 Again, supplemented by a general semantic uniqueness constraint\nsegen:\n¬ (semdom(X)=semdom(Y) ↔dep(X)=dep(Y))\nss1: syndom(X)=semdom(X) →(synlab(X)=SUBJ semlab=AG)\nThe subject of a verb is always identical to its agens.\nss2: syndom(X)=semdom(X) →(synlab(X)=OBJ semlab=PAT)\nThe direct object of a verb is always identical to its patiens.\nIt should have become obvious that the selectional restrictions as well as the\nmapping constraints at best can be taken to stand for a preferential interpreta-\ntion. They surely are much to rigid to be sensibly used within a framework of\nstrict reasoning.\nSemantic constraints need not be restricted to linguistically motivated (i.e.\nuniversally valid) ones. In particular, domain-speciﬁc restrictions play a crucial\nrole in semantic disambiguation and should urgently be incorporated whenever\npossible. Here the semantic layer oﬀers a convenient interface to a knowledge\nrepresentation component which (on demand) can contribute constraints from\ne.g. specialized ontologies, referential instantiations or temporal reasoning.\n5\nWeakening Constraints\nSo far, one of the most striking shortcomings has been the strictly binary nature\nof constraint satisfaction. Not surprisingly, it turned out to be most inappro-\npriate within the area of semantic modelling where hardly a constraint can be\nformulated without restricting oneself to a particular, preferential reading.\nIn what follows, preferences are not modelled in the usual direct manner\nby emphasizing particular well-formed interpretations but rather indirectly by\nputting a penalty on all remaining alternatives which violate a constraint. For this\npurpose each constraint gets a penalty factor pf assigned reducing the conﬁdence\nscore in negative cases. Penalty factors may range from zero to one where\npf=0\nspeciﬁes a strict constraint in the classical sense and\n0<pf<1\nindicates a soft constraint accepting contradictory cases\nwith a conﬁdence value proportional to pf\nObviously, a value of one is meaningless because it neutralizes the constraint.\nPenalty factors are combined multiplicatively, i.e. compatibility matrices within\nthe constraint satisfaction problem no longer contain binary categories but con-\nﬁdence scores also ranging from zero (for impossible combinations) up to one\n(for combinations not even violating a single constraint).\nThe indirect treatment of preference by penalty factors oﬀers a consistent\nextension to the basic paradigm of constraint satisfaction. It does not sacriﬁce\nthe eliminative nature of constraints but simply softens it. Inappropriate readings\nare excluded only if they violate strict constraints. In all other cases they are\ndowngraded to a certain degree.\nIn particular, the penalty-based approach helps to tackle some normaliza-\ntion problems otherwise inherently connected with the constraint satisfaction\napproach: Most modifying relations (or combinations of them) will pass a con-\nstraint simply because it is irrelevant for that particular conﬁguration. An in-\ncrease of goodness estimates for these cases would yield a highly undesirable,\nsince unjustiﬁed reinforcement.\nBy assigning the penalty factors pf(sy1)=pf(sy4)=0 to the constraints sy1\nand sy4 from section 3 both are declared to be strict ones, a fact obviously\nbeing valid for the toy-size sample grammar which does not take into account\ncoordinative structures. Using pf(sy2)=0.1 the agreement condition is treated\nas a rather strong one which allows exceptions only occasionally. pf(sy3)=0.3\non the other hand results in a much more permissive constraint justiﬁed by the\nfact that sy2 is meant to exclude ungrammatical utterances but sy3 only to\ndisfavour a marked ordering.\nOn the semantic layer only the licensing constraint se1 is declared as a strict\none. The compatibility constraint se4 is weakened considerably in order to ac-\ncount for double modiﬁcation as in the case of anaphoric reference. The two\nselectional constraints receive penalty factors of diﬀerent strength in order to\nmodel the lower probability of a plant eating something (se2) compared to an\nanimal being eaten (se3):\npf(se1)=0.0\npf(se2)=0.1\npf(se3)=0.7\npf(se4)=0.5\nThe mapping constraints, ﬁnally, are weighted in a way which strongly favours\nthe subject-agens and object-patiens pairings nevertheless allowing alter-\nnative interpretations e.g. in passive sentences.\npf(ss1)=0.2\npf(ss2)=0.3\nAlternative readings can but need not be speciﬁed explicitly. In more realistic\napplications though it is recommended to aim at a considerably richer modelling,\notherwise an unbalanced penalty factor as between se2 and se3 may create a\nsometimes undesired strong bias by default: If both constraints appear to be of\nno relevance the patience reading is clearly prioritized. In the example chosen\nthis corresponds to the acceptable interpretation that an arbitrary thing is more\nlikely to be eaten than to eat.\nAfter having introduced penalty factors as a means of modelling preferences\nconstraint propagation can be extended from the classical case of strictly bi-\nnary decisions to the handling of conﬁdence scores. The application of penalty-\nweighted constraints to a disambiguation problem now consists of two steps:\n1. the calculation of initial conﬁdence scores for all combinations of syntactic\nand semantic modiﬁcation relations and\n2. a selection procedure pruning the search space by sorting out unlikely inter-\npretations\nThe selection procedure is based on a local assessment function heuristically\nidentifying relations to be pruned. In order to not select promising hypothe-\nses assessment ﬁrst of all should only take into account modiﬁcation relations\ncharacterized by the following three criteria\n– being close to the global minimum for all modiﬁcation relations, combined\nwith\n– an as possible as high contrast to alternative relations and\n– a low contrast between all the conﬁdence scores supporting the relation in\nquestion.\nFor experimental purposes a selection procedure based on the sum of quadratic\nerrors for setting scores to zero has been used. Hence, structural interpretations\nviolating a high number of rather strong constraints are pruned ﬁrst.\nUsing the toy grammar speciﬁed above together with its penalty scores the\narbitration process between syntactic and semantic evidence in simple disam-\nbiguation problems can be studied. Thus in a sentence like\nPferde fressen Gras.\n(Horses eat grass.)\n(2a)\nboth layers uniformly support a single interpretation\nPferde\nfressen\nGras\nSUBJ\nOBJ\nAG\nPAT\nDue to the strong semantic support the interpretation remains unchanged if a\nmarked ordering (topicalization of the direct object) is chosen (2b), an agree-\nment error is introduced (2c) and both deviations are combined ﬁnally (2d).\nGrasP AT fressen PferdeAG.\n(2b)\nPferdAG fressen GrasP AT .\n(2c)\nGrasP AT fressen PferdAG.\n(2d)\nThe interpretation is retained even if its semantic support is neutralized as in\nthe following utterance, containing a twofold type shift.\nAutosAG fressen GeldP AT .\n(Cars eat money.)\n(3a)\nGeldP AT fressen AutosAG.\n(3b)\nAutoAG fressen GeldP AT .\n(3c)\nIt switches to the alternative interpretation only in the case of combined syn-\ntactic distortions\nGeldAG fressen AutoP AT .\n(3d)\nEven for the counterintuitive example\nGr¨aserAG fressen PferdP AT .\n(4a)\nwhich, if desired, could be taken as a headline-style utterance, syntactic evidence\nwill gain the upper hand against the violation of two selectional constraints. This\ninterpretation, however, happens to be a rather fragile one and breaks immedi-\nately under arbitrary syntactic variation.\nSince the selection procedure operates on a global assessment of local struc-\ntural conﬁgurations it cannot guarantee to ﬁnd an optimal and globally consis-\ntent interpretation. The partially local mode of operation, on the other hand,\ncan be expected to provide a quite natural explanation for human garden-path\nphenomena. Within the framework of preference-based disambiguation they turn\nout to be a special case of contradictory situations which manifest themselves as\nexpectation violations: The consequences of a pruning decision may not coincide\nwith local conﬁdence estimations elsewhere in the constraint network.\nExpectation violations not necessarily do indicate an erroneous situation.\nThey are frequently used as a speaker’s intentionally chosen means to attract\nthe attention of the audience. This happens for instance by deviating from an\nunmarked ordering to emphasize a topicalized constituent (c.f. (3b)) or by oth-\nerwise producing unexpected utterances.\nOn the other hand there are the typical erroneous situations which in case of\n– internal diﬃculties (e.g. due to early commitment strategies in garden path\nsituations) might oﬀer the possibility to initiate a reanalysis and\n– external reasons (e.g. ill-formed input) can be used to track down the error\nto ﬁnd a possible remedy for it.\nNote that in the latter case the situation coincides with basic observations for\nthe human model: Finding an interpretation for erroneous utterances will be\neasier – in terms of eﬀort to spent – than detecting the error, which in turn will\nbe less demanding than localizing or even correcting it.\nAs with human language processing there will be no predeﬁned direction for\nthe general ﬂow of expectations during arbitration. Whether syntactic evidence\nis propagated from the syntactic to the semantic layer or vice versa depends\nonly on the available information. This seems to be in accordance with recent\npsycholinguistic ﬁndings which contest the existence of purely structural disam-\nbiguation principles [16].\n6\nPreference-based Reasoning\nEliminating implausible interpretations by locally pruning less favoured modi-\nﬁcation relations represents only one, though fundamental method for the dis-\nambiguation of natural language utterances. By selecting among modifying rela-\ntions according to negative evidence from maximally dispreferred hypotheses, the\ntechnique ﬁts quite well into the constraint satisfaction approach and achieves\nits robust behaviour by avoiding extremely risky decisions on a locally topmost\nreading. Taking this as a starting point the basic way of reasoning can well be\ncomplemented by a second propagation principle based on preference-induced\nconstraints. These are activated only in situations where enough positive evi-\ndence can be derived from almost uniquely determined preferences. Since the\nexistence of convincing preferences in realistic disambiguation tasks represents\nrather the exception than the rule the nature of this propagation principle is\nsecondary.\nPreference-induced constraints consist of implications P →p C which, given\nenough evidence for the unary precondition P, require the possibly binary con-\nstraint C to hold. Constraints of this type can be used to model e.g. the higher-\norder conditions which in many mapping situations involve more than two rela-\ntions.\npss1: word(syndom(X))=im ∧synlab(X)=PHEAD\n∧semprop(dep(X))∈{TEMP,LOC}\n→p synlab(Y)=PMOD ∧dep(Y)=syndom(X)\n∧semdom(Z)=sydom(X) →semlab(Z)=PART-OF\nA prepositional phrase headed by the word form “im” ﬁlls a semantic\npart-of slot\nIn postnominal positions like\nDann nehmen wir die erste Woche im Mai.\n(5)\n(Let’s take the ﬁrst week in may.)\nthis constraint puts a preference on the lower attachment since a part-of relation\nusually is not licensed as an argument position for verbs.\nPreference-induced constraints can also be used to modify value assignments\nat certain nodes in the constraint network without the necessity to copy them.\nBy applying this technique, phrasal feature projections can be modelled in order\nto build up descriptions for partial dependency trees\npss2: synlab(X)=DET\n→p case(syndom(X)):=case(syndom(X)) ∩case(dep(X))\nA noun group carries the intersection of possible case features\nfound at its members.\nPreference-induced constraints introduce a kind of inhibitory mechanism to the\ndisambiguation procedure: Already preferred interpretations is given the chance\nto propagate their consequences over the network thus possibly leading to further\nsuppression of alternative readings.\n7\nConclusion\nCombining the eliminative nature of a disambiguation procedure with a system\narchitecture supporting bidirectional arbitration between syntactic and semantic\nevidence has turned out to be a key factor for achieving a higher level of robust-\nness in language understanding. While the disambiguation paradigm provided\nthe basic fall-back behaviour (an arbitrary utterance will get a description as-\nsigned) and the possibility to prune the search space towards a least disfavoured\nreading, the parallel arrangement of modules allows to interactively exchange\nexpectations and thus bypassing local interpretation diﬃculties. By modelling\na preference distribution based on penalty factors, the desired robust behaviour\ncan be demonstrated at least for very simple sample utterances. Although no\nconclusive judgement about the feasibility of the approach can be given until\nthe experimental setting has been scaled up to a fairly realistic problem size,\na remarkable qualitative advance over comparable approaches becomes evident\neven on this elementary level:\n– The approach departs from a predeﬁned sequential arrangement of modules\nin favour of a strictly symmetrical architecture consisting of autonomous\ncomponents for syntax and semantics.\n– It allows to treat syntactic ill-formedness and semantic deviations by pro-\nviding a mechanism for mutual compensation. Syntactically anomalous ut-\nterances can be understood as long as there is enough semantic and/or\npragmatic evidence. In order to communicate novel or unusual content a\nsuﬃciently high degree of syntactic support is required.\n– Insuﬃcient modelling information on any one of the processing layers might\nwell result in the selection of an odd interpretation but will not cause the\nlanguage processing unit to break down entirely.\n– Robustness is not an add-on feature of an otherwise temperamental proce-\ndure but falls out from the basic properties of the processing mechanism.\nSince structural disambiguation by constraint satisfaction likewise lends itself to\nthe creation of time sensitive parsing procedures [22], in the long run it might\nprovide a unifying foundation to build language processing systems upon which\nembody aspects of robustness against such diﬀerent disruptive factors as syntac-\ntically ill-formed input, metaphorical use and dynamic time constraints.\nReferences\n[1] M. E. Catt. Intelligent diagnosis of ungrammaticality in computer-assisted lan-\nguage instruction. Technical Report CSRI-218, Computer Systems Research Insti-\ntute, University of Toronto, 1988.\n[2] G. Erbach. Towards a theory of degrees of grammaticality. Report 34, Universit¨at\ndes Saarlandes, Computerlinguistik, Saarbr¨ucken, 1993.\n[3] C. J. Fillmore, P. Kay, and M. C. O’Connor. Regularity and idiomaticity in gram-\nmatical constructions: The case of let alone. Language, 64:501–538, 1988.\n[4] K. I. Forster. Levels of processing and the structure of the of the language proces-\nsor. In W. E. Cooper and E. C. T. Walker, eds., Sentence Processing: Psycholin-\nguistic studies presented to Merrill Garret, p. 27–85. Lawrence Erlbaum, Hillsdale,\nNJ, 1979.\n[5] L. Frazier.\nTheories of sentence processing.\nIn J. L. Garﬁeld, ed., Modularity\nin Knowledge Representation and Natural Language Understanding, p. 291–307.\nMIT-Press, Cambridge MA, 1987.\n[6] S. Goeser. Chart parsing of robust grammars. In Proc. 14th Int. Conf. on Com-\nputational Linguistics, Coling ’92, p. 120–126, Nancy, France, 1992.\n[7] G. Hanrieder and G. G¨orz. Robust parsing of spoken dialogue using contextual\nknowledge and recognition probabilities. In Proc. of the ESCA Workshop on Spo-\nken Dialogue Systems, Denmark, 1995.\n[8] M. P. Harper and R. A. Helzerman.\nManaging multiple knowledge sources in\nconstraint-based parsing of spoken language. Technical Report EE-94-16, School\nof Electrical Engineering, Purdue University, West Lafayette, 1994.\n[9] M. P. Harper, L. H. Jamieson, C. B. Zoltowski, L. L. McPheters, and R. A. Helz-\nerman. Semantics and constraint parsing. In Proc. of the Int. Conf. on Acoustics,\nSpeech, and Signal Processing, ICASSP-92, p. II–63–66, 1992.\n[10] A. Hauenstein and H. Weber.\nAn investigation of tightly coupled time syn-\nchronous speech language interfaces using a uniﬁcation grammar. Verbmobil Re-\nport 9, 1994.\n[11] J. K. Holbrook, K. P. Eiselt, and K. Mahesh. A uniﬁed process model of syntactic\nand semantic error recovery in sentence understanding. In Proc. of the 14th Annual\nConf. of the Cognitive Science Society, p. 195–200, Bloomington, IN, 1992.\n[12] R. S. Jackendoﬀ. Semantics and Cognition. MIT Press, Cambridge, 1983.\n[13] D. Jurafsky. A cognitive model of sentence interpretation: The construction gram-\nmar approach. Technical Report TR-93-077, International Computer Science In-\nstitute, Berkeley, 1993.\n[14] F. Karlsson. Designing a parser for unrestricted text. In F. Karlsson, A. Vouti-\nlainen, J. Heikkil¨a, and A. Anttila, eds., Constraint Grammar – A Language-\nIndependent System for Parsing Unrestricted Text, p. 1–40. Mouton de Gruyter,\nBerlin, New York, 1995.\n[15] F. Karlsson, A. Voutilainen, J. Heikkil¨a, and A. Anttila, eds. Constraint Gram-\nmar – A Language-Independent System for Parsing Unrestricted Text. Mouton de\nGruyter, Berlin, New York, 1995.\n[16] L. Konieczny, B. Hemforth, and N. Voelker. The impact of context and semantic\nbias on constituent attachment in reading. In J. J. Quantz and B. Schmitz, eds.,\nAmbiguity and Strategies of Disambiguation, p. 105–126. KIT-Report 120, Berlin,\n1994.\n[17] I. Kudo, H. Koshino, M. Chung, and T. Morimoto.\nSchema method: A frame-\nwork for correcting grammatically ill-formed input. In Proc. 12th Int. Conf. on\nComputational Linguistics, Coling ’88, p. 341–347, Budapest, 1988.\n[18] K. Mahesh and K. P. Eiselt. Uniform representations for syntax-semantic arbi-\ntration. In Proc. of the 16th Annual Conf. of the Cognitive Science Society, p.\n589–594, 1994.\n[19] W. Marslen-Wilson. Functional parallelism in spoken word-recognition. Cogni-\ntion, 25:71–102, 1987.\n[20] W. Marslen-Wilson and L. K. Tyler. Against modularity. In J. L. Garﬁeld, ed.,\nModularity in Knowledge Representation and Natural Language Understanding, p.\n37–62. MIT-Press, Cambridge, MA, 1987.\n[21] H. Maruyama. Structural disambiguation with constraint propagation. In Proc.\n28th Annual Meeting of the ACL, p. 31–38, 1990.\n[22] W. Menzel. Parsing of spoken language under time constraints. In T. Cohn, ed.,\nProc. 11th Europ. Conf. on Artiﬁcial Intelligence, p. 560–564, Amsterdam, 1994.\n[23] C. Morgenstern. Alle Galgenlieder. Cassirer, Berlin, 1933.\n[24] C. Pollard and I. A. Sag. Head-Driven Phrase Structure Grammar. The Univer-\nsity of Chicago Press, Chicago, 1994.\n[25] M. Stede. The search for robustness in natural language understanding. Artiﬁcial\nIntelligence Review, 6(4):383–414, 1992.\n[26] H. Uszkoreit. Strategies for adding control information to declarative grammars.\nComputerlinguistik Report 10, Universit¨at des Saarlandes, Saarbr¨ucken, 1991.\n[27] S. Wermter and V. Weber.\nLearning Fault-tolerant Speech Parsing with\nSCREEN. In Proc. of the 12th Nat. Conf. on Artiﬁcial Intelligence, p. 670–675,\nSeattle, 1994.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1995-07-13",
  "updated": "1995-07-14"
}