{
  "id": "http://arxiv.org/abs/cmp-lg/9603005v1",
  "title": "Integrated speech and morphological processing in a connectionist continuous speech understanding for Korean",
  "authors": [
    "Geunbae Lee",
    "Jong-Hyeok Lee"
  ],
  "abstract": "A new tightly coupled speech and natural language integration model is\npresented for a TDNN-based continuous possibly large vocabulary speech\nrecognition system for Korean. Unlike popular n-best techniques developed for\nintegrating mainly HMM-based speech recognition and natural language processing\nin a {\\em word level}, which is obviously inadequate for morphologically\ncomplex agglutinative languages, our model constructs a spoken language system\nbased on a {\\em morpheme-level} speech and language integration. With this\nintegration scheme, the spoken Korean processing engine (SKOPE) is designed and\nimplemented using a TDNN-based diphone recognition module integrated with a\nViterbi-based lexical decoding and symbolic phonological/morphological\nco-analysis. Our experiment results show that the speaker-dependent continuous\n{\\em eojeol} (Korean word) recognition and integrated morphological analysis\ncan be achieved with over 80.6% success rate directly from speech inputs for\nthe middle-level vocabularies.",
  "text": "arXiv:cmp-lg/9603005v1  18 Mar 1996\nIntegrated speech and morphological processing in a connectionist\ncontinuous speech understanding for Korean\nGeunbae Lee\nJong-Hyeok Lee\nDepartment of Computer Science & Engineering\nPohang University of Science & Technology\nSan 31, Hoja-Dong, Pohang, 790-784, Korea\nPhone: +82-562-279-2254, Fax: +82-562-279-2299\ngblee@vision.postech.ac.kr, jhlee@vision.postech.ac.kr\nAbstract\nA new tightly coupled speech and natural language integration model is presented for a\nTDNN-based continuous possibly large vocabulary speech recognition system for Korean. Un-\nlike popular n-best techniques developed for integrating mainly HMM-based speech recognition\nand natural language processing in a word level, which is obviously inadequate for morpholog-\nically complex agglutinative languages, our model constructs a spoken language system based\non a morpheme-level speech and language integration. With this integration scheme, the spoken\nKorean processing engine (SKOPE) is designed and implemented using a TDNN-based diphone\nrecognition module integrated with a Viterbi-based lexical decoding and symbolic phonologi-\ncal/morphological co-analysis. Our experiment results show that the speaker-dependent contin-\nuous eojeol (Korean word) recognition and integrated morphological analysis can be achieved\nwith over 80.6% success rate directly from speech inputs for the middle-level vocabularies.\nKeywords: speech and natural language integration, spoken language processing,\nmorphological analysis, phonological modeling, Viterbi search, time-delayed neural\nnetworks\n1\nIntroduction\nA spoken natural language system requires many diﬀerent levels of knowledge sources includ-\ning acoustic-phonetic, phonological, morphological, syntactic, semantic and even pragmatic levels.\nThese knowledge sources are grouped and processed by either speech processing models or sta-\ntistical/symbolic natural language processing models. Since the speech and the natural language\ncommunities have conducted almost independent researches, these models were not completely inte-\ngrated and often biased by neglecting either acoustic-phonetic or high-level linguistic information.\nCurrent speech and natural language integration mainly relies on word-level n-best search tech-\nniques [1, 2] as shown in ﬁgure 1. For HMM-based speech recognition systems, the n-best search\ntechniques have been successfully applied to the integration of speech and natural language process-\ning. However, current implementations of n-best techniques only support the integration at a word\nlevel by directly producing the n-best list of candidate sentences, and this type of loose coupling is\n1\nspeech\nsignals\nspeech\nrecognition\nsystem\ntop n-best\nsentences\nNLP\nsystem\nstructures\nphonetic\ndictionary\nNLP\ndictionary\nsyntactic/semantic\nFigure 1: N-best lists: current speech and natural language integration method\nonly suitable for the integration of existing speech and natural language systems, such as, e.g. [3, 4].\nThe n-best search is viable only for short sentences since the necessary n grows exponentially with\nthe sentence length. Because the n-best search directly generates word sequences, phonetic and\nnatural language dictionaries must have full word entries, which is obviously inadequate for mor-\nphologically complex agglutinative languages such as Korean. The dictionary size will grow very\nfast for full word entries because new words can be almost freely generated by concatenating the\nconstituent morphemes in these languages (e.g. noun plus noun-endings or verb plus verb-endings).\nIn this paper, we present a new morphologically conditioned integration architecture of speech\nand natural language processing for morphologically complex agglutinative languages.\nThe in-\ntegration is based on a Viterbi-based lexical decoding and symbolic phonological/morphological\nco-analysis.\nThe Viterbi search [5] is performed on diphone (explained in section 4) sequences\ngenerated from a TDNN (time-delay neural network)-based Korean speech recognizer [6], and the\nsearch process is tightly integrated with a morphological and phonological constraint checking.\nWe present a new integration architecture, not for popular HMM-based systems, but for recently\ndeveloped connectionist speech recognition systems. Connectionist speech recognition has several\nadvantages over the classical statistical speech processing [7]. Especially, the TDNN model [8] has\nbeen widely used to model the time shift invariance of speech signals. In this regard, we will present\na morpheme-level integration method for a TDNN-based continuous speech recognition model for\nKorean. This paper is organized as follows. Section 2 brieﬂy explains the characteristics of spo-\nken Korean for general readers. Section 3 introduces our speech and natural language integration\narchitecture, and section 4 and section 5 more elaborate the introduced integration architecture.\nSection 6 shows several experiment results to demonstrate the performance, and section 7 compares\nour integration scheme with similar related researches. Section 8 draws some conclusions.\n2\nFeatures of spoken Korean\nThis section brieﬂy explains the linguistic characterists of spoken Korean before describing the\nintegration architecture.\nIn this paper, Yale romanization is used for representing the Korean\nphonemes. 1) A Korean word, called eojeol, consists of more than one morphemes with clear-cut\nmorpheme boundaries (Korean is an agglutinative language). 2) Korean is a postpositional lan-\nguage with many kinds of noun-endings, verb-endings, and preﬁnal verb-endings. These functional\n2\nmorphemes determine the noun’s case roles, verb’s tenses, modals, and modiﬁcation relations be-\ntween eojeols. 3) Korean is a basically SOV language but has relatively free word order compared\nto the rigid word-order languages, such as English, except for the constraints that the verb must\nappear in a sentence-ﬁnal position. However, in Korean, some word-order constraints do exist such\nthat the auxiliary verbs representing modalities must follow the main verb, and the modiﬁers must\nbe placed before the word (called head) they modify. 4) The unit of pause in speech (which is\ncalled eonjeol) may be diﬀerent from that of a written text (an eojeol). The spoken morphological\nanalysis must deal with an eonjeol (fragment of sentence) since no eojeol boundary is provided in\nthe speech. 5) Phonological changes can occur in a morpheme, between morphemes in an eojeol,\nand even between eojeols in an eonjeol. These changes include consonant and vowel assimilation,\ndissimilation, insertion, deletion, and contraction, and so on.\n3\nSKOPE system architecture for morpheme-level integration\nThe morpheme-level integration technique processes phoneme-like unit (PLU) sequences (speech\nrecognizer’s outputs) using both Viterbi-based lexical decoding (for morpheme) and symbolic\nphonological/morphological co-analysis, and uses a single uniﬁed phonetic-morpheme (UPM) dic-\ntionary for both speech and language processing. This morpheme-level integration scheme is able to\nutilize natural language morphological processing techniques in an early stage of spoken language\nprocessing compared with the classical approaches of word-level speech and language integration.\nThe morpheme-level integration also renders a phonological rule modeling possible in the early\nstage. The phonological/morphological analysis can be performed together using the single UPM\ndictionary, and the dictionary size becomes stable regardless of the vocabulary size because only\nthe morphemes are encoded and the new words can be processed by using the existing morphemes\nin the dictionary.\nFigure 2 shows the SKOPE architecture, a morpheme-level integration model of speech and\nnatural language processing for Korean. The speech signal is analyzed using the TDNN diphone\nrecognizer. The diphone recognizer is composed of a hierarchy of TDNN networks. The recognized\ndiphone sequences are decoded using the Viterbi search on the trie-structured UPM dictionary to\nsegment out the target morpheme candidates. In the UPM dictionary, each morpheme’s phonetic\nheader is a HMM (hidden markov model) network using the diphone symbols. The Viterbi decoded\ncandidate morphemes are stored in a triangular table to be properly connected during the mor-\nphological processing. From the candidate morphemes, the Viterbi-based morphological analyzer\nproduces the morphologically analyzed eojeols by handling morphotactics veriﬁcation and irregular\nconjugations. The phonological modeling is tightly integrated into the morphological processing\nthrough a declarative phonological rule modeling in the UPM dictionary. Outputs of the integrated\narchitecture, that is, analyzed eojeol sequences, can be directly fed to the upper level syntax and\nsemantics analysis modules which are described in [9].\n4\nDiphone-based speech recognition\nFor large-vocabulary continuous speech recognition, a sub-word level recognition is usually per-\nformed. We select a group of diphones for our phoneme-like units (PLUs) because direct phoneme\nrecognition in Korean is very diﬃcult. The 46 Korean phonemes are very similar each other espe-\n3\nspeech signal\nTDNN\ndiphone recognizer\ndiphone sequences\nViterbi\ncandidate morphemes\nsequences\nsyntax \nprocessing \n.........\nsyntactic\nstructures\nanalyzed eojeol\nUPM dictionary\n(segmentation)\nlexical decoding\nmorphological\nphonological\nprocessing\nFigure 2: SKOPE speech and language morpheme-level integration architecture. Syntax and more\nhigh-level processing steps are not in the scope of this paper.\ndiphone \ndiphone\nnumbers\ndiphone \nexamples\nV                   21                        a, o, wu, i, u, ye, .....\nC1V              378                       ha, sa, ka, la, ma, kha, ......\nVC2              147                       an, am, eng, em, wun, in, ......\ntypes\nC2C1             72                         ngs, nn, ngt, ngh, ....\nFigure 3: Korean diphone groups (V: vowel, C1: syllable-ﬁrst consonant, C2: syllable-ﬁnal conso-\nnant). In C2C1 type, the C2 must be one of the nasals or liquids/glides which are similar to vowels.\nYale romanization is used to specify the diphone symbols.\ncially in the following cases: 1) the Korean diphthongs are hard to distinguish from the mono-vowels,\nand 2) the syllable-ﬁnal consonants are hard to diﬀerentiate from the syllable-ﬁrst consonants. The\nselected diphone groups (ﬁgure 3) have more suitable features for co-articulation modeling than\nthe phonemes and are much fewer in numbers than the popular triphones [10]. We also introduced\nCC-type (syllable-ﬁnal consonant, syllable-ﬁrst consonant) diphones for smooth transition model-\ning between syllables in Korean. Figure 4 shows the hierarchical structure of a group of TDNNs for\ndiphone recognition, and also shows the architecture of each component TDNN. The whole diphone\nrecognizer consists of total 19 diﬀerent TDNNs for recognition of the deﬁned Korean diphones. We\nre-classiﬁed the total diphones into 18 diﬀerent groups according to the vowel characteristics in the\ndiphones. The top-level TDNN (vowel group TDNN) identiﬁes the 18 vowel groups of the diphones\nusing relatively low frequency signal vectors (under 4 KHz). Each 18 diﬀerent sub-group TDNN\nrecognizes the target diphones using the whole frequency signal vectors. For the training of each\nTDNN, we manually segmented the digitized speech signals into 200 msec range (which includes\nroughly left-context phoneme, target diphone, and right context phoneme), and applied 512 order\nFFTs and 16 step mel-scaling [8] to get the ﬁlter-bank coeﬃcients. Each frame size is 10 msec,\nso 20 (frames) by 16 (mel-scaling factor) values are fed to the TDNNs with the proper output\nsymbols, that is, the vowel group name or the target diphone name. After the training of each\nTDNN, the diphone recognition is performed by feeding 200 msec signals to the vowel group TDNN\nand subsequently to the proper sub-group TDNNs according to the extracted vowel group. The\n200 msec signals are shifted by 30 msec steps and continuously fed to the networks to process the\n4\n16\n33\n18\n1\na e o ey i ......................cc \na an ha sa ka la ma ....  am \n1\n16\n33\n18\n18\n13\n9\n18\n20 frames\n20 frames\n18\n9\nvowel group TDNN\nsub-group TDNNs\na               e              o            ey                i\nya                ye                 yo            yu          oy                wu\nwa              we            yey          uy             u                wi                CC \nFigure 4: Top: hierarchical organization of the group of TDNNs for entire diphone recognition.\nBottom left: TDNN architecture for vowel group identiﬁcation. Note the cc group contains no\nvowels. Bottom right: Architecture of the sub-TDNN for /a/ vowel group recognition. The other\n17 sub-TDNNs have the same architecture, but diﬀerent number of output units according to the\nnumber of diphones in each of the vowel group.\n5\nphonetic\ntranscription\nheader\nno-change\n‘wu‘ sound\nno-change\nverb-ending\nadnominalizing\nverb-ending\n‘l‘ sound\nno-change\n‘l‘ sound\nno-change\n‘wu‘ sound\nno-change\nadnominalizing\nl                               l \nchange to ‘ss‘ \nsswu                  swu                     bound-noun          bound-noun       ‘s’ sound\noriginal\nmorpheme\nleft\nconnectivity\nconnectivity\nright\nmorphological\nmorphological\nci-wu                 ci-wu                regular verb          regular verb          ‘c‘ sound\nconnectivity\nleft\nconnectivity\nright\nphonemic\nphonemic\nFigure 5: The uniﬁed phonetic-morpheme (UPM) dictionary for entries ci-wu (delete), l (adnomi-\nnalizing verb-ending), and swu (bound-noun).\ncontinuous speech in an eonjeol (pause unit of Korean speech). The ﬁnal outputs are sequence of\ndiphones for each 200 msec range in 30 msec intervals. The hierarchical TDNN structure shortens\nthe training time and provides easily extensible system design. The entire recognition rate critically\ndepends on the vowel group TDNN in this hierarchical structure.\n5\nViterbi-based morphological analysis\nUnlike conventional morphological analyses for text inputs, our morphological analysis starts with\nthe recognized diphone sequences which contain insertion, deletion, and substitution speech recogni-\ntion errors. The conventional morphological analysis procedure [11], i.e., morpheme segmentation,\nmorphotactics modeling, and orthographic rule (or phonological rule) modeling, must be augmented\nand extended to cope with the recognition errors as follows: 1) The conventional morpheme segmen-\ntation is extended to deal with the speech recognition errors and between-morpheme phonological\nchanges as well as irregular conjugations during the segmentation, 2) the morphotactics modeling\nis extended to cope with the complex verb-endings and noun-endings in Korean, and 3) the ortho-\ngraphic rule modeling is combined with the phonological rule modeling to correctly transform the\ndiphone transcriptions (phonetic spelling) into the orthographically spelled morpheme sequences.\nThe central part of the morphological analysis lies in the dictionary construction. In our UPM\n(uniﬁed phonetic-morpheme) dictionary, each phonetic transcription of single morpheme has a sep-\narate dictionary entry. Figure 5 shows the UPM dictionary both for speech and language processing\nwith three diﬀerent morpheme entries ci-wu, l, swu. The extended morphological analysis is based\non the well-known tabular parsing technique for context-free languages [12] and augmented to han-\ndle the Korean phonological rules and speech recognition errors in the diphone sequence inputs.\nFigure 6 shows the extended table-driven morphological analysis process. The example diphone se-\nquence was obtained from the input speech ci-wul-sswu (meaning: can/cannot be removed), and the\nmorphological analysis produces ci-wu+l+swu (remove+ADNOMINAL+BOUND-NOUN), where\n’+’ is the morpheme boundary, and ’-’ is the syllable boundary. The morpheme segmentation is\nbasically performed using the Viterbi-based lexical decoding to recover the possible errors in the\ndiphone sequences. For Viterbi search, the phonetic transcription headers for each morpheme in\nthe UPM dictionary are converted into diphone transcription headers, and each converted header\n6\nciwu+ l + swu\nci ci ci i i i wu wu wu wul wul wul\n1       2        3        4         5            6 \n1\n2\n3\n4\n5\n6\nstarting\nposition\nending position\nciwu+l+swu\n ciwu\nl\nciwu+l\nswu\nci\nwul\nls ls ls sswu sswu sswu wu wu wu \nFigure 6: Morphological analysis of diphone sequences. From top: output morpheme sequence in\nan eonjeol, triangular parsing table, and input diphone sequence.\nis turned into a simple HMM. The converted HMMs are organized into a trie data structure for\neﬃcient search (see ﬁgure 7), and form a trie-structured diphone-based HMM index. The HMMs\nare the simplest ones which have only left-to-right and self transitions. Additional diphone nodes\n(marked with thick circles) are inserted for smooth inter-morpheme co-articulation modeling. The\ntransition probability in each HMM is deﬁned:\naij =\n\n\n\n\n\nα\ni = j\n1−α\nN\ni ̸= j ∧dt = si ∧dt+1 = sj\n0\notherwise\nwhere aij is a transition probability from state i to state j, N is the number of all possible transitions\nfrom state i. dt is a diphone observable at time t, and si is a diphone at state i. This model\nassigns self-transition probability α and left-to-right transition probability 1−α\nN . All other transition\nprobabilities are zeros. In each state, the diphone emission probabilities are deﬁned:\nbi(k) =\n(\nβ\ndk = si\n1−β\nM\notherwise\nwhere bi(k) is a probability of producing diphone dk at state i, and M is the number of all the\ndiphones in the model. We adjust α and β experimentally, and the ﬂexible adjustment helps to cope\nwith the insertion and deletion errors in the diphone sequences. The Viterbi search with the trie-\nstructured HMM index on the input diphone sequences segments out all the possible morphemes\nin the given diphone sequence, and enrolls all the segmented morphemes into the triangular table\non the proper positions. For example, in ﬁgure 6, morphemes such as ci (carry), ci-wu (delete),\n7\nci\ni\nwu\ni\nin\nnu\nu\nun\nstart\nl\nwul\nls\nwui\nnn\nsswu\nwu\nFigure 7: Trie-structured diphone-based HMM index for morphemes ciwu, l, swu, iss, nun. In\neach node, if a path from the root (start node) completes a morpheme, a pointer leads to the\ncorresponding morpheme entry in the UPM dictionary. The self-transition for each node is left out\nexcept the root for ﬁgure simplicity.\nl (adnominal verb-ending), wul (cry), swu (bound-noun) are segmented out and enrolled in the\ntable position (1,2), (1,3), (4,4), (3,4), (5,6). The position (i,j) designates the starting and ending\nposition of each morpheme in the given input eonjeol.\nThe morphotactics modeling is necessary after all the morphemes are enrolled in the table in\norder to combine only legal morphemes into an eojeol (Korean word), and the process is called\nmorpheme-connectivity-checking. Since Korean has well developed postpositions (noun-endings,\nverb-endings, preﬁnal verb-endings) which play as grammatical functional morphemes, we must\nassign each morpheme proper part-of-speech (POS) tags for the eﬃcient connectivity checking.\nOur more than 400 POS tags which are reﬁned from the 13 major Korean lexical categories are\nhierarchically organized, and contained in the UPM dictionary (in the name of left and right\nmorphological connectivity, see ﬁgure 5). In the case of idiomatic expressions, we place such idioms\ndirectly in the dictionary for eﬃciency, where two diﬀerent POS tags are necessary for the left and\nthe right morphological connectivity. For single morpheme, the left and the right POS tags are\nalways the same. The separate morpheme-connectivity-matrix (sometimes, it is called morpheme-\nadjacency-matrix) indicates the legal morpheme combinations using the POS tags deﬁned in the\ndictionary. So the morphotactics modeling is performed by utilizing two essential components: the\nPOS tags (in the dictionary) and the morpheme-connectivity-matrix. For example, in ﬁgure 6, the\nmorpheme ciwu (in position (1,3)) can be legally combined with the morpheme l (in position (4,4))\nto make ciwu+l (delete+ADNOMINAL, in position (1,4)) but ci cannot be combined with wul to\nmake ci+wul even if they are in the combinable positions.\nThe orthographic rule modeling must be integrated with the phonological rule modeling in\nspoken Korean processing. Since we must deal with the erroneous speech inputs, the conventional\nrule-based modeling requires so many number of rule applications [13]. So our solution is based on\nthe declarative modeling of both orthographic and phonological rules in a uniform way. That is, in\nour UPM dictionary, the conjugated verb forms as well as the original verb forms are all enrolled,\nand the same morphological connectivity information is applied for both original verb forms as\nwell as the conjugated ones. The phonological rule modeling is also accomplished declaratively\nby having the separate phonemic connectivity information in the dictionary (see ﬁgure 5). The\nphonemic connectivity information for each morpheme declares the possible phonemic changes\nin the ﬁrst (left) and the last (right) positioned phonemes in the morpheme, and the phoneme-\n8\nconnectivity-matrix indicates the legal sound combinations in Korean phonology using the deﬁned\nphonemic connectivity information. For example, in ﬁgure 6, the morpheme l can be combined with\nthe morpheme swu during the morpheme connectivity checking even if swu is actually pronounced\nas sswu (see the input in ﬁgure 6). The phoneme-connectivity-matrix supports the legality of the\ncombination of l sound with changed s to ss sound. This legality comes from the Korean phonology\nrule glotalization (one form of consonant dissimilation) stating that s sound becomes ss sound\nafter l sound. In this way, we can declaratively model all the major Korean phonology rules such\nas (syllable-ﬁnal consonant) standardization, consonant assimilation, palatalization, glotalization,\ninsertion, deletion, and contraction.\n6\nImplementation and experiment results\nThe SKOPE speech and natural language integration architecture was implemented using a stan-\ndard C and X-window user interface on a UNIX/Sun Sparc platform. The system’s inputs are\ncarefully articulated Korean speeches in a normal laboratory environment, and the outputs are\nmorphologically analyzed eojeol sequences which can be directly used by Korean syntactic and\nsemantic analysis modules. We constructed a 1000 morpheme-entry UPM dictionary in a UNIX\noperating system domain [14], and built morpheme connectivity and phoneme connectivity matrices\nfor the phonological/morphological co-analysis. The UPM dictionary is indexed using the diphone\ntranscribed HMM headers for each morpheme, which are organized into a trie. Since we don’t have\nany standard segmented Korean speech database yet, we constructed our own by recording and\nmanually segmenting 73 most frequent Korean diphones. The 73 diphones are acquired from the\n300 Korean eojeols (each eojeol is pronounced 15 times by a female speaker) in 50 Korean sentences\nwhich can appear in natural language commanding to the UNIX operating system [14].\nSeveral experiments were performed to verify the system’s performance of time-shift invariance,\ndiphone recognition, and ﬁnal eojeol recognition including the spoken language morphological anal-\nysis. In each experiment, the input speech patterns were prepared as follows: eojeols were recorded\nin a normal laboratory environment with an average S/N ratio of 12 dB. Speech data were sampled\nat 16kHz-16bit, and hamming-windowed. From this windowed data, 512-point DTFTs were com-\nputed at 5 msec intervals. The DTFTs were used to generate 16 Mel-scale ﬁlter-bank coeﬃcients\nat 10 msec frame size [8]. These spectra were normalized to produce suitable input levels for the\nfour-layer TDNNs. We used hyperbolic arc tangent error function for the weight updating [15] in\nthe back propagation training, and updated the weights after a small number of iterations [16].\n6.1\nTime-shift invariance of Korean diphones\nWe generated 2400 diphone samples for typical 12 Korean diphones. The input patterns for two\ntest cases are set the same in order to compare the no-time-shift and time-shift cases. Figure 8\nshows that the Korean diphone recognition maintains the time shift invariance property of TDNN\nand suggests the optimal time interval near 200 - 250 msec.\n6.2\nComparison of diphone recognition vs. phoneme recognition\nThis experiment is to show that diphones can improve the recognition rate of Korean vowels re-\ngardless of many rising diphthongs compared with the phoneme recognition. In the test, we set\n9\n% errors\n7.0\n6.0\n5.0\n4.0\n3.0\n2.0\n1.0\n      150               200           250             300        \ntime frame interval for a diphone (msec)\nFigure 8: Average error rate of the segmented time frame (solid lines) versus the same time frame\nwith maximum 40 msec left or right temporal shift (dotted lines)\nunit of\nrecognition\nnumber of\ntargets\nnumber of \nsamples\nrecognition\nrate\nphoneme         9                      1080               94.06%\n                        17                    2040               89.80%\ndiphone           9                      1080              95.42%\n                        17                    2040              95.27%\nFigure 9: Diphone recognition versus phoneme recognition test\n150 msec time range for the phoneme and 200 msec for the diphone segmentation. Compared with\nthe phoneme recognition, ﬁgure 9 shows that diphone recognition performance doesn’t drop much\nwhen the number of targets with similar features doubly increases.\n6.3\nPerformance of continuous diphone recognition\nIn this experiment, we pronounced carefully chosen 66 eojeols 15 times to generate about 5500\ndiphone patterns for training. The 5500 training samples are used to train the vowel group TDNN\nand 18 diﬀerent sub-TDNNs for each diphone group. During the recognition, the new 262 eojeols\nare selected to generate the test patterns of 2432 eojeols, and these test patterns are shifted 30\nmsec during the recognition to obtain the TDNN diphone spotting performance in a continuous\nspeech.\nFigure 10-a shows the continuous diphone spotting performance.\nWe have total 7772\ntarget diphones from the 2432 test eojeol patterns. The correct designates that the correct target\ndiphones were spotted in the testing position, and the delete designates the other case (including\nthe substitution errors). The insert designates that the non-target diphones were spotted in the\ntesting position. To compare the ability of handling the continuous speech, we also tested the\ndiphone spotting using the hand segmented test patterns with the same 7772 target diphones.\nFigure 10-b shows the segmented diphone recognition performance. Since the test data are already\nhand-segmented before input, there are no insertion and deletion errors in this case. The fact that\n10\npattern size\n total            correct           delete            insert\n(rec. rate)\nrec. rate       \nvowel group    sub-TDNNs average     total average\nb. segmented diphones\n7772       7259 (93.3%)     513 (6.4%)    3000 \n 94.9%                        98.8%                               93.8% \na. continuous diphones\nFigure 10: Continuous diphone spotting versus segmented diphone recognition\nthe segmented speech performance is not much better than the continuous one (93.8% vs. 93.3%)\ndemonstrates the diphone’s suitability to handling the Korean continuous speech.\n6.4\nPerformance of continuous speech morphological analysis\nIn order to test the ability of full eojeol recognition including the Viterbi-based lexical decoding\nand phonological/morphological co-analysis, a middle-vocabulary experiment was carried out. The\ntask is a speaker-dependent and continuous eojeol recognition which produces the morphologically\nanalyzed eojeol sequences directly from the speech inputs. In the process, the speech recognizer\nproduces the erroneous diphone sequences in input eonjeols, and then the Viterbi morphological\nanalyzer segments them with the error correction and produces the ﬁnal analyzed eojeols. So, in\nthis task, all the intermediate steps, that is, diphone spotting, lexical decoding and morphologi-\ncal/phonological analysis, are combined to produce the ﬁnal recognition performance. The same\n328 eojeols in section 6.3 were fed to the SKOPE integration architecture that has the pre-trained\nTDNNs (with 66 eojeols).\nFigure 11-a shows the performance with the trained 66 eojeols and\nﬁgure 11-b shows the ﬁnal performance of the total 328 eojeols. We have total 4266 target mor-\nphemes from the same 328 eojeols used in section 6.3. In the ﬁgure, the correct designates that the\ncorrect morpheme sequences can be analyzed from the speech input, and the delete means that the\ncorrect morpheme sequences cannot be generated (including the substitution errors). The insert\ndesignates the percentage of the spurious morphemes that are generated from the insertion errors.\nThe performance is about 80.6% correctness in the ﬁnal morphological analysis with the mostly\nuntrained new data, which is quite promising considering the complexity of the task.\n7\nComparison with related researches\nRecently, the idea of sending only n best speech recognition results to a natural language module\nhas been implemented using the time-synchronous Viterbi-style search algorithm [1]. The algorithm\nwas also improved by the word-dependent search [2] and by adding the A* backward tree search\n[17]. The n-best integration scheme has been mostly utilized for HMM-based continuous speech\nrecognition systems, and many existing speech systems and natural language systems were suc-\ncessfully integrated using the n-best word search techniques [3, 4]. However, until now, the n-best\nsearch techniques are only implemented to directly produce the n-best sentences using the word\nsequences, and this word-level integration is ineﬃcient for the morphologically complex languages\n11\npattern size\n(rec. rate)\ntotal             correct            delete            insert\n                 (88.67%)      \npattern size\n(rec. rate)\ntotal             correct            delete            insert\n4266          3440                 826                     902 \n              (80.64%) \n2083            1847               236                        311\na. for trained 66 eojeols\nb. for total 328 eojeols\nFigure 11: Continuous speech input morphological analysis performance\nsuch as Korean. On the contrary, our integration is at the morpheme-level directly decoding the\nPLU sequences with the morphological processing because we need more sophisticated phonolog-\nical/morphological handling in the early stage of the integration process. The word-level n-best\nintegration also assumes the word-level dictionary which is an unreasonable assumption for mor-\nphologically complex languages.\nAccording to the Harper and others’ recent classiﬁcation [18],\nn-best integration is a typical loosely-coupled example.\nThe HMM-LR integration [19, 20] was implemented using the HMM’s phoneme spotting abil-\nity integrated with the generalized LR parsing techniques [21]. Unlike the n-best integration, the\nHMM-LR integration was more tight and implemented at the phoneme-level by extending the LR\nparser’s terminal symbols to cover the phonetic transcriptions. In this scheme, the LR parsing\nselects the most probable parsing results by obtaining the probability of the end-point candidate\nphonemes from the HMM’s forward probability calculation.\nSo the total integrated system is\nworking by the LR parser’s prediction of the next phoneme candidates which are then veriﬁed\nby the HMM’s phoneme spotting abilities. The idea of extending the LR grammar to the pho-\nnetic transcriptions seems to be working for the phoneme-level integration. However, the scheme\ndoesn’t have any separate language-level dictionary, which results in the degenerated phonologi-\ncal/morphological processing, and also suﬀers from diﬃculty in the necessary scale-ups. On the\ncontrary, our SKOPE integration architecture focuses on the general phonological/morphological\nhandling during the integration which is essential for the agglutinative languages. The idea of ex-\ntending LR grammar to the phonetic transcriptions was also applied to the TDNN-LR integration\nmethod [22, 23] which was similarly implemented by replacing HMM’s phoneme spotting by the\nTDNN’s phoneme spotting. The integration was implemented by dynamic time warping (DTW)\nlevel-building search [24] between TDNN’s phoneme sequences and LR grammar’s phoneme se-\nquences. However, the performance was relatively poor compared with the HMM-LR integration\nmethod [22]. There are basically two reasons for the poor TDNN-LR performance compared with\nthe HMM-LR integration: 1) the TDNN model has rarely been applied to the practical large vo-\ncabulary systems yet, therefore it lacks the ﬁne tuning compared with the popular HMM models,\nand 2) the TDNN model has yet to ﬁnd a right way to be eﬀectively integrated into the natural\nlanguage processing model. The HMM model supports a natural integration into the general chart-\nbased parsing models such as generalized LR parsing because there are well-deﬁned probablistic\n12\nsearch techniques in the language as well as in the speech levels. However, output activations of\nthe multiple TDNNs are diﬃcult to normalize and therefore diﬃcult to be naturally integrated into\nthe popular probabilistic search schemes such as Viterbi search. Our SKOPE architecture adopts\nViterbi search with pre-deﬁned transition and emission probabilities, and use the Viterbi search\nfor only segmenting erroneous diphone strings. All the other morphological processing steps are\ngenerally performed according to the symbolic natural language processing model.\nThe more tightly-coupled systems have also been researched to integrate all the knowledge\nsources of spoken language processing from acoustic to semantic into a single interdependent model\nthat cannot easily be separated. In these systems, syntactic parser directly deals with acoustic-\nlevel inputs. For example, Ney [25] extended CYK parsing algorithm to cover acoustic inputs by\nexhaustively ﬁnding all possible endpoints for every terminal symbol. In the similar vein, the HMM\ncan be extended to handle recursive embedding for context-free grammar processing [26]. However,\nthese acoustic-level syntactic parsers are computationally expensive since the parsing complexity is\nat best O(n3) where n could be in several hundreds when the parsers directly deal with the speech\nframes. The SKOPE integration is tighter than loosely-coupled n-best techniques, but less tight\ncompared with these tightly-coupled systems. We agree that the high-level linguistic constraints\nshould restrict the underlying speech recognition in some ways as in the tightly-coupled systems,\nbut disagree that the constraints should be in a syntax level. The more tightly-coupled systems\nare often impractical for large-scale spoken language processing because of the time complexity.\nMoreover, we still don’t have much knowledge to tell how much top-down feedback is actually\nhelpful to improve the speech recognition process. As an engineering point of view, semi-tightly-\ncoupled systems are quite feasible for large complex systems under the current technology. In this\nregard, SKOPE project adopts a semi-tightly-coupled integration technique between speech and\nlanguage processing, especially morphological processing.\n8\nConclusions\nThis paper presents a morpheme-level integration architecture of speech and natural language in\na connectionist continuous speech recognition model for agglutinative languages such as Korean.\nOur main contributions are to present the morphologically conditioned semi-tight integration model\nthat can support sophisticated phonological/morphological processing in the integration of speech\nand language, which is essential for morphologically complex agglutinative languages. Also, the\nSKOPE integration architecture is a ﬁrst attempt to develop a morphologically general integration\nmodel using the connectionist speech recognition paradigm.\nThe SKOPE speech and language integration architecture has many novel features for speech\nand natural language processing. First, the diphone-based TDNN proposes a nice sub-word unit of\nrecognition, well reﬂecting the Korean phonetic characteristics. Secondly, the morphological analy-\nsis combined with the declarative phonological rule modeling is well suited to the phonetic spelling\ninto the orthographic morpheme mapping, which is an essential task for every spoken language pro-\ncessing model. Finally, the trie-structured HMM indexing for UPM dictionary enables the Viterbi\nstyle search to be applied to the thorny morpheme segmentation and lexical decoding problem, and\nalso provides natural integration of symbolic natural language processing techniques with proba-\nbilistic decoding schemes. The experiments show that the ﬁnal morphological analysis performance\nfrom continuous speech is over 80.6% in a middle-vocabulary speaker-dependent recognition task,\nwhich is very promising in considering the continuous speech and the combination of several steps of\n13\nperformances such as diphone spotting, lexical decoding and morphological/phonological analysis.\nSince the integration architecture is based on general linguistic notion of phoneme and morpheme,\nthe architecture is not restricted to Korean. The SKOPE architecture can be extended to any ag-\nglutinative language which has clear-cut morphological boundaries such as Japanese, and possibly\nto other Indo-European languages which exhibit well-developed morphological phenomena such as\nGerman. We are now extending the integration technique to Japanese.\nAcknowledgements\nThis research was partly supported by grants from KOSEF and ETRI in Korea. Many of our\nformer or current students implemented the idea and we thank all of them. Especially, we thank\nKyunghee Kim for implementing diphone-based Korean speech recognizer, WonIl Lee for coding\nthe lexicon and the morphological parser, and ﬁnally ByungChang Kim for implementing Viterbi\nsearch for the diphones. Professor Hong Jeong directed us to the deep world of signal processing\nfor the earlier part of the project.\nReferences\n[1] Y. Chow and R. Schwartz, “The n-best algorithm: An eﬃcient procedures for ﬁnding top N\nsentence hypothesis,” in Proceedings of the second DARPA workshop on speech and natural\nlanguage, Los Altos, CA, 1989, Morgan Kaufmann Publishers, Inc.\n[2] R. Schwartz and S. Austin, “Eﬃcient, high-performance algorithms for n-best search,” in\nProceedings of the third DARPA workshop on speech and natural language, Los Altos, CA,\n1990, Morgan Kaufmann Publishers, Inc.\n[3] M. Agnas, H. Alshawi, I. Bretan, D. Carter, K. Ceder, M. Collins, R. Crouch, V. Digalakis,\nB. Ekholm, B. Bamback, J. Kaja, J. Karlgren, B. Lyberg, P. Price, S. Pulman, M. Rayner,\nC. Samuelsson, and T. Svensson, “Spoken language trnaslator: ﬁrst year report,” Technical\nReport ISRN SICS-R-94/03-SE, Swedish Institute of Computer Science and SRI International,\n1994.\n[4] M. Bates, R. Bobrow, P. Fung, R. Ingria, F. Kubala, J. Makhoul, L. Nguyen, R. Schwartz,\nand D. Stallard, “The BBN/HARC spoken language understanding system,” in Proceedings\nof the ICASSP-93, 1993.\n[5] G. Forney, “The Viterbi algorithm,” Proc. of the IEEE, vol. 61, pp. 268–278, 1973.\n[6] K. Kim, G. Lee, and J. Lee, “Integrating TDNN-based diphone recognition with table-driven\nmorphology parsing for understanding of spoken Korean,” in Proceedings of the international\nconference on spoken language processing (ICSLP), 1994.\n[7] D. Morgan and C. L. Scoﬁeld, Neural networks and speech processing, Kluwer Academic Pub-\nlishers, Inc., 1991.\n[8] A. Waibel, T. Hanaazawa, G. Hinton, K. Shikano, and K. Lang, “Phoneme recognition using\ntime-delay neural networks,” IEEE Transactions on Acoustics, Speech and Signal Processing,\nvol. 37, no. 3, pp. 328 – 339, 1989.\n14\n[9] W. Lee, G. Lee, and J. Lee, “Table-driven neural syntactic analysis of spoken Korean,” in\nProceedings of COLING-94, 1994.\n[10] K. F. Lee, Automatic speech recognition, Kluwer Academic Publishers, Inc., 1989.\n[11] R. Sproat, Morphology and computation, The MIT Press, 1992.\n[12] A. Aho and J. D. Ullman, The theory of parsing, translation, and compiling, Vol 1: parsing,\nPrentice-Hall, Englewood Cliﬀs, NJ, 1972.\n[13] K. Koskenniemi, “Two-level model for morphological analysis,” in Proceedings of the 6th In-\nternational Joint Conference on Artiﬁcial Intelligence, 1983.\n[14] W. Lee and G. Lee, “From natural language to shell-script: A case-based reasoning system for\nautomatic unix programming,” Expert systems with applications: An international journal,\nvol. 9, no. 1, pp. 71–79, 1995.\n[15] S. E. Fahlman, “Faster-learning variations on back-propagation: An empirical study,” in Pro-\nceedings of the 1988 connectionist models summer school, 1988.\n[16] P. Haﬀner, A. Waibel, H. Sawai, and K. Shikano, “Fast back-propagation learning methods\nfor large phonemic neural networks,” in Proc. of the Eurospeech-89, 1989.\n[17] F. K. Soong and E. Huang, “A tree-trellis based fast search for ﬁnding the n-best sentence\nhypotheses in continuous speech recognition,” in Proceedings of the third DARPA workshop\non speech and natural language, Los Altos, CA, 1990, Morgan Kaufmann Publishers, Inc.\n[18] M. Harper, L. Jamieson, C. Mitchell, G. Ying, S. Potisuk, P. Srinivasan, R. Chen, C. Zoltowski,\nL. McPheters, B. Pellom, and R. Helzerman, “Integrating language models with speech recog-\nnition,” in Proceedings of the AAAI’94 workshop on integration of natural language and speech\nprocessing, 1994.\n[19] K. Kita, T. Kawabata, and H. Saito, “HMM continuous speech recognition using predictive\nLR parsing,” in Proceedings of the ICASSP-89, 1989.\n[20] T. Hanazawa, K. Kita, S. Nakamura, T. Kawabata, and K. Shikano, “ATR HMM-LR contin-\nuous speech recognition system,” in Proceedings of the ICASSP-90, 1990.\n[21] M. Tomita, Eﬃcient parsing for natural language - A fast algorithm for practical systems,\nKluwer Academic Publishers, 1986.\n[22] H. Sawai, “TDNN-LR continuous speech recognition system using adaptive incremental TDNN\ntraining,” in Proceedings of the ICASSP-91, 1991.\n[23] H. Sawai, “The TDNN-LR large-vocabulary and continuous speech recognition system,” in\nProceedings of the international conference on spoken language processing (ICSLP), 1990.\n[24] C. Myers and L. Rabiner, “A level building dynamic time warping algorithm for connected\nword recognition,” IEEE Trans. on ASSP, vol. 29, no. 2, pp. 284–279, 1981.\n15\n[25] H. Ney, “Dynamic programming parsing for context-free grammars in continuous speech recog-\nnition,” IEEE Trans. on Signal Processing, vol. 39, no. 2, , 1991.\n[26] K. Lari and S. Young, “Applications of stochastic context-free grammars using the inside-\noutside algorithm,” Computer speech and language, vol. 5, no. 3, , 1991.\n16\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1996-03-18",
  "updated": "1996-03-18"
}