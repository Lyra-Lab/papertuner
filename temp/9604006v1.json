{
  "id": "http://arxiv.org/abs/cmp-lg/9604006v1",
  "title": "The Role of the Gricean Maxims in the Generation of Referring Expressions",
  "authors": [
    "Robert Dale",
    "Ehud Reiter"
  ],
  "abstract": "Grice's maxims of conversation [Grice 1975] are framed as directives to be\nfollowed by a speaker of the language. This paper argues that, when considered\nfrom the point of view of natural language generation, such a characterisation\nis rather misleading, and that the desired behaviour falls out quite naturally\nif we view language generation as a goal-oriented process. We argue this\nposition with particular regard to the generation of referring expressions.",
  "text": "arXiv:cmp-lg/9604006v1  18 Apr 1996\nThe Role of the Gricean Maxims\nin the Generation of Referring Expressions\nRobert Dale\nMicrosoft Institute\n65 Epping Road\nNorth Ryde\nSydney nsw 2113\nAustralia\nrdale@mpce.mq.edu.au\nEhud Reiter\nDepartment of Computer Science\nKing’s College\nUniversity of Aberdeen\nAberdeen ab9 2ue\nScotland\nereiter@csd.abdn.ac.uk\nAbstract\nGrice’s maxims of conversation [Grice 1975] are framed\nas directives to be followed by a speaker of the lan-\nguage. This paper argues that, when considered from\nthe point of view of natural language generation, such a\ncharacterisation is rather misleading, and that the de-\nsired behaviour falls out quite naturally if we view lan-\nguage generation as a goal-oriented process. We argue\nthis position with particular regard to the generation\nof referring expressions.\nIntroduction\nThe position taken in this paper can be summarised as\nfollows.\n1. Grice’s maxims [Grice 1975] are framed as directives\nto the speaker, and so it is natural to consider how\nthey might impact on the task of natural language\ngeneration (nlg).\n2. A number of the maxims can collectively be expressed\nby the imperative “Don’t say too much and don’t\nsay too little.”\nThis focusses our attention on the\nlanguage generation subtask of content determi-\nnation; and one of the more constrained and well-\nexplored aspects of content determination is the gen-\neration of referring expressions. However, if we look\nat this task in detail, it becomes clear that there are\nproblems with enforcing a literal interpretation of the\nmaxims.\n3. We review some of our previous work that has tried\nto address this problem, but go on to suggest a rather\nmore radical position: that Grice’s maxims are un-\nnecessary directives from the point of view of refer-\nring expression generation, and that, provided the\nregister and sublanguage conventions of the genre in\nforce are conformed to, the behaviour the maxims\ncharacterise actually falls out quite naturally from\nviewing nlg as a goal-oriented process.\nUnder this view, the maxims are no more than post hoc\ncharacterisations of the way language works, and their\nframing as directives is ultimately rather misleading.\nCopyright c⃝2021, American Association for Artiﬁcial In-\ntelligence (www.aaai.org). All rights reserved.\nAdequate and Eﬃcient Referring\nExpressions\nGrice and Reference: Deciding What To\nSay\nIt has become commonplace to view language genera-\ntion as encompassing two kinds of concerns: deciding\nwhat to say, and deciding how to say it.1\nThe con-\nsiderations that arise in deciding what to say are often\nexpressed in terms that echo Grice’s Maxims of Quan-\ntity: don’t say too much, and don’t say too little. There\nare many reasons why such imperatives are worth at-\ntending to in the development of algorithms to be used\nby computational systems which generate natural lan-\nguage; for example, we want to make sure that we have\ngiven the hearer the information that she needs, but\nwe don’t want to bore her with a ﬂood of unnecessary\nstatements.\nFrom the point of view of implicatures,\nhowever, an additional concern is that saying too much\nmight lead the hearer to read between the lines in ways\nthat were unintended by the underlying system. Ulti-\nmately, language generation systems should be as ca-\npable of exploiting the notion of conversational impli-\ncature as much as people do; but before we can achieve\ngoals of that kind, it’s important that we know when\nwe are obeying the maxims. In other words, our ﬁrst\npriority is to ensure that the generated text does not\nunintentionally contain false implicatures.\nThe task of generating referring expressions—and\nin particular, anaphoric referring expressions—provides\nan arena where we can move towards a more formal\nspeciﬁcation of what this really involves. Given some\ninternal symbol that corresponds to an intended refer-\nent, the job of a referring expression generator is to de-\ntermine the semantic content of a noun phrase that will\nidentify the intended referent to the hearer. The ﬁrst\nserious consideration of this issue in the nlg literature\nwas probably McDonald’s [1980] discussion of potential\ndistractors—other entities in the context we might mis-\ntakenly refer to—when deciding whether or not it was\nsafe to use a pronoun to refer to an entity. Appelt [1982]\n1We take no particular stance in the present discussion\nas to how this distinction impacts on the modularity of the\narchitecture of a language generation system.\nand Novak [1988] looked at determining the content of\ndeﬁnite noun phrase referring expressions in situations\nwhere (for whatever reason) a pronoun could not be\nused. In Dale [1989], one of us characterised the task of\ndetermining the content of a referring expression as be-\ning constrained by three Grice-like principles: the prin-\nciple of sensitivity, which states that the referring\nexpression chosen should take account of the state of\nthe hearer’s knowledge; the principle of adequacy,\nwhich states that the referring expression chosen should\nbe suﬃcient to identify the intended referent; and the\nprinciple of efficiency, which states that the re-\nferring expression chosen should provide no more infor-\nmation than is necessary for the identiﬁcation of the\nintended referent.\nAn Algorithm for Saying The Right\nAmount\nSuggestions that referring expression generation should\nbe governed by principles like those just described are\ncommon, but detailed algorithms that meet the speci-\nﬁed goals are somewhat rarer. Dale [1989] proposed an\nalgorithm which assumes the following scenario:\nSuppose that we have a set of entities C (called\nthe context set) such that C = {a1, a2, . . . , an};\nand our task is to distinguish from this context\nset some intended referent r where r ∈C. Sup-\npose, also, that each entity ak is described in the\nsystem’s knowledge base by means of a set of prop-\nerties, pk1, pk2, . . . , pkm. In order to distinguish our\nintended referent r from the other entities in C, we\nneed to ﬁnd some set of properties which are to-\ngether true of r, but of no other entity in C. The\nlinguistic realisation of this set of properties con-\nstitutes a distinguishing description (dd) of r\nwith respect to the context C. A minimal dis-\ntinguishing description is then the linguistic\nrealisation of the smallest such set of properties.\nThe detail of the algorithm is unimportant for present\npurposes; basically, it consists of three steps as follows:\n1. Check Success: see if the description we have con-\nstructed so far picks out only one entity in the con-\ntext. If it does, stop. If not, go to Step 2.\n2. Choose Property: determine which property of the\nintended referent would rule out the largest number\nof other entities in the context. Go to Step 3.\n3. Extend Description: add the chosen property to the\ndescription being constructed, and remove the enti-\nties it rules out from the context. Go to Step 1 with\nthis extended description and the reduced context.\nReiter [1990] noted a serious deﬁciency of this algo-\nrithm: it will not in fact always produce a minimal\ndistinguishing description, and indeed ﬁnding a mini-\nmal distinguishing description is equivalent to solving a\nminimal set-cover problem, which is np-hard. The com-\nputational complexity of this task raises the question of\nwhether it is appropriate to insist on an algorithm that\ncreates minimal distinguishing descriptions.\nIn other\nwords, it may be unreasonable to try to construct max-\nimally adequate and eﬃcient referring expressions; or,\nmore to the point, meeting Grice’s requirements of say-\ning neither too much nor too little, if taken literally,\nmay be computationally unachievable.\nCooperative Behaviour as an\nEpiphenomenon\nAllowing in Redundancy\nOne response to Reiter’s objection is to take the view\nthat the notion of minimality sought in the algorithm\nabove is too strong. In subsequent work we took a step\nback and asked: what do people actually do when they\nconstruct referring expressions? It is very diﬃcult to\nmake any strong claims on the basis of the experiments\nthat have been done, but it does seem to be the case\nthat people do not build minimal distinguishing descrip-\ntions in the strong sense suggested above. We explored\nthese considerations in more detail in [Dale and Reiter\n1995], where we proposed a revised algorithm which is\ncomputationally eﬃcient at the cost of producing some\ninformational redundancy in the referring expressions\nit generates.\nImplicit vs Explicit Pursuance of the\nMaxims\nSo: obeying the Gricean maxims looks computationally\nproblematic, and it seems not to be what people do\n(with some caveats: we are assuming a literal interpre-\ntation of the maxims, and assuming that it is possible\nto generalise from the experimental results).\nOur early attempts to generate referring expressions\n(as presented, for example, in Dale [1989] and Re-\niter [1990]) explicitly enforced variants of the Gricean\nMaxim of Quantity. However, our current hypothesis is\nthat this is in fact unnecessary. We now take the view\nthat it is a mistake to view the Gricean Maxims as di-\nrectives; they are really no more than post hoc charac-\nterisations of what is going on. They may even mislead\nus in the construction of mechanisms that cooperate\nconversationally.\nIt is generally accepted that language generation can\nfruitfully be viewed as a goal-oriented process. In other\nwords, a natural language generation system may be\ngiven as input an agenda of goals that are to be satis-\nﬁed in the text being constructed; the system’s task is\nto ﬁnd linguistic devices that satisfy each goal, remov-\ning the goal from the agenda once it has been achieved.\nThere are a wide variety of goals that might appear in\nsuch a mechanism. In the context of referring expres-\nsion generation, typical goals could include:\n• Getting the hearer to identify the intended referent\nr.\n• Alerting the hearer to the fact that r has the property\nrepresented by the attribute value pair ⟨a, v⟩.\nA goal of the form of the ﬁrst of these will give rise to\nthe construction of a distinguishing description; a goal\nof the form of the second of these might result in the\ninclusion of information beyond that required for refer-\nent identiﬁcation (Robin [1994] is a good discussion of\nsome of the issues that arise in developing algorithms\nto achieve such goals). This information could be ex-\npressed in a separate clause, but could equally well be\nfolded into the same referring expression that is being\nused for the referent identiﬁcation in the ﬁrst goal (Ap-\npelt [1982] provides a very nice example of this).\nThe fact that information can appear in a noun\nphrase for purposes other than referent identiﬁcation\nmeans, of course, that the hearer has to do some work\nin determining what the role of each provided descrip-\ntor is. In the case of an utterance like Give me the red\npen, the speaker may be providing the term red in order\nto distinguish the intended referent from another pen\nwhich is green. It is equally possible, though, that there\nis only one pen in the context, and red is included in the\ndescription because colour has special salience (it may\nbe easier for the speaker to ﬁrst look for red objects,\nand then ﬁnd the particular red object which is a pen).\nAnother possibility—perhaps a little tenuous in the cur-\nrent example, but clearly a possibility nonetheless—is\nthat the hearer may be red–green colour blind, and the\nspeaker is imparting additional information about the\ncolour of the pen which the hearer may be able to make\nuse of later. A more common clue to descriptor pur-\npose is that fact that some properties are more likely to\nbe used for referent identiﬁcation than others. In the\nutterance Sit by the newly-painted table, for example,\nthe property newly-painted could be being used to dis-\ntinguish the intended referent from other tables in the\ncontext, but it is rather more likely that its purpose is\nto warn the hearer not to put her elbows on the table.2\nIn addition to goal orientation, aspects of genre such\nas register and sublanguage also play an important\nrole in determining appropriate referring expressions.\nIn particular, whether a speciﬁc referring expression is\ninterpreted by the hearer as being purely for identiﬁ-\ncation or not may depend on the current genre. For\nexample, in casual conversation, a hearer might inter-\npret Give me the Staedtler pen as having some purpose\nbeyond simple identiﬁcation (perhaps informing the\nhearer that the speaker prefers pens made by Staedtler),\nsince manufacturer is not a commonly used attribute in\nidentiﬁcation-only referring expressions in this genre.\nIn an inventory-stocking context, on the other hand,\nGive me the Staedtler pens might be construed as purely\nreferential, since manufacturer is often used as an iden-\ntifying attribute in this genre. Consequently, an nlg\nsystem that is generating an identiﬁcation-only refer-\nring expression should if possible use only those at-\ntributes that are typically used for identiﬁcation in the\ntarget genre; otherwise, false implicatures may arise.\nHowever, again we believe that there is no need to ex-\n2This example is due to Bonnie Webber.\nplicitly model this phenomenon as an implicature; it is\nsuﬃcient to design the system so that it uses the identi-\nfying attributes preferred in its target genre (as is done\nvia the PreferredAttributes list in the algorithm\nof [Dale and Reiter 1995]).\nAlthough in the above examples the hearer may have\nto perform some potentially complex inferencing to de-\ntermine what the speakers’ goals are, note that there\nis no need for the speaker to do anything other than\nsatisfy the list of goals using resources appropriate to\nthe current genre. Nowhere is there an explicit attempt\nto adhere to the maxims.\nReassessing the Maxims\nIn the light of the above discussion, we revisit Grice’s\nmaxims in this section and comment on how each might\nbe best interpreted in the context of natural language\ngeneration.\nThe Maxim of Quality\nTry to make your contribution one that is true. More\nspeciﬁcally:\n1. Do not say what you believe to be false.\n2. Do not say that for which you lack adequate evidence.\nNo natural language generation systems that we are\naware of deliberately say things that are false: this can\nhappen by accident, of course, but then it is not inten-\ntional.\nAn arguable exception to this claim is the work of\nJameson [1987], whose Imp system injects bias into its\nutterances in order to mislead; but even here this is not\ndone by telling lies. Certainly, in principle one could\nconstruct a generation system that ‘lied’ for purposes\nsuch as advertising or manipulation, or that produced\ndescriptions that were ‘correct’ relative to the hearer’s\nknowledge even if they were untrue in the world; for\nexample, we might want to build a system which could\ngenerate the man drinking a martini to refer to a man\nwho was actually drinking water from a martini glass.\nThis would require explicit programming, however; the\ndefault behaviour of all systems we are aware of is to\nautomatically obey the Maxim of Quality.\nThe Maxim of Quantity\n1. Make your contribution as informative as is required\n(for the current purposes of the exchange).\n2. Do not make your contribution more informative\nthan is required.\nThe ﬁrst part of the Maxim of Quantity is automatically\nfulﬁlled by a goal-oriented system: the goal will not be\nsatisﬁed until suﬃcient information is provided. What\nwe should say about the second part of the maxim, how-\never, depends on how strongly or literally we choose to\ninterpret it. If we insist that referring expressions or\nother utterances contain no unnecessary words, then we\nwill probably have to explicitly enforce this as a con-\nstraint in our nlg system; in general, nlg systems will\nnot automatically obey this rule. On the other hand, if\nwe interpret the second part of the Maxim of Quality\nas meaning ‘do not go out of your way to add extra in-\nformation that is not needed’, then this behaviour once\nmore comes for free with goal-orientation. Our experi-\nence suggests that, at least for the task of generating\nreferring expressions, the second interpretation is the\nbest one.\nThe Maxim of Relevance\nBe relevant.\nYet again, this follows directly from goal-oriented be-\nhaviour: there is no reason why the system should con-\nsider saying something that is not relevant. It is possi-\nble, of course, that an algorithm might unintentionally\ninclude irrelevant information; this is also true of hu-\nman linguistic behaviour.\nThe Maxim of Manner\nBe perspicuous. More speciﬁcally:\n1. Avoid obscurity of expression.\n2. Avoid ambiguity.\n3. Be brief (avoid unnecessary prolixy).\n4. Be orderly.\nThese are places where an anticipation feedback mech-\nanism of the kind proposed by Jameson and Wahlster\n[1982] might be appropriate: i.e, we might like a system\nto subject its proposed utterance to a self-monitoring\nstage, to make sure that it is not ambiguous and so on.\nOf all the maxims, it is perhaps these (with the excep-\ntion of the brevity submaxim, to which our response is\nthe same as our response to the Maxim of Quantity)\nwhich are most amenable to explicit modelling in the\ngeneration process; but even here it is equally possible\nthat over time we learn heuristics that do the job for us,\nso that the generation task more or less naturally pro-\nduces results that have the required characteristics (see\nLevelt’s [1989] comments on this as a possible charac-\nteristic of the human language production mechanism).\nExploiting Violations\nThe goal-oriented approach has the additional beneﬁt\nthat using violations of the maxims in order to get some\nother point across falls out as part of the same mecha-\nnism; again, see Jameson’s [1987] work in this regard.\nFrom the point of view of the generator they are not\nviolations at all.\nConclusions\nWe have argued that Grice’s Maxims do not need to be\nexplicitly enforced or modelled in a natural language\ngeneration system. Instead, they should be replaced by\nthe following system construction principles.\nGrice and Generation\nA generation system should be goal-driven, and con-\nform to the current genre. As a general architecture,\nthis suggests a process which builds an agenda of goals,\nand then searches for communicative and linguistics re-\nsources in the target genre which can be used to realize\nthese goals. There is the possibility that such a sys-\ntem may end up saying something beyond what was\nintended. That is acceptable. Minimality is not neces-\nsary; provided the information that is provided is there\nbecause it serves some purpose, hearers will not make\ninappropriate inferences.\nGrice and Interpretation\nAs a corollary to the goal-oriented view of generation,\nthe hearer should assume that every informational ele-\nment in the speaker’s utterance is there with some in-\ntended purpose. The hearer’s job is then to work out\nwhat the speaker’s intended purpose is. If we are in\na context, for example, where referent identiﬁcation is\nobviously the task, and the expression contains infor-\nmation unnecessary for identiﬁcation, the hearer must\nconsider the possibility that this information has been\nprovided by the speaker for some other purpose—but\nnote that it might not be; it might have been put in\nto help with identiﬁcation even if it turns out that the\nhearer did not make use of it for that purpose.3 Prop-\nerties which are clearly not able to help us in identi-\nfying the intended referent must be doing something\nelse. In the context of our newly-painted table example,\nit many cases it will be impossible to determine that\nsomething is newly-painted simply by looking; and it is\nunlikely that the speaker intends us to go around ac-\ntually touching all the tables to identify which one has\nthat property; so it is reasonable to assume that the\nproperty has been provided for some other purpose.\nIn Summary\nUltimately, for the generator, Grice’s maxims taken col-\nlectively mean Don’t include elements that don’t do any-\nthing. Our position is that, under a goal-oriented view\nof language generation, there is no need to explicitly\nfollow such a directive at all; the desired behaviour just\nfalls out of the mechanism.\nWe have argued, in the\npresent paper, that this is true of the referring expres-\nsion generation task; it remains to be seen whether the\nsame story can be told of all language generation, and\nwhat the impact of this is on models of language un-\nderstanding.\n3Returning to our earlier red pen example, suppose the\nhearer just happens to be focussed on the red pen when\nthe utterance is produced. Then, the property of redness\nmay have been included by the speaker in order to help the\nhearer identify the intended referent even if it is not used\nfor this purpose.\nReferences\nDouglas Appelt [1982] Planning Natural Language Ut-\nterances to Satisfy Multiple Goals. Technical Note\n259, SRI International, Menlo Park, California.\nRobert Dale [1989] Cooking Up Referring Expressions.\nIn Proceedings of 27th Annual Meeting of the Associ-\nation for Computational Linguistics, Vancouver bc.\nRobert Dale and Ehud Reiter [1995] Computational\nInterpretations of the Gricean Maxims in the Gen-\neration of Referring Expressions. Cognitive Science,\n19(2), pp233–263.\nH Paul Grice [1975] Logic and conversation. In P Cole\nand J Morgan, editors, Syntax and Semantics: Vol\n3, Speech Acts, pages 43–58. New York: Academic\nPress.\nAnthony Jameson [1987] How to Appear to be Con-\nforming to the Maxims Even if you Prefer to Violate\nThem. Chapter 2 in G Kempen (ed), Natural Lan-\nguage Generation, Dordrecht: Martinis NijhoﬀPub-\nlishers.\nAnthony Jameson and Wolfgang Wahlster [1982] User\nModelling in Anaphora Generation: Ellipsis and Def-\ninite Description. In Proceedings of the Fifth Euro-\npean Conference on Artiﬁcial Intelligence, Pisa, Italy.\nWillem Levelt [1989] Speaking: From Intention to Ar-\nticulation. Cambridge, ma: mit Press.\nDavid McDonald [1980] Natural Language Generation\nas a Process of Decision Making Under Constraints.\nPhD Thesis, mit.\nHans-Joachim Novak [1988] Generating\nRefer-\nring Phrases in a Dynamic Environment.\nChapter\n5 in M Zock and G Sabah (eds), Advances in Natu-\nral Language Generation, Volume 2, pp76–85. Pinter\nPublishers.\nEhud Reiter [1990] Generating\nAppropriate\nNatural\nLanguage Object Descriptions. PhD Thesis, Harvard\nUniversity.\nJacques Robin [1994] Revision-Based\nGeneration\nof\nNatural Language Summaries Providing Historical\nBackground. PhD Thesis, Columbia University.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1996-04-18",
  "updated": "1996-04-18"
}