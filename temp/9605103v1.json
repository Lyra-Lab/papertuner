{
  "id": "http://arxiv.org/abs/cs/9605103v1",
  "title": "Reinforcement Learning: A Survey",
  "authors": [
    "L. P. Kaelbling",
    "M. L. Littman",
    "A. W. Moore"
  ],
  "abstract": "This paper surveys the field of reinforcement learning from a\ncomputer-science perspective. It is written to be accessible to researchers\nfamiliar with machine learning. Both the historical basis of the field and a\nbroad selection of current work are summarized. Reinforcement learning is the\nproblem faced by an agent that learns behavior through trial-and-error\ninteractions with a dynamic environment. The work described here has a\nresemblance to work in psychology, but differs considerably in the details and\nin the use of the word ``reinforcement.'' The paper discusses central issues of\nreinforcement learning, including trading off exploration and exploitation,\nestablishing the foundations of the field via Markov decision theory, learning\nfrom delayed reinforcement, constructing empirical models to accelerate\nlearning, making use of generalization and hierarchy, and coping with hidden\nstate. It concludes with a survey of some implemented systems and an assessment\nof the practical utility of current methods for reinforcement learning.",
  "text": "Journal\nof\nArti\fcial\nIn\ntelligence\nResearc\nh\n\u0004\n(\u0001\t\t\u0006)\n\u0002\u0003\u0007-\u0002\b\u0005\nSubmitted\n\t/\t\u0005;\npublished\n\u0005/\t\u0006\nReinforcemen\nt\nLearning:\nA\nSurv\ney\nLeslie\nP\nac\nk\nKaelbling\nlpk@cs.br\no\nwn.edu\nMic\nhael\nL.\nLittman\nmlittman@cs.br\no\nwn.edu\nComputer\nScienc\ne\nDep\nartment,\nBox\n\u0001\t\u00010,\nBr\nown\nUniversity\nPr\novidenc\ne,\nRI\n0\u0002\t\u0001\u0002-\u0001\t\u00010\nUSA\nAndrew\nW.\nMo\nore\na\nwm@cs.cmu.edu\nSmith\nHal\nl\n\u0002\u0002\u0001,\nCarne\ngie\nMel\nlon\nUniversity,\n\u0005000\nF\norb\nes\nA\nvenue\nPittsbur\ngh,\nP\nA\n\u0001\u0005\u0002\u0001\u0003\nUSA\nAbstract\nThis\npap\ner\nsurv\neys\nthe\n\feld\nof\nreinforcemen\nt\nlearning\nfrom\na\ncomputer-science\np\ner-\nsp\nectiv\ne.\nIt\nis\nwritten\nto\nb\ne\naccessible\nto\nresearc\nhers\nfamilia\nr\nwith\nmac\nhine\nlearning.\nBoth\nthe\nhistorical\nbasis\nof\nthe\n\feld\nand\na\nbroad\nselection\nof\ncurren\nt\nw\nork\nare\nsummarized.\nReinforcemen\nt\nlearning\nis\nthe\nproblem\nfaced\nb\ny\nan\nagen\nt\nthat\nlearns\nb\neha\nvior\nthrough\ntrial-and-error\nin\nteractions\nwith\na\ndynamic\nen\nvironmen\nt.\nThe\nw\nork\ndescrib\ned\nhere\nhas\na\nresem\nblance\nto\nw\nork\nin\npsyc\nhology\n,\nbut\ndi\u000bers\nconsiderably\nin\nthe\ndetails\nand\nin\nthe\nuse\nof\nthe\nw\nord\n\\reinforcemen\nt.\"\nThe\npap\ner\ndiscusses\ncen\ntral\nissues\nof\nreinforcemen\nt\nlearning,\nincluding\ntrading\no\u000b\nexploration\nand\nexploitation,\nestablishing\nthe\nfoundations\nof\nthe\n\feld\nvia\nMark\no\nv\ndecision\ntheory\n,\nlearning\nfrom\ndela\ny\ned\nreinforcemen\nt,\nconstructing\nempirical\nmo\ndels\nto\naccelerate\nlearning,\nmaking\nuse\nof\ngeneralization\nand\nhierarc\nh\ny\n,\nand\ncoping\nwith\nhidden\nstate.\nIt\nconcludes\nwith\na\nsurv\ney\nof\nsome\nimplemen\nted\nsystems\nand\nan\nassessmen\nt\nof\nthe\npractical\nutilit\ny\nof\ncurren\nt\nmetho\nds\nfor\nreinforcemen\nt\nlearning.\n\u0001.\nIn\ntro\nduction\nReinforcemen\nt\nlearning\ndates\nbac\nk\nto\nthe\nearly\nda\nys\nof\ncyb\nernetics\nand\nw\nork\nin\nstatistics,\npsyc\nhology\n,\nneuroscience,\nand\ncomputer\nscience.\nIn\nthe\nlast\n\fv\ne\nto\nten\ny\nears,\nit\nhas\nattracted\nrapidly\nincreasing\nin\nterest\nin\nthe\nmac\nhine\nlearning\nand\narti\fcial\nin\ntelligence\ncomm\nunities.\nIts\npromise\nis\nb\neguiling|a\nw\na\ny\nof\nprogramming\nagen\nts\nb\ny\nrew\nard\nand\npunishmen\nt\nwithout\nneeding\nto\nsp\necify\nhow\nthe\ntask\nis\nto\nb\ne\nac\nhiev\ned.\nBut\nthere\nare\nformidable\ncomputational\nobstacles\nto\nful\flling\nthe\npromise.\nThis\npap\ner\nsurv\neys\nthe\nhistorical\nbasis\nof\nreinforcemen\nt\nlearning\nand\nsome\nof\nthe\ncurren\nt\nw\nork\nfrom\na\ncomputer\nscience\np\nersp\nectiv\ne.\nW\ne\ngiv\ne\na\nhigh-lev\nel\no\nv\nerview\nof\nthe\n\feld\nand\na\ntaste\nof\nsome\nsp\neci\fc\napproac\nhes.\nIt\nis,\nof\ncourse,\nimp\nossible\nto\nmen\ntion\nall\nof\nthe\nimp\nortan\nt\nw\nork\nin\nthe\n\feld;\nthis\nshould\nnot\nb\ne\ntak\nen\nto\nb\ne\nan\nexhaustiv\ne\naccoun\nt.\nReinforcemen\nt\nlearning\nis\nthe\nproblem\nfaced\nb\ny\nan\nagen\nt\nthat\nm\nust\nlearn\nb\neha\nvior\nthrough\ntrial-and-error\nin\nteractions\nwith\na\ndynamic\nen\nvironmen\nt.\nThe\nw\nork\ndescrib\ned\nhere\nhas\na\nstrong\nfamily\nresem\nblance\nto\nep\non\nymous\nw\nork\nin\npsyc\nhology\n,\nbut\ndi\u000bers\nconsiderably\nin\nthe\ndetails\nand\nin\nthe\nuse\nof\nthe\nw\nord\n\\reinforcemen\nt.\"\nIt\nis\nappropriately\nthough\nt\nof\nas\na\nclass\nof\nproblems,\nrather\nthan\nas\na\nset\nof\ntec\nhniques.\nThere\nare\nt\nw\no\nmain\nstrategies\nfor\nsolving\nreinforcemen\nt-learning\nproblems.\nThe\n\frst\nis\nto\nsearc\nh\nin\nthe\nspace\nof\nb\neha\nviors\nin\norder\nto\n\fnd\none\nthat\np\nerforms\nw\nell\nin\nthe\nen\nvironmen\nt.\nThis\napproac\nh\nhas\nb\neen\ntak\nen\nb\ny\nw\nork\nin\ngenetic\nalgorithms\nand\ngenetic\nprogramming,\nc\n\r\u0001\t\t\u0006\nAI\nAccess\nF\noundation\nand\nMorgan\nKaufmann\nPublishers.\nAll\nrigh\nts\nreserv\ned.\nKaelbling,\nLittman,\n&\nMoore\na\nT\ns\ni\nr\nB\nI\nR\nFigure\n\u0001:\nThe\nstandard\nreinforcemen\nt-learning\nmo\ndel.\nas\nw\nell\nas\nsome\nmore\nno\nv\nel\nsearc\nh\ntec\nhniques\n(Sc\nhmidh\nub\ner,\n\u0001\t\t\u0006).\nThe\nsecond\nis\nto\nuse\nstatistical\ntec\nhniques\nand\ndynamic\nprogramming\nmetho\nds\nto\nestimate\nthe\nutilit\ny\nof\ntaking\nactions\nin\nstates\nof\nthe\nw\norld.\nThis\npap\ner\nis\ndev\noted\nalmost\nen\ntirely\nto\nthe\nsecond\nset\nof\ntec\nhniques\nb\necause\nthey\ntak\ne\nadv\nan\ntage\nof\nthe\nsp\necial\nstructure\nof\nreinforcemen\nt-learning\nproblems\nthat\nis\nnot\na\nv\nailable\nin\noptimization\nproblems\nin\ngeneral.\nIt\nis\nnot\ny\net\nclear\nwhic\nh\nset\nof\napproac\nhes\nis\nb\nest\nin\nwhic\nh\ncircumstances.\nThe\nrest\nof\nthis\nsection\nis\ndev\noted\nto\nestablishing\nnotation\nand\ndescribing\nthe\nbasic\nreinforcemen\nt-learning\nmo\ndel.\nSection\n\u0002\nexplains\nthe\ntrade-o\u000b\nb\net\nw\neen\nexploration\nand\nexploitation\nand\npresen\nts\nsome\nsolutions\nto\nthe\nmost\nbasic\ncase\nof\nreinforcemen\nt-learning\nproblems,\nin\nwhic\nh\nw\ne\nw\nan\nt\nto\nmaximize\nthe\nimmediate\nrew\nard.\nSection\n\u0003\nconsiders\nthe\nmore\ngeneral\nproblem\nin\nwhic\nh\nrew\nards\ncan\nb\ne\ndela\ny\ned\nin\ntime\nfrom\nthe\nactions\nthat\nw\nere\ncrucial\nto\ngaining\nthem.\nSection\n\u0004\nconsiders\nsome\nclassic\nmo\ndel-free\nalgorithms\nfor\nreinforcemen\nt\nlearning\nfrom\ndela\ny\ned\nrew\nard:\nadaptiv\ne\nheuristic\ncritic,\nT\nD\n(\u0015)\nand\nQ-learning.\nSection\n\u0005\ndemonstrates\na\ncon\ntin\nuum\nof\nalgorithms\nthat\nare\nsensitiv\ne\nto\nthe\namoun\nt\nof\ncomputation\nan\nagen\nt\ncan\np\nerform\nb\net\nw\neen\nactual\nsteps\nof\naction\nin\nthe\nen\nvironmen\nt.\nGeneralization|the\ncornerstone\nof\nmainstream\nmac\nhine\nlearning\nresearc\nh|has\nthe\np\noten\ntial\nof\nconsiderably\naiding\nreinforcemen\nt\nlearning,\nas\ndescrib\ned\nin\nSection\n\u0006.\nSection\n\u0007\nconsiders\nthe\nproblems\nthat\narise\nwhen\nthe\nagen\nt\ndo\nes\nnot\nha\nv\ne\ncomplete\np\nerceptual\naccess\nto\nthe\nstate\nof\nthe\nen\nvironmen\nt.\nSection\n\b\ncatalogs\nsome\nof\nreinforcemen\nt\nlearning's\nsuccessful\napplications.\nFinally\n,\nSection\n\t\nconcludes\nwith\nsome\nsp\neculations\nab\nout\nimp\nortan\nt\nop\nen\nproblems\nand\nthe\nfuture\nof\nreinforcemen\nt\nlearning.\n\u0001.\u0001\nReinforcemen\nt-Learning\nMo\ndel\nIn\nthe\nstandard\nreinforcemen\nt-learning\nmo\ndel,\nan\nagen\nt\nis\nconnected\nto\nits\nen\nvironmen\nt\nvia\np\nerception\nand\naction,\nas\ndepicted\nin\nFigure\n\u0001.\nOn\neac\nh\nstep\nof\nin\nteraction\nthe\nagen\nt\nreceiv\nes\nas\ninput,\ni,\nsome\nindication\nof\nthe\ncurren\nt\nstate,\ns,\nof\nthe\nen\nvironmen\nt;\nthe\nagen\nt\nthen\nc\nho\noses\nan\naction,\na,\nto\ngenerate\nas\noutput.\nThe\naction\nc\nhanges\nthe\nstate\nof\nthe\nen\nvironmen\nt,\nand\nthe\nv\nalue\nof\nthis\nstate\ntransition\nis\ncomm\nunicated\nto\nthe\nagen\nt\nthrough\na\nscalar\nr\neinfor\nc\nement\nsignal,\nr\n.\nThe\nagen\nt's\nb\neha\nvior,\nB\n,\nshould\nc\nho\nose\nactions\nthat\ntend\nto\nincrease\nthe\nlong-run\nsum\nof\nv\nalues\nof\nthe\nreinforcemen\nt\nsignal.\nIt\ncan\nlearn\nto\ndo\nthis\no\nv\ner\ntime\nb\ny\nsystematic\ntrial\nand\nerror,\nguided\nb\ny\na\nwide\nv\nariet\ny\nof\nalgorithms\nthat\nare\nthe\nsub\nject\nof\nlater\nsections\nof\nthis\npap\ner.\n\u0002\u0003\b\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nF\normally\n,\nthe\nmo\ndel\nconsists\nof\n\u000f\na\ndiscrete\nset\nof\nen\nvironmen\nt\nstates,\nS\n;\n\u000f\na\ndiscrete\nset\nof\nagen\nt\nactions,\nA;\nand\n\u000f\na\nset\nof\nscalar\nreinforcemen\nt\nsignals;\nt\nypically\nf0;\n\u0001g,\nor\nthe\nreal\nn\num\nb\ners.\nThe\n\fgure\nalso\nincludes\nan\ninput\nfunction\nI\n,\nwhic\nh\ndetermines\nho\nw\nthe\nagen\nt\nviews\nthe\nen\nvironmen\nt\nstate;\nw\ne\nwill\nassume\nthat\nit\nis\nthe\niden\ntit\ny\nfunction\n(that\nis,\nthe\nagen\nt\np\nerceiv\nes\nthe\nexact\nstate\nof\nthe\nen\nvironmen\nt)\nun\ntil\nw\ne\nconsider\npartial\nobserv\nabilit\ny\nin\nSection\n\u0007.\nAn\nin\ntuitiv\ne\nw\na\ny\nto\nunderstand\nthe\nrelation\nb\net\nw\neen\nthe\nagen\nt\nand\nits\nen\nvironmen\nt\nis\nwith\nthe\nfollo\nwing\nexample\ndialogue.\nEn\nvironmen\nt:\nY\nou\nare\nin\nstate\n\u0006\u0005.\nY\nou\nha\nv\ne\n\u0004\np\nossible\nactions.\nAgen\nt:\nI'll\ntak\ne\naction\n\u0002.\nEn\nvironmen\nt:\nY\nou\nreceiv\ned\na\nreinforcemen\nt\nof\n\u0007\nunits.\nY\nou\nare\nno\nw\nin\nstate\n\u0001\u0005.\nY\nou\nha\nv\ne\n\u0002\np\nossible\nactions.\nAgen\nt:\nI'll\ntak\ne\naction\n\u0001.\nEn\nvironmen\nt:\nY\nou\nreceiv\ned\na\nreinforcemen\nt\nof\n-\u0004\nunits.\nY\nou\nare\nno\nw\nin\nstate\n\u0006\u0005.\nY\nou\nha\nv\ne\n\u0004\np\nossible\nactions.\nAgen\nt:\nI'll\ntak\ne\naction\n\u0002.\nEn\nvironmen\nt:\nY\nou\nreceiv\ned\na\nreinforcemen\nt\nof\n\u0005\nunits.\nY\nou\nare\nno\nw\nin\nstate\n\u0004\u0004.\nY\nou\nha\nv\ne\n\u0005\np\nossible\nactions.\n.\n.\n.\n.\n.\n.\nThe\nagen\nt's\njob\nis\nto\n\fnd\na\np\nolicy\n\u0019\n,\nmapping\nstates\nto\nactions,\nthat\nmaximizes\nsome\nlong-run\nmeasure\nof\nreinforcemen\nt.\nW\ne\nexp\nect,\nin\ngeneral,\nthat\nthe\nen\nvironmen\nt\nwill\nb\ne\nnon-deterministic;\nthat\nis,\nthat\ntaking\nthe\nsame\naction\nin\nthe\nsame\nstate\non\nt\nw\no\ndi\u000beren\nt\no\nccasions\nma\ny\nresult\nin\ndi\u000beren\nt\nnext\nstates\nand/or\ndi\u000beren\nt\nreinforcemen\nt\nv\nalues.\nThis\nhapp\nens\nin\nour\nexample\nab\no\nv\ne:\nfrom\nstate\n\u0006\u0005,\napplying\naction\n\u0002\npro\nduces\ndi\u000bering\nrein-\nforcemen\nts\nand\ndi\u000bering\nstates\non\nt\nw\no\no\nccasions.\nHo\nw\nev\ner,\nw\ne\nassume\nthe\nen\nvironmen\nt\nis\nstationary;\nthat\nis,\nthat\nthe\npr\nob\nabilities\nof\nmaking\nstate\ntransitions\nor\nreceiving\nsp\neci\fc\nreinforcemen\nt\nsignals\ndo\nnot\nc\nhange\no\nv\ner\ntime.\n\u0001\nReinforcemen\nt\nlearning\ndi\u000bers\nfrom\nthe\nmore\nwidely\nstudied\nproblem\nof\nsup\nervised\nlearn-\ning\nin\nsev\neral\nw\na\nys.\nThe\nmost\nimp\nortan\nt\ndi\u000berence\nis\nthat\nthere\nis\nno\npresen\ntation\nof\nin-\nput/output\npairs.\nInstead,\nafter\nc\nho\nosing\nan\naction\nthe\nagen\nt\nis\ntold\nthe\nimmediate\nrew\nard\nand\nthe\nsubsequen\nt\nstate,\nbut\nis\nnot\ntold\nwhic\nh\naction\nw\nould\nha\nv\ne\nb\neen\nin\nits\nb\nest\nlong-term\nin\nterests.\nIt\nis\nnecessary\nfor\nthe\nagen\nt\nto\ngather\nuseful\nexp\nerience\nab\nout\nthe\np\nossible\nsystem\nstates,\nactions,\ntransitions\nand\nrew\nards\nactiv\nely\nto\nact\noptimally\n.\nAnother\ndi\u000berence\nfrom\nsup\nervised\nlearning\nis\nthat\non-line\np\nerformance\nis\nimp\nortan\nt:\nthe\nev\naluation\nof\nthe\nsystem\nis\noften\nconcurren\nt\nwith\nlearning.\n\u0001.\nThis\nassumption\nma\ny\nb\ne\ndisapp\noin\nti\nng;\nafter\nall,\nop\neration\nin\nnon-stationary\nen\nvironmen\nts\nis\none\nof\nthe\nmotiv\nations\nfor\nbuildin\ng\nlearning\nsystems.\nIn\nfact,\nman\ny\nof\nthe\nalgorithms\ndescrib\ned\nin\nlater\nsections\nare\ne\u000bectiv\ne\nin\nslo\nwly-v\narying\nnon-stationary\nen\nvironmen\nts,\nbut\nthere\nis\nv\nery\nlittle\ntheoretical\nanalysis\nin\nthis\narea.\n\u0002\u0003\t\nKaelbling,\nLittman,\n&\nMoore\nSome\nasp\nects\nof\nreinforcemen\nt\nlearning\nare\nclosely\nrelated\nto\nsearc\nh\nand\nplanning\nissues\nin\narti\fcial\nin\ntelligence.\nAI\nsearc\nh\nalgorithms\ngenerate\na\nsatisfactory\ntra\njectory\nthrough\na\ngraph\nof\nstates.\nPlanning\nop\nerates\nin\na\nsimilar\nmanner,\nbut\nt\nypically\nwithin\na\nconstruct\nwith\nmore\ncomplexit\ny\nthan\na\ngraph,\nin\nwhic\nh\nstates\nare\nrepresen\nted\nb\ny\ncomp\nositions\nof\nlogical\nexpressions\ninstead\nof\natomic\nsym\nb\nols.\nThese\nAI\nalgorithms\nare\nless\ngeneral\nthan\nthe\nreinforcemen\nt-learning\nmetho\nds,\nin\nthat\nthey\nrequire\na\nprede\fned\nmo\ndel\nof\nstate\ntransitions,\nand\nwith\na\nfew\nexceptions\nassume\ndeterminism.\nOn\nthe\nother\nhand,\nreinforcemen\nt\nlearning,\nat\nleast\nin\nthe\nkind\nof\ndiscrete\ncases\nfor\nwhic\nh\ntheory\nhas\nb\neen\ndev\nelop\ned,\nassumes\nthat\nthe\nen\ntire\nstate\nspace\ncan\nb\ne\nen\numerated\nand\nstored\nin\nmemory|an\nassumption\nto\nwhic\nh\ncon\nv\nen\ntional\nsearc\nh\nalgorithms\nare\nnot\ntied.\n\u0001.\u0002\nMo\ndels\nof\nOptimal\nBeha\nvior\nBefore\nw\ne\ncan\nstart\nthinking\nab\nout\nalgorithms\nfor\nlearning\nto\nb\neha\nv\ne\noptimally\n,\nw\ne\nha\nv\ne\nto\ndecide\nwhat\nour\nmo\ndel\nof\noptimalit\ny\nwill\nb\ne.\nIn\nparticular,\nw\ne\nha\nv\ne\nto\nsp\necify\nho\nw\nthe\nagen\nt\nshould\ntak\ne\nthe\nfuture\nin\nto\naccoun\nt\nin\nthe\ndecisions\nit\nmak\nes\nab\nout\nho\nw\nto\nb\neha\nv\ne\nno\nw.\nThere\nare\nthree\nmo\ndels\nthat\nha\nv\ne\nb\neen\nthe\nsub\nject\nof\nthe\nma\njorit\ny\nof\nw\nork\nin\nthis\narea.\nThe\n\fnite-horizon\nmo\ndel\nis\nthe\neasiest\nto\nthink\nab\nout;\nat\na\ngiv\nen\nmomen\nt\nin\ntime,\nthe\nagen\nt\nshould\noptimize\nits\nexp\nected\nrew\nard\nfor\nthe\nnext\nh\nsteps:\nE\n(\nh\nX\nt=0\nr\nt\n)\n;\nit\nneed\nnot\nw\norry\nab\nout\nwhat\nwill\nhapp\nen\nafter\nthat.\nIn\nthis\nand\nsubsequen\nt\nexpressions,\nr\nt\nrepresen\nts\nthe\nscalar\nrew\nard\nreceiv\ned\nt\nsteps\nin\nto\nthe\nfuture.\nThis\nmo\ndel\ncan\nb\ne\nused\nin\nt\nw\no\nw\na\nys.\nIn\nthe\n\frst,\nthe\nagen\nt\nwill\nha\nv\ne\na\nnon-stationary\np\nolicy;\nthat\nis,\none\nthat\nc\nhanges\no\nv\ner\ntime.\nOn\nits\n\frst\nstep\nit\nwill\ntak\ne\nwhat\nis\ntermed\na\nh-step\noptimal\naction.\nThis\nis\nde\fned\nto\nb\ne\nthe\nb\nest\naction\na\nv\nailable\ngiv\nen\nthat\nit\nhas\nh\nsteps\nremaining\nin\nwhic\nh\nto\nact\nand\ngain\nreinforcemen\nt.\nOn\nthe\nnext\nstep\nit\nwill\ntak\ne\na\n(h\n\u0000\u0001)-step\noptimal\naction,\nand\nso\non,\nun\ntil\nit\n\fnally\ntak\nes\na\n\u0001-step\noptimal\naction\nand\nterminates.\nIn\nthe\nsecond,\nthe\nagen\nt\ndo\nes\nr\ne\nc\ne\nding-horizon\nc\nontr\nol,\nin\nwhic\nh\nit\nalw\na\nys\ntak\nes\nthe\nh-step\noptimal\naction.\nThe\nagen\nt\nalw\na\nys\nacts\naccording\nto\nthe\nsame\np\nolicy\n,\nbut\nthe\nv\nalue\nof\nh\nlimits\nho\nw\nfar\nahead\nit\nlo\noks\nin\nc\nho\nosing\nits\nactions.\nThe\n\fnite-horizon\nmo\ndel\nis\nnot\nalw\na\nys\nappropriate.\nIn\nman\ny\ncases\nw\ne\nma\ny\nnot\nkno\nw\nthe\nprecise\nlength\nof\nthe\nagen\nt's\nlife\nin\nadv\nance.\nThe\nin\fnite-horizon\ndiscoun\nted\nmo\ndel\ntak\nes\nthe\nlong-run\nrew\nard\nof\nthe\nagen\nt\nin\nto\nac-\ncoun\nt,\nbut\nrew\nards\nthat\nare\nreceiv\ned\nin\nthe\nfuture\nare\ngeometrically\ndiscoun\nted\naccording\nto\ndiscoun\nt\nfactor\n\r\n,\n(where\n0\n\u0014\n\r\n<\n\u0001):\nE\n(\n\u0001\nX\nt=0\n\r\nt\nr\nt\n)\n:\nW\ne\ncan\nin\nterpret\n\r\nin\nsev\neral\nw\na\nys.\nIt\ncan\nb\ne\nseen\nas\nan\nin\nterest\nrate,\na\nprobabilit\ny\nof\nliving\nanother\nstep,\nor\nas\na\nmathematical\ntric\nk\nto\nb\nound\nthe\nin\fnite\nsum.\nThe\nmo\ndel\nis\nconceptu-\nally\nsimilar\nto\nreceding-horizon\ncon\ntrol,\nbut\nthe\ndiscoun\nted\nmo\ndel\nis\nmore\nmathematically\ntractable\nthan\nthe\n\fnite-horizon\nmo\ndel.\nThis\nis\na\ndominan\nt\nreason\nfor\nthe\nwide\natten\ntion\nthis\nmo\ndel\nhas\nreceiv\ned.\n\u0002\u00040\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nAnother\noptimalit\ny\ncriterion\nis\nthe\naver\nage-r\newar\nd\nmo\ndel,\nin\nwhic\nh\nthe\nagen\nt\nis\nsupp\nosed\nto\ntak\ne\nactions\nthat\noptimize\nits\nlong-run\na\nv\nerage\nrew\nard:\nlim\nh!\u0001\nE\n(\n\u0001\nh\nh\nX\nt=0\nr\nt\n)\n:\nSuc\nh\na\np\nolicy\nis\nreferred\nto\nas\na\ngain\noptimal\np\nolicy;\nit\ncan\nb\ne\nseen\nas\nthe\nlimiting\ncase\nof\nthe\nin\fnite-horizon\ndiscoun\nted\nmo\ndel\nas\nthe\ndiscoun\nt\nfactor\napproac\nhes\n\u0001\n(Bertsek\nas,\n\u0001\t\t\u0005).\nOne\nproblem\nwith\nthis\ncriterion\nis\nthat\nthere\nis\nno\nw\na\ny\nto\ndistinguish\nb\net\nw\neen\nt\nw\no\np\nolicies,\none\nof\nwhic\nh\ngains\na\nlarge\namoun\nt\nof\nrew\nard\nin\nthe\ninitial\nphases\nand\nthe\nother\nof\nwhic\nh\ndo\nes\nnot.\nRew\nard\ngained\non\nan\ny\ninitial\npre\fx\nof\nthe\nagen\nt's\nlife\nis\no\nv\nershado\nw\ned\nb\ny\nthe\nlong-run\na\nv\nerage\np\nerformance.\nIt\nis\np\nossible\nto\ngeneralize\nthis\nmo\ndel\nso\nthat\nit\ntak\nes\nin\nto\naccoun\nt\nb\noth\nthe\nlong\nrun\na\nv\nerage\nand\nthe\namoun\nt\nof\ninitial\nrew\nard\nthan\ncan\nb\ne\ngained.\nIn\nthe\ngeneralized,\nbias\noptimal\nmo\ndel,\na\np\nolicy\nis\npreferred\nif\nit\nmaximizes\nthe\nlong-run\na\nv\nerage\nand\nties\nare\nbrok\nen\nb\ny\nthe\ninitial\nextra\nrew\nard.\nFigure\n\u0002\ncon\ntrasts\nthese\nmo\ndels\nof\noptimalit\ny\nb\ny\npro\nviding\nan\nen\nvironmen\nt\nin\nwhic\nh\nc\nhanging\nthe\nmo\ndel\nof\noptimalit\ny\nc\nhanges\nthe\noptimal\np\nolicy\n.\nIn\nthis\nexample,\ncircles\nrepresen\nt\nthe\nstates\nof\nthe\nen\nvironmen\nt\nand\narro\nws\nare\nstate\ntransitions.\nThere\nis\nonly\na\nsingle\naction\nc\nhoice\nfrom\nev\nery\nstate\nexcept\nthe\nstart\nstate,\nwhic\nh\nis\nin\nthe\nupp\ner\nleft\nand\nmark\ned\nwith\nan\nincoming\narro\nw.\nAll\nrew\nards\nare\nzero\nexcept\nwhere\nmark\ned.\nUnder\na\n\fnite-horizon\nmo\ndel\nwith\nh\n=\n\u0005,\nthe\nthree\nactions\nyield\nrew\nards\nof\n+\u0006:0,\n+0:0,\nand\n+0:0,\nso\nthe\n\frst\naction\nshould\nb\ne\nc\nhosen;\nunder\nan\nin\fnite-horizon\ndiscoun\nted\nmo\ndel\nwith\n\r\n=\n0:\t,\nthe\nthree\nc\nhoices\nyield\n+\u0001\u0006:\u0002,\n+\u0005\t:0,\nand\n+\u0005\b:\u0005\nso\nthe\nsecond\naction\nshould\nb\ne\nc\nhosen;\nand\nunder\nthe\na\nv\nerage\nrew\nard\nmo\ndel,\nthe\nthird\naction\nshould\nb\ne\nc\nhosen\nsince\nit\nleads\nto\nan\na\nv\nerage\nrew\nard\nof\n+\u0001\u0001.\nIf\nw\ne\nc\nhange\nh\nto\n\u0001000\nand\n\r\nto\n0.\u0002,\nthen\nthe\nsecond\naction\nis\noptimal\nfor\nthe\n\fnite-horizon\nmo\ndel\nand\nthe\n\frst\nfor\nthe\nin\fnite-horizon\ndiscoun\nted\nmo\ndel;\nho\nw\nev\ner,\nthe\na\nv\nerage\nrew\nard\nmo\ndel\nwill\nalw\na\nys\nprefer\nthe\nb\nest\nlong-term\na\nv\nerage.\nSince\nthe\nc\nhoice\nof\noptimalit\ny\nmo\ndel\nand\nparameters\nmatters\nso\nm\nuc\nh,\nit\nis\nimp\nortan\nt\nto\nc\nho\nose\nit\ncarefully\nin\nan\ny\napplication.\nThe\n\fnite-horizon\nmo\ndel\nis\nappropriate\nwhen\nthe\nagen\nt's\nlifetime\nis\nkno\nwn;\none\nim-\np\nortan\nt\nasp\nect\nof\nthis\nmo\ndel\nis\nthat\nas\nthe\nlength\nof\nthe\nremaining\nlifetime\ndecreases,\nthe\nagen\nt's\np\nolicy\nma\ny\nc\nhange.\nA\nsystem\nwith\na\nhard\ndeadline\nw\nould\nb\ne\nappropriately\nmo\ndeled\nthis\nw\na\ny\n.\nThe\nrelativ\ne\nusefulness\nof\nin\fnite-horizon\ndiscoun\nted\nand\nbias-optimal\nmo\ndels\nis\nstill\nunder\ndebate.\nBias-optimalit\ny\nhas\nthe\nadv\nan\ntage\nof\nnot\nrequiring\na\ndiscoun\nt\nparameter;\nho\nw\nev\ner,\nalgorithms\nfor\n\fnding\nbias-optimal\np\nolicies\nare\nnot\ny\net\nas\nw\nell-understo\no\nd\nas\nthose\nfor\n\fnding\noptimal\nin\fnite-horizon\ndiscoun\nted\np\nolicies.\n\u0001.\u0003\nMeasuring\nLearning\nP\nerformance\nThe\ncriteria\ngiv\nen\nin\nthe\nprevious\nsection\ncan\nb\ne\nused\nto\nassess\nthe\np\nolicies\nlearned\nb\ny\na\ngiv\nen\nalgorithm.\nW\ne\nw\nould\nalso\nlik\ne\nto\nb\ne\nable\nto\nev\naluate\nthe\nqualit\ny\nof\nlearning\nitself.\nThere\nare\nsev\neral\nincompatible\nmeasures\nin\nuse.\n\u000f\nEv\nen\ntual\ncon\nv\nergence\nto\noptimal.\nMan\ny\nalgorithms\ncome\nwith\na\npro\nv\nable\nguar-\nan\ntee\nof\nasymptotic\ncon\nv\nergence\nto\noptimal\nb\neha\nvior\n(W\natkins\n&\nDa\ny\nan,\n\u0001\t\t\u0002).\nThis\nis\nreassuring,\nbut\nuseless\nin\npractical\nterms.\nAn\nagen\nt\nthat\nquic\nkly\nreac\nhes\na\nplateau\n\u0002\u0004\u0001\nKaelbling,\nLittman,\n&\nMoore\nFinite horizon, h=4\nInﬁnite horizon, γ=0.9\nAverage reward\n+2\n+10\n+11\nFigure\n\u0002:\nComparing\nmo\ndels\nof\noptimalit\ny\n.\nAll\nunlab\neled\narro\nws\npro\nduce\na\nrew\nard\nof\nzero.\nat\n\t\t%\nof\noptimalit\ny\nma\ny\n,\nin\nman\ny\napplications,\nb\ne\npreferable\nto\nan\nagen\nt\nthat\nhas\na\nguaran\ntee\nof\nev\nen\ntual\noptimalit\ny\nbut\na\nsluggish\nearly\nlearning\nrate.\n\u000f\nSp\need\nof\ncon\nv\nergence\nto\noptimalit\ny\n.\nOptimalit\ny\nis\nusually\nan\nasymptotic\nresult,\nand\nso\ncon\nv\nergence\nsp\need\nis\nan\nill-de\fned\nmeasure.\nMore\npractical\nis\nthe\nsp\ne\ne\nd\nof\nc\nonver\ngenc\ne\nto\nne\nar-optimality.\nThis\nmeasure\nb\negs\nthe\nde\fnition\nof\nho\nw\nnear\nto\noptimalit\ny\nis\nsu\u000ecien\nt.\nA\nrelated\nmeasure\nis\nlevel\nof\np\nerformanc\ne\nafter\na\ngiven\ntime,\nwhic\nh\nsimilarly\nrequires\nthat\nsomeone\nde\fne\nthe\ngiv\nen\ntime.\nIt\nshould\nb\ne\nnoted\nthat\nhere\nw\ne\nha\nv\ne\nanother\ndi\u000berence\nb\net\nw\neen\nreinforcemen\nt\nlearning\nand\ncon\nv\nen\ntional\nsup\nervised\nlearning.\nIn\nthe\nlatter,\nexp\nected\nfuture\npredictiv\ne\naccu-\nracy\nor\nstatistical\ne\u000eciency\nare\nthe\nprime\nconcerns.\nF\nor\nexample,\nin\nthe\nw\nell-kno\nwn\nP\nA\nC\nframew\nork\n(V\nalian\nt,\n\u0001\t\b\u0004),\nthere\nis\na\nlearning\np\nerio\nd\nduring\nwhic\nh\nmistak\nes\ndo\nnot\ncoun\nt,\nthen\na\np\nerformance\np\nerio\nd\nduring\nwhic\nh\nthey\ndo.\nThe\nframew\nork\npro\nvides\nb\nounds\non\nthe\nnecessary\nlength\nof\nthe\nlearning\np\nerio\nd\nin\norder\nto\nha\nv\ne\na\nprobabilistic\nguaran\ntee\non\nthe\nsubsequen\nt\np\nerformance.\nThat\nis\nusually\nan\ninappropriate\nview\nfor\nan\nagen\nt\nwith\na\nlong\nexistence\nin\na\ncomplex\nen\nvironmen\nt.\nIn\nspite\nof\nthe\nmismatc\nh\nb\net\nw\neen\nem\nb\nedded\nreinforcemen\nt\nlearning\nand\nthe\ntrain/test\np\nersp\nectiv\ne,\nFiec\nh\nter\n(\u0001\t\t\u0004)\npro\nvides\na\nP\nA\nC\nanalysis\nfor\nQ-learning\n(describ\ned\nin\nSection\n\u0004.\u0002)\nthat\nsheds\nsome\nligh\nt\non\nthe\nconnection\nb\net\nw\neen\nthe\nt\nw\no\nviews.\nMeasures\nrelated\nto\nsp\need\nof\nlearning\nha\nv\ne\nan\nadditional\nw\neakness.\nAn\nalgorithm\nthat\nmerely\ntries\nto\nac\nhiev\ne\noptimalit\ny\nas\nfast\nas\np\nossible\nma\ny\nincur\nunnecessarily\nlarge\np\nenalties\nduring\nthe\nlearning\np\nerio\nd.\nA\nless\naggressiv\ne\nstrategy\ntaking\nlonger\nto\nac\nhiev\ne\noptimalit\ny\n,\nbut\ngaining\ngreater\ntotal\nreinforcemen\nt\nduring\nits\nlearning\nmigh\nt\nb\ne\npreferable.\n\u000f\nRegret.\nA\nmore\nappropriate\nmeasure,\nthen,\nis\nthe\nexp\nected\ndecrease\nin\nrew\nard\ngained\ndue\nto\nexecuting\nthe\nlearning\nalgorithm\ninstead\nof\nb\neha\nving\noptimally\nfrom\nthe\nv\nery\nb\neginning.\nThis\nmeasure\nis\nkno\nwn\nas\nr\ne\ngr\net\n(Berry\n&\nF\nristedt,\n\u0001\t\b\u0005).\nIt\np\nenalizes\nmistak\nes\nwherev\ner\nthey\no\nccur\nduring\nthe\nrun.\nUnfortunately\n,\nresults\nconcerning\nthe\nregret\nof\nalgorithms\nare\nquite\nhard\nto\nobtain.\n\u0002\u0004\u0002\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n\u0001.\u0004\nReinforcemen\nt\nLearning\nand\nAdaptiv\ne\nCon\ntrol\nAdaptiv\ne\ncon\ntrol\n(Burghes\n&\nGraham,\n\u0001\t\b0;\nStengel,\n\u0001\t\b\u0006)\nis\nalso\nconcerned\nwith\nalgo-\nrithms\nfor\nimpro\nving\na\nsequence\nof\ndecisions\nfrom\nexp\nerience.\nAdaptiv\ne\ncon\ntrol\nis\na\nm\nuc\nh\nmore\nmature\ndiscipline\nthat\nconcerns\nitself\nwith\ndynamic\nsystems\nin\nwhic\nh\nstates\nand\nac-\ntions\nare\nv\nectors\nand\nsystem\ndynamics\nare\nsmo\noth:\nlinear\nor\nlo\ncally\nlinearizable\naround\na\ndesired\ntra\njectory\n.\nA\nv\nery\ncommon\nform\nulation\nof\ncost\nfunctions\nin\nadaptiv\ne\ncon\ntrol\nare\nquadratic\np\nenalties\non\ndeviation\nfrom\ndesired\nstate\nand\naction\nv\nectors.\nMost\nimp\nortan\ntly\n,\nalthough\nthe\ndynamic\nmo\ndel\nof\nthe\nsystem\nis\nnot\nkno\nwn\nin\nadv\nance,\nand\nm\nust\nb\ne\nesti-\nmated\nfrom\ndata,\nthe\nstructur\ne\nof\nthe\ndynamic\nmo\ndel\nis\n\fxed,\nlea\nving\nmo\ndel\nestimation\nas\na\nparameter\nestimation\nproblem.\nThese\nassumptions\np\nermit\ndeep,\nelegan\nt\nand\np\no\nw\nerful\nmathematical\nanalysis,\nwhic\nh\nin\nturn\nlead\nto\nrobust,\npractical,\nand\nwidely\ndeplo\ny\ned\nadaptiv\ne\ncon\ntrol\nalgorithms.\n\u0002.\nExploitation\nv\nersus\nExploration:\nThe\nSingle-State\nCase\nOne\nma\njor\ndi\u000berence\nb\net\nw\neen\nreinforcemen\nt\nlearning\nand\nsup\nervised\nlearning\nis\nthat\na\nreinforcemen\nt-learner\nm\nust\nexplicitly\nexplore\nits\nen\nvironmen\nt.\nIn\norder\nto\nhighligh\nt\nthe\nproblems\nof\nexploration,\nw\ne\ntreat\na\nv\nery\nsimple\ncase\nin\nthis\nsection.\nThe\nfundamen\ntal\nissues\nand\napproac\nhes\ndescrib\ned\nhere\nwill,\nin\nman\ny\ncases,\ntransfer\nto\nthe\nmore\ncomplex\ninstances\nof\nreinforcemen\nt\nlearning\ndiscussed\nlater\nin\nthe\npap\ner.\nThe\nsimplest\np\nossible\nreinforcemen\nt-learning\nproblem\nis\nkno\nwn\nas\nthe\nk\n-armed\nbandit\nproblem,\nwhic\nh\nhas\nb\neen\nthe\nsub\nject\nof\na\ngreat\ndeal\nof\nstudy\nin\nthe\nstatistics\nand\napplied\nmathematics\nliterature\n(Berry\n&\nF\nristedt,\n\u0001\t\b\u0005).\nThe\nagen\nt\nis\nin\na\nro\nom\nwith\na\ncollection\nof\nk\ngam\nbling\nmac\nhines\n(eac\nh\ncalled\na\n\\one-armed\nbandit\"\nin\ncollo\nquial\nEnglish).\nThe\nagen\nt\nis\np\nermitted\na\n\fxed\nn\num\nb\ner\nof\npulls,\nh.\nAn\ny\narm\nma\ny\nb\ne\npulled\non\neac\nh\nturn.\nThe\nmac\nhines\ndo\nnot\nrequire\na\ndep\nosit\nto\npla\ny;\nthe\nonly\ncost\nis\nin\nw\nasting\na\npull\npla\nying\na\nsub\noptimal\nmac\nhine.\nWhen\narm\ni\nis\npulled,\nmac\nhine\ni\npa\nys\no\u000b\n\u0001\nor\n0,\naccording\nto\nsome\nunderlying\nprobabilit\ny\nparameter\np\ni\n,\nwhere\npa\ny\no\u000bs\nare\nindep\nenden\nt\nev\nen\nts\nand\nthe\np\ni\ns\nare\nunkno\nwn.\nWhat\nshould\nthe\nagen\nt's\nstrategy\nb\ne?\nThis\nproblem\nillustrates\nthe\nfundamen\ntal\ntradeo\u000b\nb\net\nw\neen\nexploitation\nand\nexploration.\nThe\nagen\nt\nmigh\nt\nb\neliev\ne\nthat\na\nparticular\narm\nhas\na\nfairly\nhigh\npa\ny\no\u000b\nprobabilit\ny;\nshould\nit\nc\nho\nose\nthat\narm\nall\nthe\ntime,\nor\nshould\nit\nc\nho\nose\nanother\none\nthat\nit\nhas\nless\ninformation\nab\nout,\nbut\nseems\nto\nb\ne\nw\norse?\nAnsw\ners\nto\nthese\nquestions\ndep\nend\non\nho\nw\nlong\nthe\nagen\nt\nis\nexp\nected\nto\npla\ny\nthe\ngame;\nthe\nlonger\nthe\ngame\nlasts,\nthe\nw\norse\nthe\nconsequences\nof\nprematurely\ncon\nv\nerging\non\na\nsub-optimal\narm,\nand\nthe\nmore\nthe\nagen\nt\nshould\nexplore.\nThere\nis\na\nwide\nv\nariet\ny\nof\nsolutions\nto\nthis\nproblem.\nW\ne\nwill\nconsider\na\nrepresen\ntativ\ne\nselection\nof\nthem,\nbut\nfor\na\ndeep\ner\ndiscussion\nand\na\nn\num\nb\ner\nof\nimp\nortan\nt\ntheoretical\nresults,\nsee\nthe\nb\no\nok\nb\ny\nBerry\nand\nF\nristedt\n(\u0001\t\b\u0005).\nW\ne\nuse\nthe\nterm\n\\action\"\nto\nindicate\nthe\nagen\nt's\nc\nhoice\nof\narm\nto\npull.\nThis\neases\nthe\ntransition\nin\nto\ndela\ny\ned\nreinforcemen\nt\nmo\ndels\nin\nSection\n\u0003.\nIt\nis\nv\nery\nimp\nortan\nt\nto\nnote\nthat\nbandit\nproblems\n\ft\nour\nde\fnition\nof\na\nreinforcemen\nt-learning\nen\nvironmen\nt\nwith\na\nsingle\nstate\nwith\nonly\nself\ntransitions.\nSection\n\u0002.\u0001\ndiscusses\nthree\nsolutions\nto\nthe\nbasic\none-state\nbandit\nproblem\nthat\nha\nv\ne\nformal\ncorrectness\nresults.\nAlthough\nthey\ncan\nb\ne\nextended\nto\nproblems\nwith\nreal-v\nalued\nrew\nards,\nthey\ndo\nnot\napply\ndirectly\nto\nthe\ngeneral\nm\nulti-state\ndela\ny\ned-reinforcemen\nt\ncase.\n\u0002\u0004\u0003\nKaelbling,\nLittman,\n&\nMoore\nSection\n\u0002.\u0002\npresen\nts\nthree\ntec\nhniques\nthat\nare\nnot\nformally\njusti\fed,\nbut\nthat\nha\nv\ne\nhad\nwide\nuse\nin\npractice,\nand\ncan\nb\ne\napplied\n(with\nsimilar\nlac\nk\nof\nguaran\ntee)\nto\nthe\ngeneral\ncase.\n\u0002.\u0001\nF\normally\nJusti\fed\nT\nec\nhniques\nThere\nis\na\nfairly\nw\nell-dev\nelop\ned\nformal\ntheory\nof\nexploration\nfor\nv\nery\nsimple\nproblems.\nAlthough\nit\nis\ninstructiv\ne,\nthe\nmetho\nds\nit\npro\nvides\ndo\nnot\nscale\nw\nell\nto\nmore\ncomplex\nproblems.\n\u0002.\u0001.\u0001\nD\nynamic-Pr\nogramming\nAppr\no\na\nch\nIf\nthe\nagen\nt\nis\ngoing\nto\nb\ne\nacting\nfor\na\ntotal\nof\nh\nsteps,\nit\ncan\nuse\nbasic\nBa\ny\nesian\nreasoning\nto\nsolv\ne\nfor\nan\noptimal\nstrategy\n(Berry\n&\nF\nristedt,\n\u0001\t\b\u0005).\nThis\nrequires\nan\nassumed\nprior\njoin\nt\ndistribution\nfor\nthe\nparameters\nfp\ni\ng,\nthe\nmost\nnatural\nof\nwhic\nh\nis\nthat\neac\nh\np\ni\nis\nindep\nenden\ntly\nuniformly\ndistributed\nb\net\nw\neen\n0\nand\n\u0001.\nW\ne\ncompute\na\nmapping\nfrom\nb\nelief\nstates\n(summaries\nof\nthe\nagen\nt's\nexp\neriences\nduring\nthis\nrun)\nto\nactions.\nHere,\na\nb\nelief\nstate\ncan\nb\ne\nrepresen\nted\nas\na\ntabulation\nof\naction\nc\nhoices\nand\npa\ny\no\u000bs:\nfn\n\u0001\n;\nw\n\u0001\n;\nn\n\u0002\n;\nw\n\u0002\n;\n:\n:\n:\n;\nn\nk\n;\nw\nk\ng\ndenotes\na\nstate\nof\npla\ny\nin\nwhic\nh\neac\nh\narm\ni\nhas\nb\neen\npulled\nn\ni\ntimes\nwith\nw\ni\npa\ny\no\u000bs.\nW\ne\nwrite\nV\n\u0003\n(n\n\u0001\n;\nw\n\u0001\n;\n:\n:\n:\n;\nn\nk\n;\nw\nk\n)\nas\nthe\nexp\nected\npa\ny\no\u000b\nremaining,\ngiv\nen\nthat\na\ntotal\nof\nh\npulls\nare\na\nv\nailable,\nand\nw\ne\nuse\nthe\nremaining\npulls\noptimally\n.\nIf\nP\ni\nn\ni\n=\nh,\nthen\nthere\nare\nno\nremaining\npulls,\nand\nV\n\u0003\n(n\n\u0001\n;\nw\n\u0001\n;\n:\n:\n:\n;\nn\nk\n;\nw\nk\n)\n=\n0.\nThis\nis\nthe\nbasis\nof\na\nrecursiv\ne\nde\fnition.\nIf\nw\ne\nkno\nw\nthe\nV\n\u0003\nv\nalue\nfor\nall\nb\nelief\nstates\nwith\nt\npulls\nremaining,\nw\ne\ncan\ncompute\nthe\nV\n\u0003\nv\nalue\nof\nan\ny\nb\nelief\nstate\nwith\nt\n+\n\u0001\npulls\nremaining:\nV\n\u0003\n(n\n\u0001\n;\nw\n\u0001\n;\n:\n:\n:\n;\nn\nk\n;\nw\nk\n)\n=\nmax\ni\nE\n\"\nF\nuture\npa\ny\no\u000b\nif\nagen\nt\ntak\nes\naction\ni,\nthen\nacts\noptimally\nfor\nremaining\npulls\n#\n=\nmax\ni\n \n\u001a\ni\nV\n\u0003\n(n\n\u0001\n;\nw\ni\n;\n:\n:\n:\n;\nn\ni\n+\n\u0001;\nw\ni\n+\n\u0001;\n:\n:\n:\n;\nn\nk\n;\nw\nk\n)+\n(\u0001\n\u0000\u001a\ni\n)V\n\u0003\n(n\n\u0001\n;\nw\ni\n;\n:\n:\n:\n;\nn\ni\n+\n\u0001;\nw\ni\n;\n:\n:\n:\n;\nn\nk\n;\nw\nk\n)\n!\nwhere\n\u001a\ni\nis\nthe\np\nosterior\nsub\njectiv\ne\nprobabilit\ny\nof\naction\ni\npa\nying\no\u000b\ngiv\nen\nn\ni\n,\nw\ni\nand\nour\nprior\nprobabilit\ny\n.\nF\nor\nthe\nuniform\npriors,\nwhic\nh\nresult\nin\na\nb\neta\ndistribution,\n\u001a\ni\n=\n(w\ni\n+\n\u0001)=(n\ni\n+\n\u0002).\nThe\nexp\nense\nof\n\flling\nin\nthe\ntable\nof\nV\n\u0003\nv\nalues\nin\nthis\nw\na\ny\nfor\nall\nattainable\nb\nelief\nstates\nis\nlinear\nin\nthe\nn\num\nb\ner\nof\nb\nelief\nstates\ntimes\nactions,\nand\nth\nus\nexp\nonen\ntial\nin\nthe\nhorizon.\n\u0002.\u0001.\u0002\nGittins\nAlloca\ntion\nIndices\nGittins\ngiv\nes\nan\n\\allo\ncation\nindex\"\nmetho\nd\nfor\n\fnding\nthe\noptimal\nc\nhoice\nof\naction\nat\neac\nh\nstep\nin\nk\n-armed\nbandit\nproblems\n(Gittins,\n\u0001\t\b\t).\nThe\ntec\nhnique\nonly\napplies\nunder\nthe\ndiscoun\nted\nexp\nected\nrew\nard\ncriterion.\nF\nor\neac\nh\naction,\nconsider\nthe\nn\num\nb\ner\nof\ntimes\nit\nhas\nb\neen\nc\nhosen,\nn,\nv\nersus\nthe\nn\num\nb\ner\nof\ntimes\nit\nhas\npaid\no\u000b,\nw\n.\nF\nor\ncertain\ndiscoun\nt\nfactors,\nthere\nare\npublished\ntables\nof\n\\index\nv\nalues,\"\nI\n(n;\nw\n)\nfor\neac\nh\npair\nof\nn\nand\nw\n.\nLo\nok\nup\nthe\nindex\nv\nalue\nfor\neac\nh\naction\ni,\nI\n(n\ni\n;\nw\ni\n).\nIt\nrepresen\nts\na\ncomparativ\ne\nmeasure\nof\nthe\ncom\nbined\nv\nalue\nof\nthe\nexp\nected\npa\ny\no\u000b\nof\naction\ni\n(giv\nen\nits\nhistory\nof\npa\ny\no\u000bs)\nand\nthe\nv\nalue\nof\nthe\ninformation\nthat\nw\ne\nw\nould\nget\nb\ny\nc\nho\nosing\nit.\nGittins\nhas\nsho\nwn\nthat\nc\nho\nosing\nthe\naction\nwith\nthe\nlargest\nindex\nv\nalue\nguaran\ntees\nthe\noptimal\nbalance\nb\net\nw\neen\nexploration\nand\nexploitation.\n\u0002\u0004\u0004\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n1\n2\n3\nN-1\nN\n2N\n2N-1\nN+3\nN+2\nN+1\na = 0\na = 1\nr = 0\nr = 1\n1\n2\n3\nN-1\nN\n2N\n2N-1\nN+3\nN+2\nN+1\na = 0\na = 1\nFigure\n\u0003:\nA\nTsetlin\nautomaton\nwith\n\u0002N\nstates.\nThe\ntop\nro\nw\nsho\nws\nthe\nstate\ntransitions\nthat\nare\nmade\nwhen\nthe\nprevious\naction\nresulted\nin\na\nrew\nard\nof\n\u0001;\nthe\nb\nottom\nro\nw\nsho\nws\ntransitions\nafter\na\nrew\nard\nof\n0.\nIn\nstates\nin\nthe\nleft\nhalf\nof\nthe\n\fgure,\naction\n0\nis\ntak\nen;\nin\nthose\non\nthe\nrigh\nt,\naction\n\u0001\nis\ntak\nen.\nBecause\nof\nthe\nguaran\ntee\nof\noptimal\nexploration\nand\nthe\nsimplicit\ny\nof\nthe\ntec\nhnique\n(giv\nen\nthe\ntable\nof\nindex\nv\nalues),\nthis\napproac\nh\nholds\na\ngreat\ndeal\nof\npromise\nfor\nuse\nin\nmore\ncomplex\napplications.\nThis\nmetho\nd\npro\nv\ned\nuseful\nin\nan\napplication\nto\nrob\notic\nmanipulation\nwith\nimmediate\nrew\nard\n(Salganico\u000b\n&\nUngar,\n\u0001\t\t\u0005).\nUnfortunately\n,\nno\none\nhas\ny\net\nb\neen\nable\nto\n\fnd\nan\nanalog\nof\nindex\nv\nalues\nfor\ndela\ny\ned\nreinforcemen\nt\nproblems.\n\u0002.\u0001.\u0003\nLearning\nA\nutoma\nt\na\nA\nbranc\nh\nof\nthe\ntheory\nof\nadaptiv\ne\ncon\ntrol\nis\ndev\noted\nto\nle\narning\nautomata,\nsurv\ney\ned\nb\ny\nNarendra\nand\nThathac\nhar\n(\u0001\t\b\t),\nwhic\nh\nw\nere\noriginally\ndescrib\ned\nexplicitly\nas\n\fnite\nstate\nautomata.\nThe\nTsetlin\nautomaton\nsho\nwn\nin\nFigure\n\u0003\npro\nvides\nan\nexample\nthat\nsolv\nes\na\n\u0002-armed\nbandit\narbitrarily\nnear\noptimally\nas\nN\napproac\nhes\nin\fnit\ny\n.\nIt\nis\nincon\nv\nenien\nt\nto\ndescrib\ne\nalgorithms\nas\n\fnite-state\nautomata,\nso\na\nmo\nv\ne\nw\nas\nmade\nto\ndescrib\ne\nthe\nin\nternal\nstate\nof\nthe\nagen\nt\nas\na\nprobabilit\ny\ndistribution\naccording\nto\nwhic\nh\nactions\nw\nould\nb\ne\nc\nhosen.\nThe\nprobabilities\nof\ntaking\ndi\u000beren\nt\nactions\nw\nould\nb\ne\nadjusted\naccording\nto\ntheir\nprevious\nsuccesses\nand\nfailures.\nAn\nexample,\nwhic\nh\nstands\namong\na\nset\nof\nalgorithms\nindep\nenden\ntly\ndev\nelop\ned\nin\nthe\nmathematical\npsyc\nhology\nliterature\n(Hilgard\n&\nBo\nw\ner,\n\u0001\t\u0007\u0005),\nis\nthe\nline\nar\nr\newar\nd-inaction\nalgorithm.\nLet\np\ni\nb\ne\nthe\nagen\nt's\nprobabilit\ny\nof\ntaking\naction\ni.\n\u000f\nWhen\naction\na\ni\nsucceeds,\np\ni\n:=\np\ni\n+\n\u000b(\u0001\n\u0000p\ni\n)\np\nj\n:=\np\nj\n\u0000\u000bp\nj\nfor\nj\n\u0006=\ni\n\u000f\nWhen\naction\na\ni\nfails,\np\nj\nremains\nunc\nhanged\n(for\nall\nj\n).\nThis\nalgorithm\ncon\nv\nerges\nwith\nprobabilit\ny\n\u0001\nto\na\nv\nector\ncon\ntaining\na\nsingle\n\u0001\nand\nthe\nrest\n0's\n(c\nho\nosing\na\nparticular\naction\nwith\nprobabilit\ny\n\u0001).\nUnfortunately\n,\nit\ndo\nes\nnot\nalw\na\nys\ncon\nv\nerge\nto\nthe\ncorrect\naction;\nbut\nthe\nprobabilit\ny\nthat\nit\ncon\nv\nerges\nto\nthe\nwrong\none\ncan\nb\ne\nmade\narbitrarily\nsmall\nb\ny\nmaking\n\u000b\nsmall\n(Narendra\n&\nThathac\nhar,\n\u0001\t\u0007\u0004).\nThere\nis\nno\nliterature\non\nthe\nregret\nof\nthis\nalgorithm.\n\u0002\u0004\u0005\nKaelbling,\nLittman,\n&\nMoore\n\u0002.\u0002\nAd-Ho\nc\nT\nec\nhniques\nIn\nreinforcemen\nt-learning\npractice,\nsome\nsimple,\nad\nho\nc\nstrategies\nha\nv\ne\nb\neen\np\nopular.\nThey\nare\nrarely\n,\nif\nev\ner,\nthe\nb\nest\nc\nhoice\nfor\nthe\nmo\ndels\nof\noptimalit\ny\nw\ne\nha\nv\ne\nused,\nbut\nthey\nma\ny\nb\ne\nview\ned\nas\nreasonable,\ncomputationally\ntractable,\nheuristics.\nThrun\n(\u0001\t\t\u0002)\nhas\nsurv\ney\ned\na\nv\nariet\ny\nof\nthese\ntec\nhniques.\n\u0002.\u0002.\u0001\nGreed\ny\nStra\ntegies\nThe\n\frst\nstrategy\nthat\ncomes\nto\nmind\nis\nto\nalw\na\nys\nc\nho\nose\nthe\naction\nwith\nthe\nhighest\nesti-\nmated\npa\ny\no\u000b.\nThe\n\ra\nw\nis\nthat\nearly\nunluc\nky\nsampling\nmigh\nt\nindicate\nthat\nthe\nb\nest\naction's\nrew\nard\nis\nless\nthan\nthe\nrew\nard\nobtained\nfrom\na\nsub\noptimal\naction.\nThe\nsub\noptimal\naction\nwill\nalw\na\nys\nb\ne\npic\nk\ned,\nlea\nving\nthe\ntrue\noptimal\naction\nstarv\ned\nof\ndata\nand\nits\nsup\neriorit\ny\nnev\ner\ndisco\nv\nered.\nAn\nagen\nt\nm\nust\nexplore\nto\nameliorate\nthis\noutcome.\nA\nuseful\nheuristic\nis\noptimism\nin\nthe\nfac\ne\nof\nunc\nertainty\nin\nwhic\nh\nactions\nare\nselected\ngreedily\n,\nbut\nstrongly\noptimistic\nprior\nb\neliefs\nare\nput\non\ntheir\npa\ny\no\u000bs\nso\nthat\nstrong\nnegativ\ne\nevidence\nis\nneeded\nto\neliminate\nan\naction\nfrom\nconsideration.\nThis\nstill\nhas\na\nmeasurable\ndanger\nof\nstarving\nan\noptimal\nbut\nunluc\nky\naction,\nbut\nthe\nrisk\nof\nthis\ncan\nb\ne\nmade\narbitrar-\nily\nsmall.\nT\nec\nhniques\nlik\ne\nthis\nha\nv\ne\nb\neen\nused\nin\nsev\neral\nreinforcemen\nt\nlearning\nalgorithms\nincluding\nthe\nin\nterv\nal\nexploration\nmetho\nd\n(Kaelbling,\n\u0001\t\t\u0003b)\n(describ\ned\nshortly),\nthe\nex-\nplor\nation\nb\nonus\nin\nDyna\n(Sutton,\n\u0001\t\t0),\ncuriosity-driven\nexplor\nation\n(Sc\nhmidh\nub\ner,\n\u0001\t\t\u0001a),\nand\nthe\nexploration\nmec\nhanism\nin\nprioritized\nsw\neeping\n(Mo\nore\n&\nA\ntk\neson,\n\u0001\t\t\u0003).\n\u0002.\u0002.\u0002\nRandomized\nStra\ntegies\nAnother\nsimple\nexploration\nstrategy\nis\nto\ntak\ne\nthe\naction\nwith\nthe\nb\nest\nestimated\nexp\nected\nrew\nard\nb\ny\ndefault,\nbut\nwith\nprobabilit\ny\np,\nc\nho\nose\nan\naction\nat\nrandom.\nSome\nv\nersions\nof\nthis\nstrategy\nstart\nwith\na\nlarge\nv\nalue\nof\np\nto\nencourage\ninitial\nexploration,\nwhic\nh\nis\nslo\nwly\ndecreased.\nAn\nob\njection\nto\nthe\nsimple\nstrategy\nis\nthat\nwhen\nit\nexp\nerimen\nts\nwith\na\nnon-greedy\naction\nit\nis\nno\nmore\nlik\nely\nto\ntry\na\npromising\nalternativ\ne\nthan\na\nclearly\nhop\neless\nalternativ\ne.\nA\nsligh\ntly\nmore\nsophisticated\nstrategy\nis\nBoltzmann\nexplor\nation.\nIn\nthis\ncase,\nthe\nexp\nected\nrew\nard\nfor\ntaking\naction\na,\nE\nR(a)\nis\nused\nto\nc\nho\nose\nan\naction\nprobabilisticall\ny\naccording\nto\nthe\ndistribution\nP\n(a)\n=\ne\nE\nR(a)=T\nP\na\n0\n\u0002A\ne\nE\nR(a\n0\n)=T\n:\nThe\ntemp\ner\natur\ne\nparameter\nT\ncan\nb\ne\ndecreased\no\nv\ner\ntime\nto\ndecrease\nexploration.\nThis\nmetho\nd\nw\norks\nw\nell\nif\nthe\nb\nest\naction\nis\nw\nell\nseparated\nfrom\nthe\nothers,\nbut\nsu\u000bers\nsomewhat\nwhen\nthe\nv\nalues\nof\nthe\nactions\nare\nclose.\nIt\nma\ny\nalso\ncon\nv\nerge\nunnecessarily\nslo\nwly\nunless\nthe\ntemp\nerature\nsc\nhedule\nis\nman\nually\ntuned\nwith\ngreat\ncare.\n\u0002.\u0002.\u0003\nInter\nv\nal-based\nTechniques\nExploration\nis\noften\nmore\ne\u000ecien\nt\nwhen\nit\nis\nbased\non\nsecond-order\ninformation\nab\nout\nthe\ncertain\nt\ny\nor\nv\nariance\nof\nthe\nestimated\nv\nalues\nof\nactions.\nKaelbling's\ninterval\nestimation\nalgorithm\n(\u0001\t\t\u0003b)\nstores\nstatistics\nfor\neac\nh\naction\na\ni\n:\nw\ni\nis\nthe\nn\num\nb\ner\nof\nsuccesses\nand\nn\ni\nthe\nn\num\nb\ner\nof\ntrials.\nAn\naction\nis\nc\nhosen\nb\ny\ncomputing\nthe\nupp\ner\nb\nound\nof\na\n\u000100\n\u0001\n(\u0001\n\u0000\u000b)%\n\u0002\u0004\u0006\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\ncon\fdence\nin\nterv\nal\non\nthe\nsuccess\nprobabilit\ny\nof\neac\nh\naction\nand\nc\nho\nosing\nthe\naction\nwith\nthe\nhighest\nupp\ner\nb\nound.\nSmaller\nv\nalues\nof\nthe\n\u000b\nparameter\nencourage\ngreater\nexploration.\nWhen\npa\ny\no\u000bs\nare\nb\no\nolean,\nthe\nnormal\nappro\nximation\nto\nthe\nbinomial\ndistribution\ncan\nb\ne\nused\nto\nconstruct\nthe\ncon\fdence\nin\nterv\nal\n(though\nthe\nbinomial\nshould\nb\ne\nused\nfor\nsmall\nn).\nOther\npa\ny\no\u000b\ndistributions\ncan\nb\ne\nhandled\nusing\ntheir\nasso\nciated\nstatistics\nor\nwith\nnonparametric\nmetho\nds.\nThe\nmetho\nd\nw\norks\nv\nery\nw\nell\nin\nempirical\ntrials.\nIt\nis\nalso\nrelated\nto\na\ncertain\nclass\nof\nstatistical\ntec\nhniques\nkno\nwn\nas\nexp\neriment\ndesign\nmetho\nds\n(Bo\nx\n&\nDrap\ner,\n\u0001\t\b\u0007),\nwhic\nh\nare\nused\nfor\ncomparing\nm\nultiple\ntreatmen\nts\n(for\nexample,\nfertilizers\nor\ndrugs)\nto\ndetermine\nwhic\nh\ntreatmen\nt\n(if\nan\ny)\nis\nb\nest\nin\nas\nsmall\na\nset\nof\nexp\nerimen\nts\nas\np\nossible.\n\u0002.\u0003\nMore\nGeneral\nProblems\nWhen\nthere\nare\nm\nultiple\nstates,\nbut\nreinforcemen\nt\nis\nstill\nimmediate,\nthen\nan\ny\nof\nthe\nab\no\nv\ne\nsolutions\ncan\nb\ne\nreplicated,\nonce\nfor\neac\nh\nstate.\nHo\nw\nev\ner,\nwhen\ngeneralization\nis\nrequired,\nthese\nsolutions\nm\nust\nb\ne\nin\ntegrated\nwith\ngeneralization\nmetho\nds\n(see\nsection\n\u0006);\nthis\nis\nstraigh\ntforw\nard\nfor\nthe\nsimple\nad-ho\nc\nmetho\nds,\nbut\nit\nis\nnot\nundersto\no\nd\nho\nw\nto\nmain\ntain\ntheoretical\nguaran\ntees.\nMan\ny\nof\nthese\ntec\nhniques\nfo\ncus\non\ncon\nv\nerging\nto\nsome\nregime\nin\nwhic\nh\nexploratory\nactions\nare\ntak\nen\nrarely\nor\nnev\ner;\nthis\nis\nappropriate\nwhen\nthe\nen\nvironmen\nt\nis\nstationary\n.\nHo\nw\nev\ner,\nwhen\nthe\nen\nvironmen\nt\nis\nnon-stationary\n,\nexploration\nm\nust\ncon\ntin\nue\nto\ntak\ne\nplace,\nin\norder\nto\nnotice\nc\nhanges\nin\nthe\nw\norld.\nAgain,\nthe\nmore\nad-ho\nc\ntec\nhniques\ncan\nb\ne\nmo\ndi\fed\nto\ndeal\nwith\nthis\nin\na\nplausible\nmanner\n(k\neep\ntemp\nerature\nparameters\nfrom\ngoing\nto\n0;\ndeca\ny\nthe\nstatistics\nin\nin\nterv\nal\nestimation),\nbut\nnone\nof\nthe\ntheoretically\nguaran\nteed\nmetho\nds\ncan\nb\ne\napplied.\n\u0003.\nDela\ny\ned\nRew\nard\nIn\nthe\ngeneral\ncase\nof\nthe\nreinforcemen\nt\nlearning\nproblem,\nthe\nagen\nt's\nactions\ndetermine\nnot\nonly\nits\nimmediate\nrew\nard,\nbut\nalso\n(at\nleast\nprobabilistically)\nthe\nnext\nstate\nof\nthe\nen\nvironmen\nt.\nSuc\nh\nen\nvironmen\nts\ncan\nb\ne\nthough\nt\nof\nas\nnet\nw\norks\nof\nbandit\nproblems,\nbut\nthe\nagen\nt\nm\nust\ntak\ne\nin\nto\naccoun\nt\nthe\nnext\nstate\nas\nw\nell\nas\nthe\nimmediate\nrew\nard\nwhen\nit\ndecides\nwhic\nh\naction\nto\ntak\ne.\nThe\nmo\ndel\nof\nlong-run\noptimalit\ny\nthe\nagen\nt\nis\nusing\ndetermines\nexactly\nho\nw\nit\nshould\ntak\ne\nthe\nv\nalue\nof\nthe\nfuture\nin\nto\naccoun\nt.\nThe\nagen\nt\nwill\nha\nv\ne\nto\nb\ne\nable\nto\nlearn\nfrom\ndela\ny\ned\nreinforcemen\nt:\nit\nma\ny\ntak\ne\na\nlong\nsequence\nof\nactions,\nreceiving\ninsigni\fcan\nt\nreinforcemen\nt,\nthen\n\fnally\narriv\ne\nat\na\nstate\nwith\nhigh\nreinforcemen\nt.\nThe\nagen\nt\nm\nust\nb\ne\nable\nto\nlearn\nwhic\nh\nof\nits\nactions\nare\ndesirable\nbased\non\nrew\nard\nthat\ncan\ntak\ne\nplace\narbitrarily\nfar\nin\nthe\nfuture.\n\u0003.\u0001\nMark\no\nv\nDecision\nPro\ncesses\nProblems\nwith\ndela\ny\ned\nreinforcemen\nt\nare\nw\nell\nmo\ndeled\nas\nMarkov\nde\ncision\npr\no\nc\nesses\n(MDPs).\nAn\nMDP\nconsists\nof\n\u000f\na\nset\nof\nstates\nS\n,\n\u000f\na\nset\nof\nactions\nA,\n\u0002\u0004\u0007\nKaelbling,\nLittman,\n&\nMoore\n\u000f\na\nrew\nard\nfunction\nR\n:\nS\n\u0002\nA\n!\n<,\nand\n\u000f\na\nstate\ntransition\nfunction\nT\n:\nS\n\u0002\nA\n!\n\u0005(S\n),\nwhere\na\nmem\nb\ner\nof\n\u0005(S\n)\nis\na\nprobabilit\ny\ndistribution\no\nv\ner\nthe\nset\nS\n(i.e.\nit\nmaps\nstates\nto\nprobabilities).\nW\ne\nwrite\nT\n(s;\na;\ns\n0\n)\nfor\nthe\nprobabilit\ny\nof\nmaking\na\ntransition\nfrom\nstate\ns\nto\nstate\ns\n0\nusing\naction\na.\nThe\nstate\ntransition\nfunction\nprobabilistically\nsp\neci\fes\nthe\nnext\nstate\nof\nthe\nen\nvironmen\nt\nas\na\nfunction\nof\nits\ncurren\nt\nstate\nand\nthe\nagen\nt's\naction.\nThe\nrew\nard\nfunction\nsp\neci\fes\nexp\nected\ninstan\ntaneous\nrew\nard\nas\na\nfunction\nof\nthe\ncurren\nt\nstate\nand\naction.\nThe\nmo\ndel\nis\nMarkov\nif\nthe\nstate\ntransitions\nare\nindep\nenden\nt\nof\nan\ny\nprevious\nen\nvironmen\nt\nstates\nor\nagen\nt\nactions.\nThere\nare\nman\ny\ngo\no\nd\nreferences\nto\nMDP\nmo\ndels\n(Bellman,\n\u0001\t\u0005\u0007;\nBertsek\nas,\n\u0001\t\b\u0007;\nHo\nw\nard,\n\u0001\t\u00060;\nPuterman,\n\u0001\t\t\u0004).\nAlthough\ngeneral\nMDPs\nma\ny\nha\nv\ne\nin\fnite\n(ev\nen\nuncoun\ntable)\nstate\nand\naction\nspaces,\nw\ne\nwill\nonly\ndiscuss\nmetho\nds\nfor\nsolving\n\fnite-state\nand\n\fnite-action\nproblems.\nIn\nsection\n\u0006,\nw\ne\ndiscuss\nmetho\nds\nfor\nsolving\nproblems\nwith\ncon\ntin\nuous\ninput\nand\noutput\nspaces.\n\u0003.\u0002\nFinding\na\nP\nolicy\nGiv\nen\na\nMo\ndel\nBefore\nw\ne\nconsider\nalgorithms\nfor\nlearning\nto\nb\neha\nv\ne\nin\nMDP\nen\nvironmen\nts,\nw\ne\nwill\nex-\nplore\ntec\nhniques\nfor\ndetermining\nthe\noptimal\np\nolicy\ngiv\nen\na\ncorrect\nmo\ndel.\nThese\ndynamic\nprogramming\ntec\nhniques\nwill\nserv\ne\nas\nthe\nfoundation\nand\ninspiration\nfor\nthe\nlearning\nal-\ngorithms\nto\nfollo\nw.\nW\ne\nrestrict\nour\natten\ntion\nmainly\nto\n\fnding\noptimal\np\nolicies\nfor\nthe\nin\fnite-horizon\ndiscoun\nted\nmo\ndel,\nbut\nmost\nof\nthese\nalgorithms\nha\nv\ne\nanalogs\nfor\nthe\n\fnite-\nhorizon\nand\na\nv\nerage-case\nmo\ndels\nas\nw\nell.\nW\ne\nrely\non\nthe\nresult\nthat,\nfor\nthe\nin\fnite-horizon\ndiscoun\nted\nmo\ndel,\nthere\nexists\nan\noptimal\ndeterministic\nstationary\np\nolicy\n(Bellman,\n\u0001\t\u0005\u0007).\nW\ne\nwill\nsp\neak\nof\nthe\noptimal\nvalue\nof\na\nstate|it\nis\nthe\nexp\nected\nin\fnite\ndiscoun\nted\nsum\nof\nrew\nard\nthat\nthe\nagen\nt\nwill\ngain\nif\nit\nstarts\nin\nthat\nstate\nand\nexecutes\nthe\noptimal\np\nolicy\n.\nUsing\n\u0019\nas\na\ncomplete\ndecision\np\nolicy\n,\nit\nis\nwritten\nV\n\u0003\n(s)\n=\nmax\n\u0019\nE\n \n\u0001\nX\nt=0\n\r\nt\nr\nt\n!\n:\nThis\noptimal\nv\nalue\nfunction\nis\nunique\nand\ncan\nb\ne\nde\fned\nas\nthe\nsolution\nto\nthe\nsim\nultaneous\nequations\nV\n\u0003\n(s)\n=\nmax\na\n0\n@\nR(s;\na)\n+\n\r\nX\ns\n0\n\u0002S\nT\n(s;\na;\ns\n0\n)V\n\u0003\n(s\n0\n)\n\u0001\nA\n;\n\bs\n\u0002\nS\n;\n(\u0001)\nwhic\nh\nassert\nthat\nthe\nv\nalue\nof\na\nstate\ns\nis\nthe\nexp\nected\ninstan\ntaneous\nrew\nard\nplus\nthe\nexp\nected\ndiscoun\nted\nv\nalue\nof\nthe\nnext\nstate,\nusing\nthe\nb\nest\na\nv\nailable\naction.\nGiv\nen\nthe\noptimal\nv\nalue\nfunction,\nw\ne\ncan\nsp\necify\nthe\noptimal\np\nolicy\nas\n\u0019\n\u0003\n(s)\n=\narg\nmax\na\n0\n@\nR(s;\na)\n+\n\r\nX\ns\n0\n\u0002S\nT\n(s;\na;\ns\n0\n)V\n\u0003\n(s\n0\n)\n\u0001\nA\n:\n\u0003.\u0002.\u0001\nV\nalue\nItera\ntion\nOne\nw\na\ny\n,\nthen,\nto\n\fnd\nan\noptimal\np\nolicy\nis\nto\n\fnd\nthe\noptimal\nv\nalue\nfunction.\nIt\ncan\nb\ne\ndetermined\nb\ny\na\nsimple\niterativ\ne\nalgorithm\ncalled\nvalue\niter\nation\nthat\ncan\nb\ne\nsho\nwn\nto\ncon\nv\nerge\nto\nthe\ncorrect\nV\n\u0003\nv\nalues\n(Bellman,\n\u0001\t\u0005\u0007;\nBertsek\nas,\n\u0001\t\b\u0007).\n\u0002\u0004\b\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\ninitialize\nV\n(s)\narbitrarily\nloop\nuntil\npolicy\ngood\nenough\nloop\nfor\ns\n\u0002\nS\nloop\nfor\na\n\u0002\nA\nQ(s;\na)\n:=\nR(s;\na)\n+\n\r\nP\ns\n0\n\u0002S\nT\n(s;\na;\ns\n0\n)V\n(s\n0\n)\nV\n(s)\n:=\nmax\na\nQ(s;\na)\nend\nloop\nend\nloop\nIt\nis\nnot\nob\nvious\nwhen\nto\nstop\nthe\nv\nalue\niteration\nalgorithm.\nOne\nimp\nortan\nt\nresult\nb\nounds\nthe\np\nerformance\nof\nthe\ncurren\nt\ngreedy\np\nolicy\nas\na\nfunction\nof\nthe\nBel\nlman\nr\nesidual\nof\nthe\ncurren\nt\nv\nalue\nfunction\n(Williams\n&\nBaird,\n\u0001\t\t\u0003b).\nIt\nsa\nys\nthat\nif\nthe\nmaxim\num\ndi\u000berence\nb\net\nw\neen\nt\nw\no\nsuccessiv\ne\nv\nalue\nfunctions\nis\nless\nthan\n\u000f,\nthen\nthe\nv\nalue\nof\nthe\ngreedy\np\nolicy\n,\n(the\np\nolicy\nobtained\nb\ny\nc\nho\nosing,\nin\nev\nery\nstate,\nthe\naction\nthat\nmaximizes\nthe\nestimated\ndiscoun\nted\nrew\nard,\nusing\nthe\ncurren\nt\nestimate\nof\nthe\nv\nalue\nfunction)\ndi\u000bers\nfrom\nthe\nv\nalue\nfunction\nof\nthe\noptimal\np\nolicy\nb\ny\nno\nmore\nthan\n\u0002\u000f\r\n=(\u0001\n\u0000\r\n)\nat\nan\ny\nstate.\nThis\npro\nvides\nan\ne\u000bectiv\ne\nstopping\ncriterion\nfor\nthe\nalgorithm.\nPuterman\n(\u0001\t\t\u0004)\ndiscusses\nanother\nstopping\ncriterion,\nbased\non\nthe\nsp\nan\nsemi-norm,\nwhic\nh\nma\ny\nresult\nin\nearlier\ntermination.\nAnother\nimp\nortan\nt\nresult\nis\nthat\nthe\ngreedy\np\nolicy\nis\nguaran\nteed\nto\nb\ne\noptimal\nin\nsome\n\fnite\nn\num\nb\ner\nof\nsteps\nev\nen\nthough\nthe\nv\nalue\nfunction\nma\ny\nnot\nha\nv\ne\ncon\nv\nerged\n(Bertsek\nas,\n\u0001\t\b\u0007).\nAnd\nin\npractice,\nthe\ngreedy\np\nolicy\nis\noften\noptimal\nlong\nb\nefore\nthe\nv\nalue\nfunction\nhas\ncon\nv\nerged.\nV\nalue\niteration\nis\nv\nery\n\rexible.\nThe\nassignmen\nts\nto\nV\nneed\nnot\nb\ne\ndone\nin\nstrict\norder\nas\nsho\nwn\nab\no\nv\ne,\nbut\ninstead\ncan\no\nccur\nasync\nhronously\nin\nparallel\npro\nvided\nthat\nthe\nv\nalue\nof\nev\nery\nstate\ngets\nup\ndated\nin\fnitely\noften\non\nan\nin\fnite\nrun.\nThese\nissues\nare\ntreated\nextensiv\nely\nb\ny\nBertsek\nas\n(\u0001\t\b\t),\nwho\nalso\npro\nv\nes\ncon\nv\nergence\nresults.\nUp\ndates\nbased\non\nEquation\n\u0001\nare\nkno\nwn\nas\nful\nl\nb\nackups\nsince\nthey\nmak\ne\nuse\nof\ninfor-\nmation\nfrom\nall\np\nossible\nsuccessor\nstates.\nIt\ncan\nb\ne\nsho\nwn\nthat\nup\ndates\nof\nthe\nform\nQ(s;\na)\n:=\nQ(s;\na)\n+\n\u000b(r\n+\n\r\nmax\na\n0\nQ(s\n0\n;\na\n0\n)\n\u0000Q(s;\na))\ncan\nalso\nb\ne\nused\nas\nlong\nas\neac\nh\npairing\nof\na\nand\ns\nis\nup\ndated\nin\fnitely\noften,\ns\n0\nis\nsampled\nfrom\nthe\ndistribution\nT\n(s;\na;\ns\n0\n),\nr\nis\nsampled\nwith\nmean\nR(s;\na)\nand\nb\nounded\nv\nariance,\nand\nthe\nlearning\nrate\n\u000b\nis\ndecreased\nslo\nwly\n.\nThis\nt\nyp\ne\nof\nsample\nb\nackup\n(Singh,\n\u0001\t\t\u0003)\nis\ncritical\nto\nthe\nop\neration\nof\nthe\nmo\ndel-free\nmetho\nds\ndiscussed\nin\nthe\nnext\nsection.\nThe\ncomputational\ncomplexit\ny\nof\nthe\nv\nalue-iteration\nalgorithm\nwith\nfull\nbac\nkups,\np\ner\niteration,\nis\nquadratic\nin\nthe\nn\num\nb\ner\nof\nstates\nand\nlinear\nin\nthe\nn\num\nb\ner\nof\nactions.\nCom-\nmonly\n,\nthe\ntransition\nprobabilities\nT\n(s;\na;\ns\n0\n)\nare\nsparse.\nIf\nthere\nare\non\na\nv\nerage\na\nconstan\nt\nn\num\nb\ner\nof\nnext\nstates\nwith\nnon-zero\nprobabilit\ny\nthen\nthe\ncost\np\ner\niteration\nis\nlinear\nin\nthe\nn\num\nb\ner\nof\nstates\nand\nlinear\nin\nthe\nn\num\nb\ner\nof\nactions.\nThe\nn\num\nb\ner\nof\niterations\nrequired\nto\nreac\nh\nthe\noptimal\nv\nalue\nfunction\nis\np\nolynomial\nin\nthe\nn\num\nb\ner\nof\nstates\nand\nthe\nmagnitude\nof\nthe\nlargest\nrew\nard\nif\nthe\ndiscoun\nt\nfactor\nis\nheld\nconstan\nt.\nHo\nw\nev\ner,\nin\nthe\nw\norst\ncase\nthe\nn\num\nb\ner\nof\niterations\ngro\nws\np\nolynomially\nin\n\u0001=(\u0001\n\u0000\r\n),\nso\nthe\ncon\nv\nergence\nrate\nslo\nws\nconsiderably\nas\nthe\ndiscoun\nt\nfactor\napproac\nhes\n\u0001\n(Littman,\nDean,\n&\nKaelbling,\n\u0001\t\t\u0005b).\n\u0002\u0004\t\nKaelbling,\nLittman,\n&\nMoore\n\u0003.\u0002.\u0002\nPolicy\nItera\ntion\nThe\np\nolicy\niter\nation\nalgorithm\nmanipulates\nthe\np\nolicy\ndirectly\n,\nrather\nthan\n\fnding\nit\nindi-\nrectly\nvia\nthe\noptimal\nv\nalue\nfunction.\nIt\nop\nerates\nas\nfollo\nws:\nchoose\nan\narbitrary\npolicy\n\u0019\n0\nloop\n\u0019\n:=\n\u0019\n0\ncompute\nthe\nvalue\nfunction\nof\npolicy\n\u0019\n:\nsolve\nthe\nlinear\nequations\nV\n\u0019\n(s)\n=\nR(s;\n\u0019\n(s))\n+\n\r\nP\ns\n0\n\u0002S\nT\n(s;\n\u0019\n(s);\ns\n0\n)V\n\u0019\n(s\n0\n)\nimprove\nthe\npolicy\nat\neach\nstate:\n\u0019\n0\n(s)\n:=\narg\nmax\na\n(R(s;\na)\n+\n\r\nP\ns\n0\n\u0002S\nT\n(s;\na;\ns\n0\n)V\n\u0019\n(s\n0\n))\nuntil\n\u0019\n=\n\u0019\n0\nThe\nv\nalue\nfunction\nof\na\np\nolicy\nis\njust\nthe\nexp\nected\nin\fnite\ndiscoun\nted\nrew\nard\nthat\nwill\nb\ne\ngained,\nat\neac\nh\nstate,\nb\ny\nexecuting\nthat\np\nolicy\n.\nIt\ncan\nb\ne\ndetermined\nb\ny\nsolving\na\nset\nof\nlinear\nequations.\nOnce\nw\ne\nkno\nw\nthe\nv\nalue\nof\neac\nh\nstate\nunder\nthe\ncurren\nt\np\nolicy\n,\nw\ne\nconsider\nwhether\nthe\nv\nalue\ncould\nb\ne\nimpro\nv\ned\nb\ny\nc\nhanging\nthe\n\frst\naction\ntak\nen.\nIf\nit\ncan,\nw\ne\nc\nhange\nthe\np\nolicy\nto\ntak\ne\nthe\nnew\naction\nwhenev\ner\nit\nis\nin\nthat\nsituation.\nThis\nstep\nis\nguaran\nteed\nto\nstrictly\nimpro\nv\ne\nthe\np\nerformance\nof\nthe\np\nolicy\n.\nWhen\nno\nimpro\nv\nemen\nts\nare\np\nossible,\nthen\nthe\np\nolicy\nis\nguaran\nteed\nto\nb\ne\noptimal.\nSince\nthere\nare\nat\nmost\njAj\njS\nj\ndistinct\np\nolicies,\nand\nthe\nsequence\nof\np\nolicies\nimpro\nv\nes\nat\neac\nh\nstep,\nthis\nalgorithm\nterminates\nin\nat\nmost\nan\nexp\nonen\ntial\nn\num\nb\ner\nof\niterations\n(Puter-\nman,\n\u0001\t\t\u0004).\nHo\nw\nev\ner,\nit\nis\nan\nimp\nortan\nt\nop\nen\nquestion\nho\nw\nman\ny\niterations\np\nolicy\niteration\ntak\nes\nin\nthe\nw\norst\ncase.\nIt\nis\nkno\nwn\nthat\nthe\nrunning\ntime\nis\npseudop\nolynomial\nand\nthat\nfor\nan\ny\n\fxed\ndiscoun\nt\nfactor,\nthere\nis\na\np\nolynomial\nb\nound\nin\nthe\ntotal\nsize\nof\nthe\nMDP\n(Littman\net\nal.,\n\u0001\t\t\u0005b).\n\u0003.\u0002.\u0003\nEnhancement\nto\nV\nalue\nItera\ntion\nand\nPolicy\nItera\ntion\nIn\npractice,\nv\nalue\niteration\nis\nm\nuc\nh\nfaster\np\ner\niteration,\nbut\np\nolicy\niteration\ntak\nes\nfew\ner\niterations.\nArgumen\nts\nha\nv\ne\nb\neen\nput\nforth\nto\nthe\ne\u000bect\nthat\neac\nh\napproac\nh\nis\nb\netter\nfor\nlarge\nproblems.\nPuterman's\nmo\ndi\fe\nd\np\nolicy\niter\nation\nalgorithm\n(Puterman\n&\nShin,\n\u0001\t\u0007\b)\npro\nvides\na\nmetho\nd\nfor\ntrading\niteration\ntime\nfor\niteration\nimpro\nv\nemen\nt\nin\na\nsmo\nother\nw\na\ny\n.\nThe\nbasic\nidea\nis\nthat\nthe\nexp\nensiv\ne\npart\nof\np\nolicy\niteration\nis\nsolving\nfor\nthe\nexact\nv\nalue\nof\nV\n\u0019\n.\nInstead\nof\n\fnding\nan\nexact\nv\nalue\nfor\nV\n\u0019\n,\nw\ne\ncan\np\nerform\na\nfew\nsteps\nof\na\nmo\ndi\fed\nv\nalue-iteration\nstep\nwhere\nthe\np\nolicy\nis\nheld\n\fxed\no\nv\ner\nsuccessiv\ne\niterations.\nThis\ncan\nb\ne\nsho\nwn\nto\npro\nduce\nan\nappro\nximation\nto\nV\n\u0019\nthat\ncon\nv\nerges\nlinearly\nin\n\r\n.\nIn\npractice,\nthis\ncan\nresult\nin\nsubstan\ntial\nsp\needups.\nSev\neral\nstandard\nn\numerical-analysis\ntec\nhniques\nthat\nsp\need\nthe\ncon\nv\nergence\nof\ndynamic\nprogramming\ncan\nb\ne\nused\nto\naccelerate\nv\nalue\nand\np\nolicy\niteration.\nMultigrid\nmetho\nds\ncan\nb\ne\nused\nto\nquic\nkly\nseed\na\ngo\no\nd\ninitial\nappro\nximation\nto\na\nhigh\nresolution\nv\nalue\nfunction\nb\ny\ninitially\np\nerforming\nv\nalue\niteration\nat\na\ncoarser\nresolution\n(R\n\nude,\n\u0001\t\t\u0003).\nState\naggr\ne-\ngation\nw\norks\nb\ny\ncollapsing\ngroups\nof\nstates\nto\na\nsingle\nmeta-state\nsolving\nthe\nabstracted\nproblem\n(Bertsek\nas\n&\nCasta\n~\nnon,\n\u0001\t\b\t).\n\u0002\u00050\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n\u0003.\u0002.\u0004\nComput\na\ntional\nComplexity\nV\nalue\niteration\nw\norks\nb\ny\npro\nducing\nsuccessiv\ne\nappro\nximations\nof\nthe\noptimal\nv\nalue\nfunction.\nEac\nh\niteration\ncan\nb\ne\np\nerformed\nin\nO\n(jAjjS\nj\n\u0002\n)\nsteps,\nor\nfaster\nif\nthere\nis\nsparsit\ny\nin\nthe\ntransition\nfunction.\nHo\nw\nev\ner,\nthe\nn\num\nb\ner\nof\niterations\nrequired\ncan\ngro\nw\nexp\nonen\ntially\nin\nthe\ndiscoun\nt\nfactor\n(Condon,\n\u0001\t\t\u0002);\nas\nthe\ndiscoun\nt\nfactor\napproac\nhes\n\u0001,\nthe\ndecisions\nm\nust\nb\ne\nbased\non\nresults\nthat\nhapp\nen\nfarther\nand\nfarther\nin\nto\nthe\nfuture.\nIn\npractice,\np\nolicy\niteration\ncon\nv\nerges\nin\nfew\ner\niterations\nthan\nv\nalue\niteration,\nalthough\nthe\np\ner-iteration\ncosts\nof\nO\n(jAjjS\nj\n\u0002\n+\njS\nj\n\u0003\n)\ncan\nb\ne\nprohibitiv\ne.\nThere\nis\nno\nkno\nwn\ntigh\nt\nw\norst-case\nb\nound\na\nv\nailable\nfor\np\nolicy\niteration\n(Littman\net\nal.,\n\u0001\t\t\u0005b).\nMo\ndi\fed\np\nolicy\niteration\n(Puterman\n&\nShin,\n\u0001\t\u0007\b)\nseeks\na\ntrade-o\u000b\nb\net\nw\neen\nc\nheap\nand\ne\u000bectiv\ne\niterations\nand\nis\npreferred\nb\ny\nsome\npractictioners\n(Rust,\n\u0001\t\t\u0006).\nLinear\nprogramming\n(Sc\nhrijv\ner,\n\u0001\t\b\u0006)\nis\nan\nextremely\ngeneral\nproblem,\nand\nMDPs\ncan\nb\ne\nsolv\ned\nb\ny\ngeneral-purp\nose\nlinear-programming\npac\nk\nages\n(Derman,\n\u0001\t\u00070;\nD'Ep\nenoux,\n\u0001\t\u0006\u0003;\nHo\u000bman\n&\nKarp,\n\u0001\t\u0006\u0006).\nAn\nadv\nan\ntage\nof\nthis\napproac\nh\nis\nthat\ncommercial-qualit\ny\nlinear-programming\npac\nk\nages\nare\na\nv\nailable,\nalthough\nthe\ntime\nand\nspace\nrequiremen\nts\ncan\nstill\nb\ne\nquite\nhigh.\nF\nrom\na\ntheoretic\np\nersp\nectiv\ne,\nlinear\nprogramming\nis\nthe\nonly\nkno\nwn\nalgorithm\nthat\ncan\nsolv\ne\nMDPs\nin\np\nolynomial\ntime,\nalthough\nthe\ntheoretically\ne\u000ecien\nt\nalgorithms\nha\nv\ne\nnot\nb\neen\nsho\nwn\nto\nb\ne\ne\u000ecien\nt\nin\npractice.\n\u0004.\nLearning\nan\nOptimal\nP\nolicy:\nMo\ndel-free\nMetho\nds\nIn\nthe\nprevious\nsection\nw\ne\nreview\ned\nmetho\nds\nfor\nobtaining\nan\noptimal\np\nolicy\nfor\nan\nMDP\nassuming\nthat\nw\ne\nalready\nhad\na\nmo\ndel.\nThe\nmo\ndel\nconsists\nof\nkno\nwledge\nof\nthe\nstate\ntran-\nsition\nprobabilit\ny\nfunction\nT\n(s;\na;\ns\n0\n)\nand\nthe\nreinforcemen\nt\nfunction\nR(s;\na).\nReinforcemen\nt\nlearning\nis\nprimarily\nconcerned\nwith\nho\nw\nto\nobtain\nthe\noptimal\np\nolicy\nwhen\nsuc\nh\na\nmo\ndel\nis\nnot\nkno\nwn\nin\nadv\nance.\nThe\nagen\nt\nm\nust\nin\nteract\nwith\nits\nen\nvironmen\nt\ndirectly\nto\nobtain\ninformation\nwhic\nh,\nb\ny\nmeans\nof\nan\nappropriate\nalgorithm,\ncan\nb\ne\npro\ncessed\nto\npro\nduce\nan\noptimal\np\nolicy\n.\nA\nt\nthis\np\noin\nt,\nthere\nare\nt\nw\no\nw\na\nys\nto\npro\nceed.\n\u000f\nMo\ndel-free:\nLearn\na\ncon\ntroller\nwithout\nlearning\na\nmo\ndel.\n\u000f\nMo\ndel-based:\nLearn\na\nmo\ndel,\nand\nuse\nit\nto\nderiv\ne\na\ncon\ntroller.\nWhic\nh\napproac\nh\nis\nb\netter?\nThis\nis\na\nmatter\nof\nsome\ndebate\nin\nthe\nreinforcemen\nt-learning\ncomm\nunit\ny\n.\nA\nn\num\nb\ner\nof\nalgorithms\nha\nv\ne\nb\neen\nprop\nosed\non\nb\noth\nsides.\nThis\nquestion\nalso\napp\nears\nin\nother\n\felds,\nsuc\nh\nas\nadaptiv\ne\ncon\ntrol,\nwhere\nthe\ndic\nhotom\ny\nis\nb\net\nw\neen\ndir\ne\nct\nand\nindir\ne\nct\nadaptiv\ne\ncon\ntrol.\nThis\nsection\nexamines\nmo\ndel-free\nlearning,\nand\nSection\n\u0005\nexamines\nmo\ndel-based\nmeth-\no\nds.\nThe\nbiggest\nproblem\nfacing\na\nreinforcemen\nt-learning\nagen\nt\nis\ntemp\nor\nal\ncr\ne\ndit\nassignment.\nHo\nw\ndo\nw\ne\nkno\nw\nwhether\nthe\naction\njust\ntak\nen\nis\na\ngo\no\nd\none,\nwhen\nit\nmigh\nt\nha\nv\ne\nfar-\nreac\nhing\ne\u000bects?\nOne\nstrategy\nis\nto\nw\nait\nun\ntil\nthe\n\\end\"\nand\nrew\nard\nthe\nactions\ntak\nen\nif\nthe\nresult\nw\nas\ngo\no\nd\nand\npunish\nthem\nif\nthe\nresult\nw\nas\nbad.\nIn\nongoing\ntasks,\nit\nis\ndi\u000ecult\nto\nkno\nw\nwhat\nthe\n\\end\"\nis,\nand\nthis\nmigh\nt\nrequire\na\ngreat\ndeal\nof\nmemory\n.\nInstead,\nw\ne\nwill\nuse\ninsigh\nts\nfrom\nv\nalue\niteration\nto\nadjust\nthe\nestimated\nv\nalue\nof\na\nstate\nbased\non\n\u0002\u0005\u0001\nKaelbling,\nLittman,\n&\nMoore\nAHC\nRL\nv\ns\nr\na\nFigure\n\u0004:\nArc\nhitecture\nfor\nthe\nadaptiv\ne\nheuristic\ncritic.\nthe\nimmediate\nrew\nard\nand\nthe\nestimated\nv\nalue\nof\nthe\nnext\nstate.\nThis\nclass\nof\nalgorithms\nis\nkno\nwn\nas\ntemp\nor\nal\ndi\u000ber\nenc\ne\nmetho\nds\n(Sutton,\n\u0001\t\b\b).\nW\ne\nwill\nconsider\nt\nw\no\ndi\u000beren\nt\ntemp\noral-di\u000berence\nlearning\nstrategies\nfor\nthe\ndiscoun\nted\nin\fnite-horizon\nmo\ndel.\n\u0004.\u0001\nAdaptiv\ne\nHeuristic\nCritic\nand\nTD\n(\u0015)\nThe\nadaptive\nheuristic\ncritic\nalgorithm\nis\nan\nadaptiv\ne\nv\nersion\nof\np\nolicy\niteration\n(Barto,\nSutton,\n&\nAnderson,\n\u0001\t\b\u0003)\nin\nwhic\nh\nthe\nv\nalue-function\ncomputation\nis\nno\nlonger\nimple-\nmen\nted\nb\ny\nsolving\na\nset\nof\nlinear\nequations,\nbut\nis\ninstead\ncomputed\nb\ny\nan\nalgorithm\ncalled\nT\nD\n(0).\nA\nblo\nc\nk\ndiagram\nfor\nthis\napproac\nh\nis\ngiv\nen\nin\nFigure\n\u0004.\nIt\nconsists\nof\nt\nw\no\ncomp\no-\nnen\nts:\na\ncritic\n(lab\neled\nAHC),\nand\na\nreinforcemen\nt-learning\ncomp\nonen\nt\n(lab\neled\nRL).\nThe\nreinforcemen\nt-learning\ncomp\nonen\nt\ncan\nb\ne\nan\ninstance\nof\nan\ny\nof\nthe\nk\n-armed\nbandit\nalgo-\nrithms,\nmo\ndi\fed\nto\ndeal\nwith\nm\nultiple\nstates\nand\nnon-stationary\nrew\nards.\nBut\ninstead\nof\nacting\nto\nmaximize\ninstan\ntaneous\nrew\nard,\nit\nwill\nb\ne\nacting\nto\nmaximize\nthe\nheuristic\nv\nalue,\nv\n,\nthat\nis\ncomputed\nb\ny\nthe\ncritic.\nThe\ncritic\nuses\nthe\nreal\nexternal\nreinforcemen\nt\nsignal\nto\nlearn\nto\nmap\nstates\nto\ntheir\nexp\nected\ndiscoun\nted\nv\nalues\ngiv\nen\nthat\nthe\np\nolicy\nb\neing\nexecuted\nis\nthe\none\ncurren\ntly\ninstan\ntiated\nin\nthe\nRL\ncomp\nonen\nt.\nW\ne\ncan\nsee\nthe\nanalogy\nwith\nmo\ndi\fed\np\nolicy\niteration\nif\nw\ne\nimagine\nthese\ncomp\nonen\nts\nw\norking\nin\nalternation.\nThe\np\nolicy\n\u0019\nimplemen\nted\nb\ny\nRL\nis\n\fxed\nand\nthe\ncritic\nlearns\nthe\nv\nalue\nfunction\nV\n\u0019\nfor\nthat\np\nolicy\n.\nNo\nw\nw\ne\n\fx\nthe\ncritic\nand\nlet\nthe\nRL\ncomp\nonen\nt\nlearn\na\nnew\np\nolicy\n\u0019\n0\nthat\nmaximizes\nthe\nnew\nv\nalue\nfunction,\nand\nso\non.\nIn\nmost\nimplemen\ntations,\nho\nw\nev\ner,\nb\noth\ncomp\nonen\nts\nop\nerate\nsim\nultaneously\n.\nOnly\nthe\nalternating\nimplemen\ntation\ncan\nb\ne\nguaran\nteed\nto\ncon\nv\nerge\nto\nthe\noptimal\np\nolicy\n,\nunder\nappropriate\nconditions.\nWilliams\nand\nBaird\nexplored\nthe\ncon\nv\nergence\nprop\nerties\nof\na\nclass\nof\nAHC-related\nalgorithms\nthey\ncall\n\\incremen\ntal\nv\narian\nts\nof\np\nolicy\niteration\"\n(Williams\n&\nBaird,\n\u0001\t\t\u0003a).\nIt\nremains\nto\nexplain\nho\nw\nthe\ncritic\ncan\nlearn\nthe\nv\nalue\nof\na\np\nolicy\n.\nW\ne\nde\fne\nhs;\na;\nr\n;\ns\n0\ni\nto\nb\ne\nan\nexp\nerienc\ne\ntuple\nsummarizing\na\nsingle\ntransition\nin\nthe\nen\nvironmen\nt.\nHere\ns\nis\nthe\nagen\nt's\nstate\nb\nefore\nthe\ntransition,\na\nis\nits\nc\nhoice\nof\naction,\nr\nthe\ninstan\ntaneous\nrew\nard\nit\nreceiv\nes,\nand\ns\n0\nits\nresulting\nstate.\nThe\nv\nalue\nof\na\np\nolicy\nis\nlearned\nusing\nSutton's\nT\nD\n(0)\nalgorithm\n(Sutton,\n\u0001\t\b\b)\nwhic\nh\nuses\nthe\nup\ndate\nrule\nV\n(s)\n:=\nV\n(s)\n+\n\u000b(r\n+\n\r\nV\n(s\n0\n)\n\u0000V\n(s))\n:\nWhenev\ner\na\nstate\ns\nis\nvisited,\nits\nestimated\nv\nalue\nis\nup\ndated\nto\nb\ne\ncloser\nto\nr\n+\n\r\nV\n(s\n0\n),\nsince\nr\nis\nthe\ninstan\ntaneous\nrew\nard\nreceiv\ned\nand\nV\n(s\n0\n)\nis\nthe\nestimated\nv\nalue\nof\nthe\nactually\no\nccurring\nnext\nstate.\nThis\nis\nanalogous\nto\nthe\nsample-bac\nkup\nrule\nfrom\nv\nalue\niteration|the\nonly\ndi\u000berence\nis\nthat\nthe\nsample\nis\ndra\nwn\nfrom\nthe\nreal\nw\norld\nrather\nthan\nb\ny\nsim\nulating\na\nkno\nwn\nmo\ndel.\nThe\nk\ney\nidea\nis\nthat\nr\n+\n\r\nV\n(s\n0\n)\nis\na\nsample\nof\nthe\nv\nalue\nof\nV\n(s),\nand\nit\nis\n\u0002\u0005\u0002\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nmore\nlik\nely\nto\nb\ne\ncorrect\nb\necause\nit\nincorp\norates\nthe\nreal\nr\n.\nIf\nthe\nlearning\nrate\n\u000b\nis\nadjusted\nprop\nerly\n(it\nm\nust\nb\ne\nslo\nwly\ndecreased)\nand\nthe\np\nolicy\nis\nheld\n\fxed,\nT\nD\n(0)\nis\nguaran\nteed\nto\ncon\nv\nerge\nto\nthe\noptimal\nv\nalue\nfunction.\nThe\nT\nD\n(0)\nrule\nas\npresen\nted\nab\no\nv\ne\nis\nreally\nan\ninstance\nof\na\nmore\ngeneral\nclass\nof\nalgorithms\ncalled\nT\nD\n(\u0015),\nwith\n\u0015\n=\n0.\nT\nD\n(0)\nlo\noks\nonly\none\nstep\nahead\nwhen\nadjusting\nv\nalue\nestimates;\nalthough\nit\nwill\nev\nen\ntually\narriv\ne\nat\nthe\ncorrect\nansw\ner,\nit\ncan\ntak\ne\nquite\na\nwhile\nto\ndo\nso.\nThe\ngeneral\nT\nD\n(\u0015)\nrule\nis\nsimilar\nto\nthe\nT\nD\n(0)\nrule\ngiv\nen\nab\no\nv\ne,\nV\n(u)\n:=\nV\n(u)\n+\n\u000b(r\n+\n\r\nV\n(s\n0\n)\n\u0000V\n(s))e(u)\n;\nbut\nit\nis\napplied\nto\nevery\nstate\naccording\nto\nits\neligibili\nt\ny\ne(u),\nrather\nthan\njust\nto\nthe\nimmediately\nprevious\nstate,\ns.\nOne\nv\nersion\nof\nthe\neligibil\nit\ny\ntrace\nis\nde\fned\nto\nb\ne\ne(s)\n=\nt\nX\nk\n=\u0001\n(\u0015\r\n)\nt\u0000k\n\u000e\ns;s\nk\n,\nwhere\n\u000e\ns;s\nk\n=\n(\n\u0001\nif\ns\n=\ns\nk\n0\notherwise\n.\nThe\neligibili\nt\ny\nof\na\nstate\ns\nis\nthe\ndegree\nto\nwhic\nh\nit\nhas\nb\neen\nvisited\nin\nthe\nrecen\nt\npast;\nwhen\na\nreinforcemen\nt\nis\nreceiv\ned,\nit\nis\nused\nto\nup\ndate\nall\nthe\nstates\nthat\nha\nv\ne\nb\neen\nrecen\ntly\nvisited,\naccording\nto\ntheir\neligibili\nt\ny\n.\nWhen\n\u0015\n=\n0\nthis\nis\nequiv\nalen\nt\nto\nT\nD\n(0).\nWhen\n\u0015\n=\n\u0001,\nit\nis\nroughly\nequiv\nalen\nt\nto\nup\ndating\nall\nthe\nstates\naccording\nto\nthe\nn\num\nb\ner\nof\ntimes\nthey\nw\nere\nvisited\nb\ny\nthe\nend\nof\na\nrun.\nNote\nthat\nw\ne\ncan\nup\ndate\nthe\neligibili\nt\ny\nonline\nas\nfollo\nws:\ne(s)\n:=\n(\n\r\n\u0015e(s)\n+\n\u0001\nif\ns\n=\ncurren\nt\nstate\n\r\n\u0015e(s)\notherwise\n.\nIt\nis\ncomputationally\nmore\nexp\nensiv\ne\nto\nexecute\nthe\ngeneral\nT\nD\n(\u0015),\nthough\nit\noften\ncon\nv\nerges\nconsiderably\nfaster\nfor\nlarge\n\u0015\n(Da\ny\nan,\n\u0001\t\t\u0002;\nDa\ny\nan\n&\nSejno\nwski,\n\u0001\t\t\u0004).\nThere\nhas\nb\neen\nsome\nrecen\nt\nw\nork\non\nmaking\nthe\nup\ndates\nmore\ne\u000ecien\nt\n(Cic\nhosz\n&\nMula\nwk\na,\n\u0001\t\t\u0005)\nand\non\nc\nhanging\nthe\nde\fnition\nto\nmak\ne\nT\nD\n(\u0015)\nmore\nconsisten\nt\nwith\nthe\ncertain\nt\ny-equiv\nalen\nt\nmetho\nd\n(Singh\n&\nSutton,\n\u0001\t\t\u0006),\nwhic\nh\nis\ndiscussed\nin\nSection\n\u0005.\u0001.\n\u0004.\u0002\nQ-learning\nThe\nw\nork\nof\nthe\nt\nw\no\ncomp\nonen\nts\nof\nAHC\ncan\nb\ne\naccomplished\nin\na\nuni\fed\nmanner\nb\ny\nW\natkins'\nQ-learning\nalgorithm\n(W\natkins,\n\u0001\t\b\t;\nW\natkins\n&\nDa\ny\nan,\n\u0001\t\t\u0002).\nQ-learning\nis\nt\nypically\neasier\nto\nimplemen\nt.\nIn\norder\nto\nunderstand\nQ-learning,\nw\ne\nha\nv\ne\nto\ndev\nelop\nsome\nadditional\nnotation.\nLet\nQ\n\u0003\n(s;\na)\nb\ne\nthe\nexp\nected\ndiscoun\nted\nreinforcemen\nt\nof\ntaking\naction\na\nin\nstate\ns,\nthen\ncon\ntin\nuing\nb\ny\nc\nho\nosing\nactions\noptimally\n.\nNote\nthat\nV\n\u0003\n(s)\nis\nthe\nv\nalue\nof\ns\nassuming\nthe\nb\nest\naction\nis\ntak\nen\ninitially\n,\nand\nso\nV\n\u0003\n(s)\n=\nmax\na\nQ\n\u0003\n(s;\na).\nQ\n\u0003\n(s;\na)\ncan\nhence\nb\ne\nwritten\nrecursiv\nely\nas\nQ\n\u0003\n(s;\na)\n=\nR(s;\na)\n+\n\r\nX\ns\n0\n\u0002S\nT\n(s;\na;\ns\n0\n)\nmax\na\n0\nQ\n\u0003\n(s\n0\n;\na\n0\n)\n:\nNote\nalso\nthat,\nsince\nV\n\u0003\n(s)\n=\nmax\na\nQ\n\u0003\n(s;\na),\nw\ne\nha\nv\ne\n\u0019\n\u0003\n(s)\n=\narg\nmax\na\nQ\n\u0003\n(s;\na)\nas\nan\noptimal\np\nolicy\n.\nBecause\nthe\nQ\nfunction\nmak\nes\nthe\naction\nexplicit,\nw\ne\ncan\nestimate\nthe\nQ\nv\nalues\non-\nline\nusing\na\nmetho\nd\nessen\ntially\nthe\nsame\nas\nT\nD\n(0),\nbut\nalso\nuse\nthem\nto\nde\fne\nthe\np\nolicy\n,\n\u0002\u0005\u0003\nKaelbling,\nLittman,\n&\nMoore\nb\necause\nan\naction\ncan\nb\ne\nc\nhosen\njust\nb\ny\ntaking\nthe\none\nwith\nthe\nmaxim\num\nQ\nv\nalue\nfor\nthe\ncurren\nt\nstate.\nThe\nQ-learning\nrule\nis\nQ(s;\na)\n:=\nQ(s;\na)\n+\n\u000b(r\n+\n\r\nmax\na\n0\nQ(s\n0\n;\na\n0\n)\n\u0000Q(s;\na))\n;\nwhere\nhs;\na;\nr\n;\ns\n0\ni\nis\nan\nexp\nerience\ntuple\nas\ndescrib\ned\nearlier.\nIf\neac\nh\naction\nis\nexecuted\nin\neac\nh\nstate\nan\nin\fnite\nn\num\nb\ner\nof\ntimes\non\nan\nin\fnite\nrun\nand\n\u000b\nis\ndeca\ny\ned\nappropriately\n,\nthe\nQ\nv\nalues\nwill\ncon\nv\nerge\nwith\nprobabilit\ny\n\u0001\nto\nQ\n\u0003\n(W\natkins,\n\u0001\t\b\t;\nTsitsiklis,\n\u0001\t\t\u0004;\nJaakk\nola,\nJordan,\n&\nSingh,\n\u0001\t\t\u0004).\nQ-learning\ncan\nalso\nb\ne\nextended\nto\nup\ndate\nstates\nthat\no\nccurred\nmore\nthan\none\nstep\npreviously\n,\nas\nin\nT\nD\n(\u0015)\n(P\neng\n&\nWilliams,\n\u0001\t\t\u0004).\nWhen\nthe\nQ\nv\nalues\nare\nnearly\ncon\nv\nerged\nto\ntheir\noptimal\nv\nalues,\nit\nis\nappropriate\nfor\nthe\nagen\nt\nto\nact\ngreedily\n,\ntaking,\nin\neac\nh\nsituation,\nthe\naction\nwith\nthe\nhighest\nQ\nv\nalue.\nDuring\nlearning,\nho\nw\nev\ner,\nthere\nis\na\ndi\u000ecult\nexploitation\nv\nersus\nexploration\ntrade-o\u000b\nto\nb\ne\nmade.\nThere\nare\nno\ngo\no\nd,\nformally\njusti\fed\napproac\nhes\nto\nthis\nproblem\nin\nthe\ngeneral\ncase;\nstandard\npractice\nis\nto\nadopt\none\nof\nthe\nad\nho\nc\nmetho\nds\ndiscussed\nin\nsection\n\u0002.\u0002.\nAHC\narc\nhitectures\nseem\nto\nb\ne\nmore\ndi\u000ecult\nto\nw\nork\nwith\nthan\nQ-learning\non\na\npractical\nlev\nel.\nIt\ncan\nb\ne\nhard\nto\nget\nthe\nrelativ\ne\nlearning\nrates\nrigh\nt\nin\nAHC\nso\nthat\nthe\nt\nw\no\ncomp\nonen\nts\ncon\nv\nerge\ntogether.\nIn\naddition,\nQ-learning\nis\nexplor\nation\ninsensitive:\nthat\nis,\nthat\nthe\nQ\nv\nalues\nwill\ncon\nv\nerge\nto\nthe\noptimal\nv\nalues,\nindep\nenden\nt\nof\nho\nw\nthe\nagen\nt\nb\neha\nv\nes\nwhile\nthe\ndata\nis\nb\neing\ncollected\n(as\nlong\nas\nall\nstate-action\npairs\nare\ntried\noften\nenough).\nThis\nmeans\nthat,\nalthough\nthe\nexploration-exploitation\nissue\nm\nust\nb\ne\naddressed\nin\nQ-learning,\nthe\ndetails\nof\nthe\nexploration\nstrategy\nwill\nnot\na\u000bect\nthe\ncon\nv\nergence\nof\nthe\nlearning\nalgorithm.\nF\nor\nthese\nreasons,\nQ-learning\nis\nthe\nmost\np\nopular\nand\nseems\nto\nb\ne\nthe\nmost\ne\u000bectiv\ne\nmo\ndel-free\nalgorithm\nfor\nlearning\nfrom\ndela\ny\ned\nreinforcemen\nt.\nIt\ndo\nes\nnot,\nho\nw\nev\ner,\naddress\nan\ny\nof\nthe\nissues\nin\nv\nolv\ned\nin\ngeneralizing\no\nv\ner\nlarge\nstate\nand/or\naction\nspaces.\nIn\naddition,\nit\nma\ny\ncon\nv\nerge\nquite\nslo\nwly\nto\na\ngo\no\nd\np\nolicy\n.\n\u0004.\u0003\nMo\ndel-free\nLearning\nWith\nAv\nerage\nRew\nard\nAs\ndescrib\ned,\nQ-learning\ncan\nb\ne\napplied\nto\ndiscoun\nted\nin\fnite-horizon\nMDPs.\nIt\ncan\nalso\nb\ne\napplied\nto\nundiscoun\nted\nproblems\nas\nlong\nas\nthe\noptimal\np\nolicy\nis\nguaran\nteed\nto\nreac\nh\na\nrew\nard-free\nabsorbing\nstate\nand\nthe\nstate\nis\np\nerio\ndicall\ny\nreset.\nSc\nh\nw\nartz\n(\u0001\t\t\u0003)\nexamined\nthe\nproblem\nof\nadapting\nQ-learning\nto\nan\na\nv\nerage-rew\nard\nframew\nork.\nAlthough\nhis\nR-learning\nalgorithm\nseems\nto\nexhibit\ncon\nv\nergence\nproblems\nfor\nsome\nMDPs,\nsev\neral\nresearc\nhers\nha\nv\ne\nfound\nthe\na\nv\nerage-rew\nard\ncriterion\ncloser\nto\nthe\ntrue\nproblem\nthey\nwish\nto\nsolv\ne\nthan\na\ndiscoun\nted\ncriterion\nand\ntherefore\nprefer\nR-learning\nto\nQ-learning\n(Mahadev\nan,\n\u0001\t\t\u0004).\nWith\nthat\nin\nmind,\nresearc\nhers\nha\nv\ne\nstudied\nthe\nproblem\nof\nlearning\noptimal\na\nv\nerage-\nrew\nard\np\nolicies.\nMahadev\nan\n(\u0001\t\t\u0006)\nsurv\ney\ned\nmo\ndel-based\na\nv\nerage-rew\nard\nalgorithms\nfrom\na\nreinforcemen\nt-learning\np\nersp\nectiv\ne\nand\nfound\nsev\neral\ndi\u000eculties\nwith\nexisting\nalgorithms.\nIn\nparticular,\nhe\nsho\nw\ned\nthat\nexisting\nreinforcemen\nt-learning\nalgorithms\nfor\na\nv\nerage\nrew\nard\n(and\nsome\ndynamic\nprogramming\nalgorithms)\ndo\nnot\nalw\na\nys\npro\nduce\nbias-optimal\np\noli-\ncies.\nJaakk\nola,\nJordan\nand\nSingh\n(\u0001\t\t\u0005)\ndescrib\ned\nan\na\nv\nerage-rew\nard\nlearning\nalgorithm\nwith\nguaran\nteed\ncon\nv\nergence\nprop\nerties.\nIt\nuses\na\nMon\nte-Carlo\ncomp\nonen\nt\nto\nestimate\nthe\nexp\nected\nfuture\nrew\nard\nfor\neac\nh\nstate\nas\nthe\nagen\nt\nmo\nv\nes\nthrough\nthe\nen\nvironmen\nt.\nIn\n\u0002\u0005\u0004\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\naddition,\nBertsek\nas\npresen\nts\na\nQ-learning-lik\ne\nalgorithm\nfor\na\nv\nerage-case\nrew\nard\nin\nhis\nnew\ntextb\no\nok\n(\u0001\t\t\u0005).\nAlthough\nthis\nrecen\nt\nw\nork\npro\nvides\na\nm\nuc\nh\nneeded\ntheoretical\nfoundation\nto\nthis\narea\nof\nreinforcemen\nt\nlearning,\nman\ny\nimp\nortan\nt\nproblems\nremain\nunsolv\ned.\n\u0005.\nComputing\nOptimal\nP\nolicies\nb\ny\nLearning\nMo\ndels\nThe\nprevious\nsection\nsho\nw\ned\nho\nw\nit\nis\np\nossible\nto\nlearn\nan\noptimal\np\nolicy\nwithout\nkno\nwing\nthe\nmo\ndels\nT\n(s;\na;\ns\n0\n)\nor\nR(s;\na)\nand\nwithout\nev\nen\nlearning\nthose\nmo\ndels\nen\nroute.\nAlthough\nman\ny\nof\nthese\nmetho\nds\nare\nguaran\nteed\nto\n\fnd\noptimal\np\nolicies\nev\nen\ntually\nand\nuse\nv\nery\nlittle\ncomputation\ntime\np\ner\nexp\nerience,\nthey\nmak\ne\nextremely\nine\u000ecien\nt\nuse\nof\nthe\ndata\nthey\ngather\nand\ntherefore\noften\nrequire\na\ngreat\ndeal\nof\nexp\nerience\nto\nac\nhiev\ne\ngo\no\nd\np\nerformance.\nIn\nthis\nsection\nw\ne\nstill\nb\negin\nb\ny\nassuming\nthat\nw\ne\ndon't\nkno\nw\nthe\nmo\ndels\nin\nadv\nance,\nbut\nw\ne\nexamine\nalgorithms\nthat\ndo\nop\nerate\nb\ny\nlearning\nthese\nmo\ndels.\nThese\nalgorithms\nare\nesp\necially\nimp\nortan\nt\nin\napplications\nin\nwhic\nh\ncomputation\nis\nconsidered\nto\nb\ne\nc\nheap\nand\nreal-w\norld\nexp\nerience\ncostly\n.\n\u0005.\u0001\nCertain\nt\ny\nEquiv\nalen\nt\nMetho\nds\nW\ne\nb\negin\nwith\nthe\nmost\nconceptually\nstraigh\ntforw\nard\nmetho\nd:\n\frst,\nlearn\nthe\nT\nand\nR\nfunctions\nb\ny\nexploring\nthe\nen\nvironmen\nt\nand\nk\neeping\nstatistics\nab\nout\nthe\nresults\nof\neac\nh\naction;\nnext,\ncompute\nan\noptimal\np\nolicy\nusing\none\nof\nthe\nmetho\nds\nof\nSection\n\u0003.\nThis\nmetho\nd\nis\nkno\nwn\nas\nc\nertainty\ne\nquivlanc\ne\n(Kumar\n&\nV\naraiy\na,\n\u0001\t\b\u0006).\nThere\nare\nsome\nserious\nob\njections\nto\nthis\nmetho\nd:\n\u000f\nIt\nmak\nes\nan\narbitrary\ndivision\nb\net\nw\neen\nthe\nlearning\nphase\nand\nthe\nacting\nphase.\n\u000f\nHo\nw\nshould\nit\ngather\ndata\nab\nout\nthe\nen\nvironmen\nt\ninitially?\nRandom\nexploration\nmigh\nt\nb\ne\ndangerous,\nand\nin\nsome\nen\nvironmen\nts\nis\nan\nimmensely\nine\u000ecien\nt\nmetho\nd\nof\ngathering\ndata,\nrequiring\nexp\nonen\ntially\nmore\ndata\n(Whitehead,\n\u0001\t\t\u0001)\nthan\na\nsystem\nthat\nin\nterlea\nv\nes\nexp\nerience\ngathering\nwith\np\nolicy-buil\ndin\ng\nmore\ntigh\ntly\n(Ko\nenig\n&\nSimmons,\n\u0001\t\t\u0003).\nSee\nFigure\n\u0005\nfor\nan\nexample.\n\u000f\nThe\np\nossibilit\ny\nof\nc\nhanges\nin\nthe\nen\nvironmen\nt\nis\nalso\nproblematic.\nBreaking\nup\nan\nagen\nt's\nlife\nin\nto\na\npure\nlearning\nand\na\npure\nacting\nphase\nhas\na\nconsiderable\nrisk\nthat\nthe\noptimal\ncon\ntroller\nbased\non\nearly\nlife\nb\necomes,\nwithout\ndetection,\na\nsub\noptimal\ncon\ntroller\nif\nthe\nen\nvironmen\nt\nc\nhanges.\nA\nv\nariation\non\nthis\nidea\nis\nc\nertainty\ne\nquivalenc\ne,\nin\nwhic\nh\nthe\nmo\ndel\nis\nlearned\ncon\ntin\nually\nthrough\nthe\nagen\nt's\nlifetime\nand,\nat\neac\nh\nstep,\nthe\ncurren\nt\nmo\ndel\nis\nused\nto\ncompute\nan\noptimal\np\nolicy\nand\nv\nalue\nfunction.\nThis\nmetho\nd\nmak\nes\nv\nery\ne\u000bectiv\ne\nuse\nof\na\nv\nailable\ndata,\nbut\nstill\nignores\nthe\nquestion\nof\nexploration\nand\nis\nextremely\ncomputationally\ndemanding,\nev\nen\nfor\nfairly\nsmall\nstate\nspaces.\nF\nortunately\n,\nthere\nare\na\nn\num\nb\ner\nof\nother\nmo\ndel-based\nalgorithms\nthat\nare\nmore\npractical.\n\u0005.\u0002\nDyna\nSutton's\nDyna\narc\nhitecture\n(\u0001\t\t0,\n\u0001\t\t\u0001)\nexploits\na\nmiddle\nground,\nyielding\nstrategies\nthat\nare\nb\noth\nmore\ne\u000bectiv\ne\nthan\nmo\ndel-free\nlearning\nand\nmore\ncomputationally\ne\u000ecien\nt\nthan\n\u0002\u0005\u0005\nKaelbling,\nLittman,\n&\nMoore\n. . . . . . . \nGoal\n1\n2\n3\nn\nFigure\n\u0005:\nIn\nthis\nen\nvironmen\nt,\ndue\nto\nWhitehead\n(\u0001\t\t\u0001),\nrandom\nexploration\nw\nould\ntak\ne\ntak\ne\nO\n(\u0002\nn\n)\nsteps\nto\nreac\nh\nthe\ngoal\nev\nen\nonce,\nwhereas\na\nmore\nin\ntelligen\nt\nexplo-\nration\nstrategy\n(e.g.\n\\assume\nan\ny\nun\ntried\naction\nleads\ndirectly\nto\ngoal\")\nw\nould\nrequire\nonly\nO\n(n\n\u0002\n)\nsteps.\nthe\ncertain\nt\ny-equiv\nalence\napproac\nh.\nIt\nsim\nultaneously\nuses\nexp\nerience\nto\nbuild\na\nmo\ndel\n(\n^\nT\nand\n^\nR),\nuses\nexp\nerience\nto\nadjust\nthe\np\nolicy\n,\nand\nuses\nthe\nmo\ndel\nto\nadjust\nthe\np\nolicy\n.\nDyna\nop\nerates\nin\na\nlo\nop\nof\nin\nteraction\nwith\nthe\nen\nvironmen\nt.\nGiv\nen\nan\nexp\nerience\ntuple\nhs;\na;\ns\n0\n;\nr\ni,\nit\nb\neha\nv\nes\nas\nfollo\nws:\n\u000f\nUp\ndate\nthe\nmo\ndel,\nincremen\nting\nstatistics\nfor\nthe\ntransition\nfrom\ns\nto\ns\n0\non\naction\na\nand\nfor\nreceiving\nrew\nard\nr\nfor\ntaking\naction\na\nin\nstate\ns.\nThe\nup\ndated\nmo\ndels\nare\n^\nT\nand\n^\nR.\n\u000f\nUp\ndate\nthe\np\nolicy\nat\nstate\ns\nbased\non\nthe\nnewly\nup\ndated\nmo\ndel\nusing\nthe\nrule\nQ(s;\na)\n:=\n^\nR(s;\na)\n+\n\r\nX\ns\n0\n^\nT\n(s;\na;\ns\n0\n)\nmax\na\n0\nQ(s\n0\n;\na\n0\n)\n;\nwhic\nh\nis\na\nv\nersion\nof\nthe\nv\nalue-iteration\nup\ndate\nfor\nQ\nv\nalues.\n\u000f\nP\nerform\nk\nadditional\nup\ndates:\nc\nho\nose\nk\nstate-action\npairs\nat\nrandom\nand\nup\ndate\nthem\naccording\nto\nthe\nsame\nrule\nas\nb\nefore:\nQ(s\nk\n;\na\nk\n):=\n^\nR(s\nk\n;\na\nk\n)\n+\n\r\nX\ns\n0\n^\nT\n(s\nk\n;\na\nk\n;\ns\n0\n)\nmax\na\n0\nQ(s\n0\n;\na\n0\n)\n:\n\u000f\nCho\nose\nan\naction\na\n0\nto\np\nerform\nin\nstate\ns\n0\n,\nbased\non\nthe\nQ\nv\nalues\nbut\np\nerhaps\nmo\ndi\fed\nb\ny\nan\nexploration\nstrategy\n.\nThe\nDyna\nalgorithm\nrequires\nab\nout\nk\ntimes\nthe\ncomputation\nof\nQ-learning\np\ner\ninstance,\nbut\nthis\nis\nt\nypically\nv\nastly\nless\nthan\nfor\nthe\nnaiv\ne\nmo\ndel-based\nmetho\nd.\nA\nreasonable\nv\nalue\nof\nk\ncan\nb\ne\ndetermined\nbased\non\nthe\nrelativ\ne\nsp\needs\nof\ncomputation\nand\nof\ntaking\naction.\nFigure\n\u0006\nsho\nws\na\ngrid\nw\norld\nin\nwhic\nh\nin\neac\nh\ncell\nthe\nagen\nt\nhas\nfour\nactions\n(N,\nS,\nE,\nW)\nand\ntransitions\nare\nmade\ndeterministically\nto\nan\nadjacen\nt\ncell,\nunless\nthere\nis\na\nblo\nc\nk,\nin\nwhic\nh\ncase\nno\nmo\nv\nemen\nt\no\nccurs.\nAs\nw\ne\nwill\nsee\nin\nT\nable\n\u0001,\nDyna\nrequires\nan\norder\nof\nmagnitude\nfew\ner\nsteps\nof\nexp\nerience\nthan\ndo\nes\nQ-learning\nto\narriv\ne\nat\nan\noptimal\np\nolicy\n.\nDyna\nrequires\nab\nout\nsix\ntimes\nmore\ncomputational\ne\u000bort,\nho\nw\nev\ner.\n\u0002\u0005\u0006\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nFigure\n\u0006:\nA\n\u0003\u0002\u0007\u0007-state\ngrid\nw\norld.\nThis\nw\nas\nform\nulated\nas\na\nshortest-path\nreinforcemen\nt-\nlearning\nproblem,\nwhic\nh\nyields\nthe\nsame\nresult\nas\nif\na\nrew\nard\nof\n\u0001\nis\ngiv\nen\nat\nthe\ngoal,\na\nrew\nard\nof\nzero\nelsewhere\nand\na\ndiscoun\nt\nfactor\nis\nused.\nSteps\nb\nefore\nBac\nkups\nb\nefore\ncon\nv\nergence\ncon\nv\nergence\nQ-learning\n\u0005\u0003\u0001,000\n\u0005\u0003\u0001,000\nDyna\n\u0006\u0002,000\n\u0003,0\u0005\u0005,000\nprioritized\nsw\neeping\n\u0002\b,000\n\u0001,0\u00010,000\nT\nable\n\u0001:\nThe\np\nerformance\nof\nthree\nalgorithms\ndescrib\ned\nin\nthe\ntext.\nAll\nmetho\nds\nused\nthe\nexploration\nheuristic\nof\n\\optimism\nin\nthe\nface\nof\nuncertain\nt\ny\":\nan\ny\nstate\nnot\npreviously\nvisited\nw\nas\nassumed\nb\ny\ndefault\nto\nb\ne\na\ngoal\nstate.\nQ-learning\nused\nits\noptimal\nlearning\nrate\nparameter\nfor\na\ndeterministic\nmaze:\n\u000b\n=\n\u0001.\nDyna\nand\nprioritized\nsw\neeping\nw\nere\np\nermitted\nto\ntak\ne\nk\n=\n\u000200\nbac\nkups\np\ner\ntransition.\nF\nor\nprioritized\nsw\neeping,\nthe\npriorit\ny\nqueue\noften\nemptied\nb\nefore\nall\nbac\nkups\nw\nere\nused.\n\u0002\u0005\u0007\nKaelbling,\nLittman,\n&\nMoore\n\u0005.\u0003\nPrioritized\nSw\neeping\n/\nQueue-Dyna\nAlthough\nDyna\nis\na\ngreat\nimpro\nv\nemen\nt\non\nprevious\nmetho\nds,\nit\nsu\u000bers\nfrom\nb\neing\nrelativ\nely\nundirected.\nIt\nis\nparticularly\nunhelpful\nwhen\nthe\ngoal\nhas\njust\nb\neen\nreac\nhed\nor\nwhen\nthe\nagen\nt\nis\nstuc\nk\nin\na\ndead\nend;\nit\ncon\ntin\nues\nto\nup\ndate\nrandom\nstate-action\npairs,\nrather\nthan\nconcen\ntrating\non\nthe\n\\in\nteresting\"\nparts\nof\nthe\nstate\nspace.\nThese\nproblems\nare\naddressed\nb\ny\nprioritized\nsw\neeping\n(Mo\nore\n&\nA\ntk\neson,\n\u0001\t\t\u0003)\nand\nQueue-Dyna\n(P\neng\n&\nWilliams,\n\u0001\t\t\u0003),\nwhic\nh\nare\nt\nw\no\nindep\nenden\ntly-dev\nel\nop\ned\nbut\nv\nery\nsimilar\ntec\nhniques.\nW\ne\nwill\ndescrib\ne\nprioritized\nsw\neeping\nin\nsome\ndetail.\nThe\nalgorithm\nis\nsimilar\nto\nDyna,\nexcept\nthat\nup\ndates\nare\nno\nlonger\nc\nhosen\nat\nrandom\nand\nv\nalues\nare\nno\nw\nasso\nciated\nwith\nstates\n(as\nin\nv\nalue\niteration)\ninstead\nof\nstate-action\npairs\n(as\nin\nQ-learning).\nT\no\nmak\ne\nappropriate\nc\nhoices,\nw\ne\nm\nust\nstore\nadditional\ninformation\nin\nthe\nmo\ndel.\nEac\nh\nstate\nremem\nb\ners\nits\npr\ne\nde\nc\nessors:\nthe\nstates\nthat\nha\nv\ne\na\nnon-zero\ntransition\nprobabilit\ny\nto\nit\nunder\nsome\naction.\nIn\naddition,\neac\nh\nstate\nhas\na\npriority,\ninitially\nset\nto\nzero.\nInstead\nof\nup\ndating\nk\nrandom\nstate-action\npairs,\nprioritized\nsw\neeping\nup\ndates\nk\nstates\nwith\nthe\nhighest\npriorit\ny\n.\nF\nor\neac\nh\nhigh-priorit\ny\nstate\ns,\nit\nw\norks\nas\nfollo\nws:\n\u000f\nRemem\nb\ner\nthe\ncurren\nt\nv\nalue\nof\nthe\nstate:\nV\nold\n=\nV\n(s).\n\u000f\nUp\ndate\nthe\nstate's\nv\nalue\nV\n(s)\n:=\nmax\na\n \n^\nR(s;\na)\n+\n\r\nX\ns\n0\n^\nT\n(s;\na;\ns\n0\n)V\n(s\n0\n)\n!\n:\n\u000f\nSet\nthe\nstate's\npriorit\ny\nbac\nk\nto\n0.\n\u000f\nCompute\nthe\nv\nalue\nc\nhange\n\u0001\n=\njV\nold\n\u0000V\n(s)j.\n\u000f\nUse\n\u0001\nto\nmo\ndify\nthe\npriorities\nof\nthe\npredecessors\nof\ns.\nIf\nw\ne\nha\nv\ne\nup\ndated\nthe\nV\nv\nalue\nfor\nstate\ns\n0\nand\nit\nhas\nc\nhanged\nb\ny\namoun\nt\n\u0001,\nthen\nthe\nimmediate\npredecessors\nof\ns\n0\nare\ninformed\nof\nthis\nev\nen\nt.\nAn\ny\nstate\ns\nfor\nwhic\nh\nthere\nexists\nan\naction\na\nsuc\nh\nthat\n^\nT\n(s;\na;\ns\n0\n)\n\u0006=\n0\nhas\nits\npriorit\ny\npromoted\nto\n\u0001\n\u0001\n^\nT\n(s;\na;\ns\n0\n),\nunless\nits\npriorit\ny\nalready\nexceeded\nthat\nv\nalue.\nThe\nglobal\nb\neha\nvior\nof\nthis\nalgorithm\nis\nthat\nwhen\na\nreal-w\norld\ntransition\nis\n\\surprising\"\n(the\nagen\nt\nhapp\nens\nup\non\na\ngoal\nstate,\nfor\ninstance),\nthen\nlots\nof\ncomputation\nis\ndirected\nto\npropagate\nthis\nnew\ninformation\nbac\nk\nto\nrelev\nan\nt\npredecessor\nstates.\nWhen\nthe\nreal-\nw\norld\ntransition\nis\n\\b\noring\"\n(the\nactual\nresult\nis\nv\nery\nsimilar\nto\nthe\npredicted\nresult),\nthen\ncomputation\ncon\ntin\nues\nin\nthe\nmost\ndeserving\npart\nof\nthe\nspace.\nRunning\nprioritized\nsw\neeping\non\nthe\nproblem\nin\nFigure\n\u0006,\nw\ne\nsee\na\nlarge\nimpro\nv\nemen\nt\no\nv\ner\nDyna.\nThe\noptimal\np\nolicy\nis\nreac\nhed\nin\nab\nout\nhalf\nthe\nn\num\nb\ner\nof\nsteps\nof\nexp\nerience\nand\none-third\nthe\ncomputation\nas\nDyna\nrequired\n(and\ntherefore\nab\nout\n\u00020\ntimes\nfew\ner\nsteps\nand\nt\nwice\nthe\ncomputational\ne\u000bort\nof\nQ-learning).\n\u0002\u0005\b\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n\u0005.\u0004\nOther\nMo\ndel-Based\nMetho\nds\nMetho\nds\nprop\nosed\nfor\nsolving\nMDPs\ngiv\nen\na\nmo\ndel\ncan\nb\ne\nused\nin\nthe\ncon\ntext\nof\nmo\ndel-\nbased\nmetho\nds\nas\nw\nell.\nR\nTDP\n(real-time\ndynamic\nprogramming)\n(Barto,\nBradtk\ne,\n&\nSingh,\n\u0001\t\t\u0005)\nis\nanother\nmo\ndel-based\nmetho\nd\nthat\nuses\nQ-learning\nto\nconcen\ntrate\ncomputational\ne\u000bort\non\nthe\nareas\nof\nthe\nstate-space\nthat\nthe\nagen\nt\nis\nmost\nlik\nely\nto\no\nccup\ny\n.\nIt\nis\nsp\neci\fc\nto\nproblems\nin\nwhic\nh\nthe\nagen\nt\nis\ntrying\nto\nac\nhiev\ne\na\nparticular\ngoal\nstate\nand\nthe\nrew\nard\nev\nerywhere\nelse\nis\n0.\nBy\ntaking\nin\nto\naccoun\nt\nthe\nstart\nstate,\nit\ncan\n\fnd\na\nshort\npath\nfrom\nthe\nstart\nto\nthe\ngoal,\nwithout\nnecessarily\nvisiting\nthe\nrest\nof\nthe\nstate\nspace.\nThe\nPlexus\nplanning\nsystem\n(Dean,\nKaelbling,\nKirman,\n&\nNic\nholson,\n\u0001\t\t\u0003;\nKirman,\n\u0001\t\t\u0004)\nexploits\na\nsimilar\nin\ntuition.\nIt\nstarts\nb\ny\nmaking\nan\nappro\nximate\nv\nersion\nof\nthe\nMDP\nwhic\nh\nis\nm\nuc\nh\nsmaller\nthan\nthe\noriginal\none.\nThe\nappro\nximate\nMDP\ncon\ntains\na\nset\nof\nstates,\ncalled\nthe\nenvelop\ne,\nthat\nincludes\nthe\nagen\nt's\ncurren\nt\nstate\nand\nthe\ngoal\nstate,\nif\nthere\nis\none.\nStates\nthat\nare\nnot\nin\nthe\nen\nv\nelop\ne\nare\nsummarized\nb\ny\na\nsingle\n\\out\"\nstate.\nThe\nplanning\npro\ncess\nis\nan\nalternation\nb\net\nw\neen\n\fnding\nan\noptimal\np\nolicy\non\nthe\nappro\nximate\nMDP\nand\nadding\nuseful\nstates\nto\nthe\nen\nv\nelop\ne.\nAction\nma\ny\ntak\ne\nplace\nin\nparallel\nwith\nplanning,\nin\nwhic\nh\ncase\nirrelev\nan\nt\nstates\nare\nalso\npruned\nout\nof\nthe\nen\nv\nelop\ne.\n\u0006.\nGeneralization\nAll\nof\nthe\nprevious\ndiscussion\nhas\ntacitly\nassumed\nthat\nit\nis\np\nossible\nto\nen\numerate\nthe\nstate\nand\naction\nspaces\nand\nstore\ntables\nof\nv\nalues\no\nv\ner\nthem.\nExcept\nin\nv\nery\nsmall\nen\nvironmen\nts,\nthis\nmeans\nimpractical\nmemory\nrequiremen\nts.\nIt\nalso\nmak\nes\nine\u000ecien\nt\nuse\nof\nexp\nerience.\nIn\na\nlarge,\nsmo\noth\nstate\nspace\nw\ne\ngenerally\nexp\nect\nsimilar\nstates\nto\nha\nv\ne\nsimilar\nv\nalues\nand\nsim-\nilar\noptimal\nactions.\nSurely\n,\ntherefore,\nthere\nshould\nb\ne\nsome\nmore\ncompact\nrepresen\ntation\nthan\na\ntable.\nMost\nproblems\nwill\nha\nv\ne\ncon\ntin\nuous\nor\nlarge\ndiscrete\nstate\nspaces;\nsome\nwill\nha\nv\ne\nlarge\nor\ncon\ntin\nuous\naction\nspaces.\nThe\nproblem\nof\nlearning\nin\nlarge\nspaces\nis\naddressed\nthrough\ngener\nalization\nte\nchniques,\nwhic\nh\nallo\nw\ncompact\nstorage\nof\nlearned\ninformation\nand\ntransfer\nof\nkno\nwledge\nb\net\nw\neen\n\\similar\"\nstates\nand\nactions.\nThe\nlarge\nliterature\nof\ngeneralization\ntec\nhniques\nfrom\ninductiv\ne\nconcept\nlearning\ncan\nb\ne\napplied\nto\nreinforcemen\nt\nlearning.\nHo\nw\nev\ner,\ntec\nhniques\noften\nneed\nto\nb\ne\ntailored\nto\nsp\neci\fc\ndetails\nof\nthe\nproblem.\nIn\nthe\nfollo\nwing\nsections,\nw\ne\nexplore\nthe\napplication\nof\nstandard\nfunction-appro\nximation\ntec\nhniques,\nadaptiv\ne\nresolution\nmo\ndels,\nand\nhierarc\nhical\nmetho\nds\nto\nthe\nproblem\nof\nreinforcemen\nt\nlearning.\nThe\nreinforcemen\nt-learning\narc\nhitectures\nand\nalgorithms\ndiscussed\nab\no\nv\ne\nha\nv\ne\nincluded\nthe\nstorage\nof\na\nv\nariet\ny\nof\nmappings,\nincluding\nS\n!\nA\n(p\nolicies),\nS\n!\n<\n(v\nalue\nfunctions),\nS\n\u0002\nA\n!\n<\n(Q\nfunctions\nand\nrew\nards),\nS\n\u0002\nA\n!\nS\n(deterministic\ntransitions),\nand\nS\n\u0002\nA\n\u0002\nS\n!\n[0;\n\u0001]\n(transition\nprobabilities).\nSome\nof\nthese\nmappings,\nsuc\nh\nas\ntransitions\nand\nimmediate\nrew\nards,\ncan\nb\ne\nlearned\nusing\nstraigh\ntforw\nard\nsup\nervised\nlearning,\nand\ncan\nb\ne\nhandled\nusing\nan\ny\nof\nthe\nwide\nv\nariet\ny\nof\nfunction-appro\nximation\ntec\nhniques\nfor\nsup\nervised\nlearning\nthat\nsupp\nort\nnoisy\ntraining\nexamples.\nP\nopular\ntec\nhniques\ninclude\nv\narious\nneural-\nnet\nw\nork\nmetho\nds\n(Rumelhart\n&\nMcClelland,\n\u0001\t\b\u0006),\nfuzzy\nlogic\n(Berenji,\n\u0001\t\t\u0001;\nLee,\n\u0001\t\t\u0001).\nCMA\nC\n(Albus,\n\u0001\t\b\u0001),\nand\nlo\ncal\nmemory-based\nmetho\nds\n(Mo\nore,\nA\ntk\neson,\n&\nSc\nhaal,\n\u0001\t\t\u0005),\nsuc\nh\nas\ngeneralizations\nof\nnearest\nneigh\nb\nor\nmetho\nds.\nOther\nmappings,\nesp\necially\nthe\np\nolicy\n\u0002\u0005\t\nKaelbling,\nLittman,\n&\nMoore\nmapping,\nt\nypically\nneed\nsp\necialized\nalgorithms\nb\necause\ntraining\nsets\nof\ninput-output\npairs\nare\nnot\na\nv\nailable.\n\u0006.\u0001\nGeneralization\no\nv\ner\nInput\nA\nreinforcemen\nt-learning\nagen\nt's\ncurren\nt\nstate\npla\nys\na\ncen\ntral\nrole\nin\nits\nselection\nof\nrew\nard-\nmaximizing\nactions.\nViewing\nthe\nagen\nt\nas\na\nstate-free\nblac\nk\nb\no\nx,\na\ndescription\nof\nthe\ncurren\nt\nstate\nis\nits\ninput.\nDep\nending\non\nthe\nagen\nt\narc\nhitecture,\nits\noutput\nis\neither\nan\naction\nselection,\nor\nan\nev\naluation\nof\nthe\ncurren\nt\nstate\nthat\ncan\nb\ne\nused\nto\nselect\nan\naction.\nThe\nproblem\nof\ndeciding\nho\nw\nthe\ndi\u000beren\nt\nasp\nects\nof\nan\ninput\na\u000bect\nthe\nv\nalue\nof\nthe\noutput\nis\nsometimes\ncalled\nthe\n\\structural\ncredit-assignmen\nt\"\nproblem.\nThis\nsection\nexamines\napproac\nhes\nto\ngenerating\nactions\nor\nev\naluations\nas\na\nfunction\nof\na\ndescription\nof\nthe\nagen\nt's\ncurren\nt\nstate.\nThe\n\frst\ngroup\nof\ntec\nhniques\nco\nv\nered\nhere\nis\nsp\necialized\nto\nthe\ncase\nwhen\nrew\nard\nis\nnot\ndela\ny\ned;\nthe\nsecond\ngroup\nis\nmore\ngenerally\napplicable.\n\u0006.\u0001.\u0001\nImmedia\nte\nRew\nard\nWhen\nthe\nagen\nt's\nactions\ndo\nnot\nin\ruence\nstate\ntransitions,\nthe\nresulting\nproblem\nb\necomes\none\nof\nc\nho\nosing\nactions\nto\nmaximize\nimmediate\nrew\nard\nas\na\nfunction\nof\nthe\nagen\nt's\ncurren\nt\nstate.\nThese\nproblems\nb\near\na\nresem\nblance\nto\nthe\nbandit\nproblems\ndiscussed\nin\nSection\n\u0002\nexcept\nthat\nthe\nagen\nt\nshould\ncondition\nits\naction\nselection\non\nthe\ncurren\nt\nstate.\nF\nor\nthis\nreason,\nthis\nclass\nof\nproblems\nhas\nb\neen\ndescrib\ned\nas\nasso\nciative\nreinforcemen\nt\nlearning.\nThe\nalgorithms\nin\nthis\nsection\naddress\nthe\nproblem\nof\nlearning\nfrom\nimmediate\nb\no\nolean\nreinforcemen\nt\nwhere\nthe\nstate\nis\nv\nector\nv\nalued\nand\nthe\naction\nis\na\nb\no\nolean\nv\nector.\nSuc\nh\nalgorithms\ncan\nand\nha\nv\ne\nb\neen\nused\nin\nthe\ncon\ntext\nof\na\ndela\ny\ned\nreinforcemen\nt,\nfor\ninstance,\nas\nthe\nRL\ncomp\nonen\nt\nin\nthe\nAHC\narc\nhitecture\ndescrib\ned\nin\nSection\n\u0004.\u0001.\nThey\ncan\nalso\nb\ne\ngeneralized\nto\nreal-v\nalued\nrew\nard\nthrough\nr\newar\nd\nc\nomp\narison\nmetho\nds\n(Sutton,\n\u0001\t\b\u0004).\nCRBP\nThe\ncomplemen\ntary\nreinforcemen\nt\nbac\nkpropagation\nalgorithm\n(Ac\nkley\n&\nLittman,\n\u0001\t\t0)\n(crbp)\nconsists\nof\na\nfeed-forw\nard\nnet\nw\nork\nmapping\nan\nenco\nding\nof\nthe\nstate\nto\nan\nenco\nding\nof\nthe\naction.\nThe\naction\nis\ndetermined\nprobabilistically\nfrom\nthe\nactiv\nation\nof\nthe\noutput\nunits:\nif\noutput\nunit\ni\nhas\nactiv\nation\ny\ni\n,\nthen\nbit\ni\nof\nthe\naction\nv\nector\nhas\nv\nalue\n\u0001\nwith\nprobabilit\ny\ny\ni\n,\nand\n0\notherwise.\nAn\ny\nneural-net\nw\nork\nsup\nervised\ntraining\npro\ncedure\ncan\nb\ne\nused\nto\nadapt\nthe\nnet\nw\nork\nas\nfollo\nws.\nIf\nthe\nresult\nof\ngenerating\naction\na\nis\nr\n=\n\u0001,\nthen\nthe\nnet\nw\nork\nis\ntrained\nwith\ninput-output\npair\nhs;\nai.\nIf\nthe\nresult\nis\nr\n=\n0,\nthen\nthe\nnet\nw\nork\nis\ntrained\nwith\ninput-output\npair\nhs;\n\u0016\na\ni,\nwhere\n\u0016\na\n=\n(\u0001\n\u0000a\n\u0001\n;\n:\n:\n:\n;\n\u0001\n\u0000a\nn\n).\nThe\nidea\nb\nehind\nthis\ntraining\nrule\nis\nthat\nwhenev\ner\nan\naction\nfails\nto\ngenerate\nrew\nard,\ncrbp\nwill\ntry\nto\ngenerate\nan\naction\nthat\nis\ndi\u000beren\nt\nfrom\nthe\ncurren\nt\nc\nhoice.\nAlthough\nit\nseems\nlik\ne\nthe\nalgorithm\nmigh\nt\noscillate\nb\net\nw\neen\nan\naction\nand\nits\ncomplemen\nt,\nthat\ndo\nes\nnot\nhapp\nen.\nOne\nstep\nof\ntraining\na\nnet\nw\nork\nwill\nonly\nc\nhange\nthe\naction\nsligh\ntly\nand\nsince\nthe\noutput\nprobabilities\nwill\ntend\nto\nmo\nv\ne\nto\nw\nard\n0.\u0005,\nthis\nmak\nes\naction\nselection\nmore\nrandom\nand\nincreases\nsearc\nh.\nThe\nhop\ne\nis\nthat\nthe\nrandom\ndistribution\nwill\ngenerate\nan\naction\nthat\nw\norks\nb\netter,\nand\nthen\nthat\naction\nwill\nb\ne\nreinforced.\nAR\nC\nThe\nasso\nciativ\ne\nreinforcemen\nt\ncomparison\n(ar\nc)\nalgorithm\n(Sutton,\n\u0001\t\b\u0004)\nis\nan\ninstance\nof\nthe\nahc\narc\nhitecture\nfor\nthe\ncase\nof\nb\no\nolean\nactions,\nconsisting\nof\nt\nw\no\nfeed-\n\u0002\u00060\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nforw\nard\nnet\nw\norks.\nOne\nlearns\nthe\nv\nalue\nof\nsituations,\nthe\nother\nlearns\na\np\nolicy\n.\nThese\ncan\nb\ne\nsimple\nlinear\nnet\nw\norks\nor\ncan\nha\nv\ne\nhidden\nunits.\nIn\nthe\nsimplest\ncase,\nthe\nen\ntire\nsystem\nlearns\nonly\nto\noptimize\nimmediate\nrew\nard.\nFirst,\nlet\nus\nconsider\nthe\nb\neha\nvior\nof\nthe\nnet\nw\nork\nthat\nlearns\nthe\np\nolicy\n,\na\nmapping\nfrom\na\nv\nector\ndescribing\ns\nto\na\n0\nor\n\u0001.\nIf\nthe\noutput\nunit\nhas\nactiv\nation\ny\ni\n,\nthen\na,\nthe\naction\ngenerated,\nwill\nb\ne\n\u0001\nif\ny\n+\n\u0017\n>\n0,\nwhere\n\u0017\nis\nnormal\nnoise,\nand\n0\notherwise.\nThe\nadjustmen\nt\nfor\nthe\noutput\nunit\nis,\nin\nthe\nsimplest\ncase,\ne\n=\nr\n(a\n\u0000\u0001=\u0002)\n;\nwhere\nthe\n\frst\nfactor\nis\nthe\nrew\nard\nreceiv\ned\nfor\ntaking\nthe\nmost\nrecen\nt\naction\nand\nthe\nsecond\nenco\ndes\nwhic\nh\naction\nw\nas\ntak\nen.\nThe\nactions\nare\nenco\nded\nas\n0\nand\n\u0001,\nso\na\n\u0000\u0001=\u0002\nalw\na\nys\nhas\nthe\nsame\nmagnitude;\nif\nthe\nrew\nard\nand\nthe\naction\nha\nv\ne\nthe\nsame\nsign,\nthen\naction\n\u0001\nwill\nb\ne\nmade\nmore\nlik\nely\n,\notherwise\naction\n0\nwill\nb\ne.\nAs\ndescrib\ned,\nthe\nnet\nw\nork\nwill\ntend\nto\nseek\nactions\nthat\ngiv\nen\np\nositiv\ne\nrew\nard.\nT\no\nextend\nthis\napproac\nh\nto\nmaximize\nrew\nard,\nw\ne\ncan\ncompare\nthe\nrew\nard\nto\nsome\nbaseline,\nb.\nThis\nc\nhanges\nthe\nadjustmen\nt\nto\ne\n=\n(r\n\u0000b)(a\n\u0000\u0001=\u0002)\n;\nwhere\nb\nis\nthe\noutput\nof\nthe\nsecond\nnet\nw\nork.\nThe\nsecond\nnet\nw\nork\nis\ntrained\nin\na\nstandard\nsup\nervised\nmo\nde\nto\nestimate\nr\nas\na\nfunction\nof\nthe\ninput\nstate\ns.\nV\nariations\nof\nthis\napproac\nh\nha\nv\ne\nb\neen\nused\nin\na\nv\nariet\ny\nof\napplications\n(Anderson,\n\u0001\t\b\u0006;\nBarto\net\nal.,\n\u0001\t\b\u0003;\nLin,\n\u0001\t\t\u0003b;\nSutton,\n\u0001\t\b\u0004).\nREINF\nOR\nCE\nAlgorithms\nWilliams\n(\u0001\t\b\u0007,\n\u0001\t\t\u0002)\nstudied\nthe\nproblem\nof\nc\nho\nosing\nac-\ntions\nto\nmaximize\nimmedate\nrew\nard.\nHe\niden\nti\fed\na\nbroad\nclass\nof\nup\ndate\nrules\nthat\np\ner-\nform\ngradien\nt\ndescen\nt\non\nthe\nexp\nected\nrew\nard\nand\nsho\nw\ned\nho\nw\nto\nin\ntegrate\nthese\nrules\nwith\nbac\nkpropagation.\nThis\nclass,\ncalled\nreinf\nor\nce\nalgorithms,\nincludes\nlinear\nrew\nard-inaction\n(Section\n\u0002.\u0001.\u0003)\nas\na\nsp\necial\ncase.\nThe\ngeneric\nreinf\nor\nce\nup\ndate\nfor\na\nparameter\nw\nij\ncan\nb\ne\nwritten\n\u0001w\nij\n=\n\u000b\nij\n(r\n\u0000b\nij\n)\n@\n@\nw\nij\nln\n(g\nj\n)\nwhere\n\u000b\nij\nis\na\nnon-negativ\ne\nfactor,\nr\nthe\ncurren\nt\nreinforcemen\nt,\nb\nij\na\nreinforcemen\nt\nbaseline,\nand\ng\ni\nis\nthe\nprobabilit\ny\ndensit\ny\nfunction\nused\nto\nrandomly\ngenerate\nactions\nbased\non\nunit\nactiv\nations.\nBoth\n\u000b\nij\nand\nb\nij\ncan\ntak\ne\non\ndi\u000beren\nt\nv\nalues\nfor\neac\nh\nw\nij\n,\nho\nw\nev\ner,\nwhen\n\u000b\nij\nis\nconstan\nt\nthroughout\nthe\nsystem,\nthe\nexp\nected\nup\ndate\nis\nexactly\nin\nthe\ndirection\nof\nthe\nexp\nected\nrew\nard\ngradien\nt.\nOtherwise,\nthe\nup\ndate\nis\nin\nthe\nsame\nhalf\nspace\nas\nthe\ngradien\nt\nbut\nnot\nnecessarily\nin\nthe\ndirection\nof\nsteep\nest\nincrease.\nWilliams\np\noin\nts\nout\nthat\nthe\nc\nhoice\nof\nbaseline,\nb\nij\n,\ncan\nha\nv\ne\na\nprofound\ne\u000bect\non\nthe\ncon\nv\nergence\nsp\need\nof\nthe\nalgorithm.\nLogic-Based\nMetho\nds\nAnother\nstrategy\nfor\ngeneralization\nin\nreinforcemen\nt\nlearning\nis\nto\nreduce\nthe\nlearning\nproblem\nto\nan\nasso\nciativ\ne\nproblem\nof\nlearning\nb\no\nolean\nfunctions.\nA\nb\no\nolean\nfunction\nhas\na\nv\nector\nof\nb\no\nolean\ninputs\nand\na\nsingle\nb\no\nolean\noutput.\nT\naking\ninspiration\nfrom\nmainstream\nmac\nhine\nlearning\nw\nork,\nKaelbling\ndev\nelop\ned\nt\nw\no\nalgorithms\nfor\nlearning\nb\no\nolean\nfunctions\nfrom\nreinforcemen\nt:\none\nuses\nthe\nbias\nof\nk\n-DNF\nto\ndriv\ne\n\u0002\u0006\u0001\nKaelbling,\nLittman,\n&\nMoore\nthe\ngeneralization\npro\ncess\n(Kaelbling,\n\u0001\t\t\u0004b);\nthe\nother\nsearc\nhes\nthe\nspace\nof\nsyn\ntactic\ndescriptions\nof\nfunctions\nusing\na\nsimple\ngenerate-and-test\nmetho\nd\n(Kaelbling,\n\u0001\t\t\u0004a).\nThe\nrestriction\nto\na\nsingle\nb\no\nolean\noutput\nmak\nes\nthese\ntec\nhniques\ndi\u000ecult\nto\napply\n.\nIn\nv\nery\nb\nenign\nlearning\nsituations,\nit\nis\np\nossible\nto\nextend\nthis\napproac\nh\nto\nuse\na\ncollection\nof\nlearners\nto\nindep\nenden\ntly\nlearn\nthe\nindividual\nbits\nthat\nmak\ne\nup\na\ncomplex\noutput.\nIn\ngeneral,\nho\nw\nev\ner,\nthat\napproac\nh\nsu\u000bers\nfrom\nthe\nproblem\nof\nv\nery\nunreliable\nreinforcemen\nt:\nif\na\nsingle\nlearner\ngenerates\nan\ninappropriate\noutput\nbit,\nall\nof\nthe\nlearners\nreceiv\ne\na\nlo\nw\nreinforcemen\nt\nv\nalue.\nThe\ncascade\nmetho\nd\n(Kaelbling,\n\u0001\t\t\u0003b)\nallo\nws\na\ncollection\nof\nlearners\nto\nb\ne\ntrained\ncollectiv\nely\nto\ngenerate\nappropriate\njoin\nt\noutputs;\nit\nis\nconsiderably\nmore\nreliable,\nbut\ncan\nrequire\nadditional\ncomputational\ne\u000bort.\n\u0006.\u0001.\u0002\nDela\nyed\nRew\nard\nAnother\nmetho\nd\nto\nallo\nw\nreinforcemen\nt-learning\ntec\nhniques\nto\nb\ne\napplied\nin\nlarge\nstate\nspaces\nis\nmo\ndeled\non\nv\nalue\niteration\nand\nQ-learning.\nHere,\na\nfunction\nappro\nximator\nis\nused\nto\nrepresen\nt\nthe\nv\nalue\nfunction\nb\ny\nmapping\na\nstate\ndescription\nto\na\nv\nalue.\nMan\ny\nreseac\nhers\nha\nv\ne\nexp\nerimen\nted\nwith\nthis\napproac\nh:\nBo\ny\nan\nand\nMo\nore\n(\u0001\t\t\u0005)\nused\nlo\ncal\nmemory-based\nmetho\nds\nin\nconjunction\nwith\nv\nalue\niteration;\nLin\n(\u0001\t\t\u0001)\nused\nbac\nkprop-\nagation\nnet\nw\norks\nfor\nQ-learning;\nW\natkins\n(\u0001\t\b\t)\nused\nCMA\nC\nfor\nQ-learning;\nT\nesauro\n(\u0001\t\t\u0002,\n\u0001\t\t\u0005)\nused\nbac\nkpropagation\nfor\nlearning\nthe\nv\nalue\nfunction\nin\nbac\nkgammon\n(describ\ned\nin\nSection\n\b.\u0001);\nZhang\nand\nDietteric\nh\n(\u0001\t\t\u0005)\nused\nbac\nkpropagation\nand\nT\nD\n(\u0015)\nto\nlearn\ngo\no\nd\nstrategies\nfor\njob-shop\nsc\nheduling.\nAlthough\nthere\nha\nv\ne\nb\neen\nsome\np\nositiv\ne\nexamples,\nin\ngeneral\nthere\nare\nunfortunate\nin-\nteractions\nb\net\nw\neen\nfunction\nappro\nximation\nand\nthe\nlearning\nrules.\nIn\ndiscrete\nen\nvironmen\nts\nthere\nis\na\nguaran\ntee\nthat\nan\ny\nop\neration\nthat\nup\ndates\nthe\nv\nalue\nfunction\n(according\nto\nthe\nBellman\nequations)\ncan\nonly\nreduce\nthe\nerror\nb\net\nw\neen\nthe\ncurren\nt\nv\nalue\nfunction\nand\nthe\noptimal\nv\nalue\nfunction.\nThis\nguaran\ntee\nno\nlonger\nholds\nwhen\ngeneralization\nis\nused.\nThese\nissues\nare\ndiscussed\nb\ny\nBo\ny\nan\nand\nMo\nore\n(\u0001\t\t\u0005),\nwho\ngiv\ne\nsome\nsimple\nexamples\nof\nv\nalue\nfunction\nerrors\ngro\nwing\narbitrarily\nlarge\nwhen\ngeneralization\nis\nused\nwith\nv\nalue\niteration.\nTheir\nsolution\nto\nthis,\napplicable\nonly\nto\ncertain\nclasses\nof\nproblems,\ndiscourages\nsuc\nh\ndiv\ner-\ngence\nb\ny\nonly\np\nermitting\nup\ndates\nwhose\nestimated\nv\nalues\ncan\nb\ne\nsho\nwn\nto\nb\ne\nnear-optimal\nvia\na\nbattery\nof\nMon\nte-Carlo\nexp\nerimen\nts.\nThrun\nand\nSc\nh\nw\nartz\n(\u0001\t\t\u0003)\ntheorize\nthat\nfunction\nappro\nximation\nof\nv\nalue\nfunctions\nis\nalso\ndangerous\nb\necause\nthe\nerrors\nin\nv\nalue\nfunctions\ndue\nto\ngeneralization\ncan\nb\necome\ncomp\nounded\nb\ny\nthe\n\\max\"\nop\nerator\nin\nthe\nde\fnition\nof\nthe\nv\nalue\nfunction.\nSev\neral\nrecen\nt\nresults\n(Gordon,\n\u0001\t\t\u0005;\nTsitsiklis\n&\nV\nan\nRo\ny\n,\n\u0001\t\t\u0006)\nsho\nw\nho\nw\nthe\nappro-\npriate\nc\nhoice\nof\nfunction\nappro\nximator\ncan\nguaran\ntee\ncon\nv\nergence,\nthough\nnot\nnecessarily\nto\nthe\noptimal\nv\nalues.\nBaird's\nr\nesidual\ngr\nadient\ntec\nhnique\n(Baird,\n\u0001\t\t\u0005)\npro\nvides\nguaran\nteed\ncon\nv\nergence\nto\nlo\ncally\noptimal\nsolutions.\nP\nerhaps\nthe\nglo\nominess\nof\nthese\ncoun\nter-examples\nis\nmisplaced.\nBo\ny\nan\nand\nMo\nore\n(\u0001\t\t\u0005)\nrep\nort\nthat\ntheir\ncoun\nter-examples\nc\nan\nb\ne\nmade\nto\nw\nork\nwith\nproblem-sp\neci\fc\nhand-tuning\ndespite\nthe\nunreliabilit\ny\nof\nun\ntuned\nalgorithms\nthat\npro\nv\nably\ncon\nv\nerge\nin\ndiscrete\ndomains.\nSutton\n(\u0001\t\t\u0006)\nsho\nws\nho\nw\nmo\ndi\fed\nv\nersions\nof\nBo\ny\nan\nand\nMo\nore's\nexamples\ncan\ncon\nv\nerge\nsuccessfully\n.\nAn\nop\nen\nquestion\nis\nwhether\ngeneral\nprinciples,\nideally\nsupp\norted\nb\ny\ntheory\n,\ncan\nhelp\nus\nunderstand\nwhen\nv\nalue\nfunction\nappro\nximation\nwill\nsucceed.\nIn\nSutton's\ncom-\n\u0002\u0006\u0002\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nparativ\ne\nexp\nerimen\nts\nwith\nBo\ny\nan\nand\nMo\nore's\ncoun\nter-examples,\nhe\nc\nhanges\nfour\nasp\nects\nof\nthe\nexp\nerimen\nts:\n\u0001.\nSmall\nc\nhanges\nto\nthe\ntask\nsp\neci\fcations.\n\u0002.\nA\nv\nery\ndi\u000beren\nt\nkind\nof\nfunction\nappro\nximator\n(CMA\nC\n(Albus,\n\u0001\t\u0007\u0005))\nthat\nhas\nw\neak\ngeneralization.\n\u0003.\nA\ndi\u000beren\nt\nlearning\nalgorithm:\nSARSA\n(Rummery\n&\nNiranjan,\n\u0001\t\t\u0004)\ninstead\nof\nv\nalue\niteration.\n\u0004.\nA\ndi\u000beren\nt\ntraining\nregime.\nBo\ny\nan\nand\nMo\nore\nsampled\nstates\nuniformly\nin\nstate\nspace,\nwhereas\nSutton's\nmetho\nd\nsampled\nalong\nempirical\ntra\njectories.\nThere\nare\nin\ntuitiv\ne\nreasons\nto\nb\neliev\ne\nthat\nthe\nfourth\nfactor\nis\nparticularly\nimp\nortan\nt,\nbut\nmore\ncareful\nresearc\nh\nis\nneeded.\nAdaptiv\ne\nResolution\nMo\ndels\nIn\nman\ny\ncases,\nwhat\nw\ne\nw\nould\nlik\ne\nto\ndo\nis\npartition\nthe\nen\nvironmen\nt\nin\nto\nregions\nof\nstates\nthat\ncan\nb\ne\nconsidered\nthe\nsame\nfor\nthe\npurp\noses\nof\nlearning\nand\ngenerating\nactions.\nWithout\ndetailed\nprior\nkno\nwledge\nof\nthe\nen\nvironmen\nt,\nit\nis\nv\nery\ndi\u000ecult\nto\nkno\nw\nwhat\ngran\nularit\ny\nor\nplacemen\nt\nof\npartitions\nis\nappropriate.\nThis\nproblem\nis\no\nv\nercome\nin\nmetho\nds\nthat\nuse\nadaptiv\ne\nresolution;\nduring\nthe\ncourse\nof\nlearning,\na\npartition\nis\nconstructed\nthat\nis\nappropriate\nto\nthe\nen\nvironmen\nt.\nDecision\nT\nrees\nIn\nen\nvironmen\nts\nthat\nare\nc\nharacterized\nb\ny\na\nset\nof\nb\no\nolean\nor\ndiscrete-\nv\nalued\nv\nariables,\nit\nis\np\nossible\nto\nlearn\ncompact\ndecision\ntrees\nfor\nrepresen\nting\nQ\nv\nalues.\nThe\nG-le\narning\nalgorithm\n(Chapman\n&\nKaelbling,\n\u0001\t\t\u0001),\nw\norks\nas\nfollo\nws.\nIt\nstarts\nb\ny\nassuming\nthat\nno\npartitioning\nis\nnecessary\nand\ntries\nto\nlearn\nQ\nv\nalues\nfor\nthe\nen\ntire\nen\nvironmen\nt\nas\nif\nit\nw\nere\none\nstate.\nIn\nparallel\nwith\nthis\npro\ncess,\nit\ngathers\nstatistics\nbased\non\nindividual\ninput\nbits;\nit\nasks\nthe\nquestion\nwhether\nthere\nis\nsome\nbit\nb\nin\nthe\nstate\ndescription\nsuc\nh\nthat\nthe\nQ\nv\nalues\nfor\nstates\nin\nwhic\nh\nb\n=\n\u0001\nare\nsigni\fcan\ntly\ndi\u000beren\nt\nfrom\nQ\nv\nalues\nfor\nstates\nin\nwhic\nh\nb\n=\n0.\nIf\nsuc\nh\na\nbit\nis\nfound,\nit\nis\nused\nto\nsplit\nthe\ndecision\ntree.\nThen,\nthe\npro\ncess\nis\nrep\neated\nin\neac\nh\nof\nthe\nlea\nv\nes.\nThis\nmetho\nd\nw\nas\nable\nto\nlearn\nv\nery\nsmall\nrepresen\ntations\nof\nthe\nQ\nfunction\nin\nthe\npresence\nof\nan\no\nv\nerwhelming\nn\num\nb\ner\nof\nirrelev\nan\nt,\nnoisy\nstate\nattributes.\nIt\noutp\nerformed\nQ-learning\nwith\nbac\nkpropagation\nin\na\nsimple\nvideo-\ngame\nen\nvironmen\nt\nand\nw\nas\nused\nb\ny\nMcCallum\n(\u0001\t\t\u0005)\n(in\nconjunction\nwith\nother\ntec\nhniques\nfor\ndealing\nwith\npartial\nobserv\nabilit\ny)\nto\nlearn\nb\neha\nviors\nin\na\ncomplex\ndriving-sim\nulator.\nIt\ncannot,\nho\nw\nev\ner,\nacquire\npartitions\nin\nwhic\nh\nattributes\nare\nonly\nsigni\fcan\nt\nin\ncom\nbination\n(suc\nh\nas\nthose\nneeded\nto\nsolv\ne\nparit\ny\nproblems).\nV\nariable\nResolution\nDynamic\nProgramming\nThe\nVRDP\nalgorithm\n(Mo\nore,\n\u0001\t\t\u0001)\nenables\ncon\nv\nen\ntional\ndynamic\nprogramming\nto\nb\ne\np\nerformed\nin\nreal-v\nalued\nm\nultiv\nariate\nstate-spaces\nwhere\nstraigh\ntforw\nard\ndiscretization\nw\nould\nfall\nprey\nto\nthe\ncurse\nof\ndimension-\nalit\ny\n.\nA\nk\nd-tree\n(similar\nto\na\ndecision\ntree)\nis\nused\nto\npartition\nstate\nspace\nin\nto\ncoarse\nregions.\nThe\ncoarse\nregions\nare\nre\fned\nin\nto\ndetailed\nregions,\nbut\nonly\nin\nparts\nof\nthe\nstate\nspace\nwhic\nh\nare\npredicted\nto\nb\ne\nimp\nortan\nt.\nThis\nnotion\nof\nimp\nortance\nis\nobtained\nb\ny\nrun-\nning\n\\men\ntal\ntra\njectories\"\nthrough\nstate\nspace.\nThis\nalgorithm\npro\nv\ned\ne\u000bectiv\ne\non\na\nn\num\nb\ner\nof\nproblems\nfor\nwhic\nh\nfull\nhigh-resolution\narra\nys\nw\nould\nha\nv\ne\nb\neen\nimpractical.\nIt\nhas\nthe\ndisadv\nan\ntage\nof\nrequiring\na\nguess\nat\nan\ninitially\nv\nalid\ntra\njectory\nthrough\nstate-space.\n\u0002\u0006\u0003\nKaelbling,\nLittman,\n&\nMoore\nG\nStart\nGoal\n(a)\nG\n(b)\nG\n(c)\nFigure\n\u0007:\n(a)\nA\nt\nw\no-dimensional\nmaze\nproblem.\nThe\np\noin\nt\nrob\not\nm\nust\n\fnd\na\npath\nfrom\nstart\nto\ngoal\nwithout\ncrossing\nan\ny\nof\nthe\nbarrier\nlines.\n(b)\nThe\npath\ntak\nen\nb\ny\nP\nartiGame\nduring\nthe\nen\ntire\n\frst\ntrial.\nIt\nb\negins\nwith\nin\ntense\nexploration\nto\n\fnd\na\nroute\nout\nof\nthe\nalmost\nen\ntirely\nenclosed\nstart\nregion.\nHa\nving\nev\nen\ntually\nreac\nhed\na\nsu\u000ecien\ntly\nhigh\nresolution,\nit\ndisco\nv\ners\nthe\ngap\nand\npro\nceeds\ngreedily\nto\nw\nards\nthe\ngoal,\nonly\nto\nb\ne\ntemp\norarily\nblo\nc\nk\ned\nb\ny\nthe\ngoal's\nbarrier\nregion.\n(c)\nThe\nsecond\ntrial.\nP\nartiGame\nAlgorithm\nMo\nore's\nP\nartiGame\nalgorithm\n(Mo\nore,\n\u0001\t\t\u0004)\nis\nanother\nsolution\nto\nthe\nproblem\nof\nlearning\nto\nac\nhiev\ne\ngoal\ncon\fgurations\nin\ndeterministic\nhigh-dimensional\ncon\ntin\nuous\nspaces\nb\ny\nlearning\nan\nadaptiv\ne-resolution\nmo\ndel.\nIt\nalso\ndivides\nthe\nen\nvironmen\nt\nin\nto\ncells;\nbut\nin\neac\nh\ncell,\nthe\nactions\na\nv\nailable\nconsist\nof\naiming\nat\nthe\nneigh\nb\noring\ncells\n(this\naiming\nis\naccomplished\nb\ny\na\nlo\ncal\ncon\ntroller,\nwhic\nh\nm\nust\nb\ne\npro\nvided\nas\npart\nof\nthe\nproblem\nstatemen\nt).\nThe\ngraph\nof\ncell\ntransitions\nis\nsolv\ned\nfor\nshortest\npaths\nin\nan\nonline\nincremen\ntal\nmanner,\nbut\na\nminimax\ncriterion\nis\nused\nto\ndetect\nwhen\na\ngroup\nof\ncells\nis\nto\no\ncoarse\nto\nprev\nen\nt\nmo\nv\nemen\nt\nb\net\nw\neen\nobstacles\nor\nto\na\nv\noid\nlimit\ncycles.\nThe\no\u000bending\ncells\nare\nsplit\nto\nhigher\nresolution.\nEv\nen\ntually\n,\nthe\nen\nvironmen\nt\nis\ndivided\nup\njust\nenough\nto\nc\nho\nose\nappropriate\nactions\nfor\nac\nhieving\nthe\ngoal,\nbut\nno\nunnecessary\ndistinctions\nare\nmade.\nAn\nimp\nortan\nt\nfeature\nis\nthat,\nas\nw\nell\nas\nreducing\nmemory\nand\ncomputational\nrequiremen\nts,\nit\nalso\nstructures\nexploration\nof\nstate\nspace\nin\na\nm\nulti-resolution\nmanner.\nGiv\nen\na\nfailure,\nthe\nagen\nt\nwill\ninitially\ntry\nsomething\nv\nery\ndi\u000beren\nt\nto\nrectify\nthe\nfailure,\nand\nonly\nresort\nto\nsmall\nlo\ncal\nc\nhanges\nwhen\nall\nthe\nqualitativ\nely\ndi\u000beren\nt\nstrategies\nha\nv\ne\nb\neen\nexhausted.\nFigure\n\u0007a\nsho\nws\na\nt\nw\no-dimensional\ncon\ntin\nuous\nmaze.\nFigure\n\u0007b\nsho\nws\nthe\np\nerformance\nof\na\nrob\not\nusing\nthe\nP\nartiGame\nalgorithm\nduring\nthe\nv\nery\n\frst\ntrial.\nFigure\n\u0007c\nsho\nws\nthe\nsecond\ntrial,\nstarted\nfrom\na\nsligh\ntly\ndi\u000beren\nt\np\nosition.\nThis\nis\na\nv\nery\nfast\nalgorithm,\nlearning\np\nolicies\nin\nspaces\nof\nup\nto\nnine\ndimensions\nin\nless\nthan\na\nmin\nute.\nThe\nrestriction\nof\nthe\ncurren\nt\nimplemen\ntation\nto\ndeterministic\nen\nvironmen\nts\nlimits\nits\napplicabili\nt\ny\n,\nho\nw\nev\ner.\nMcCallum\n(\u0001\t\t\u0005)\nsuggests\nsome\nrelated\ntree-structured\nmetho\nds.\n\u0002\u0006\u0004\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n\u0006.\u0002\nGeneralization\no\nv\ner\nActions\nThe\nnet\nw\norks\ndescrib\ned\nin\nSection\n\u0006.\u0001.\u0001\ngeneralize\no\nv\ner\nstate\ndescriptions\npresen\nted\nas\ninputs.\nThey\nalso\npro\nduce\noutputs\nin\na\ndiscrete,\nfactored\nrepresen\ntation\nand\nth\nus\ncould\nb\ne\nseen\nas\ngeneralizing\no\nv\ner\nactions\nas\nw\nell.\nIn\ncases\nsuc\nh\nas\nthis\nwhen\nactions\nare\ndescrib\ned\ncom\nbinatorially\n,\nit\nis\nimp\nortan\nt\nto\ngeneralize\no\nv\ner\nactions\nto\na\nv\noid\nk\neeping\nseparate\nstatistics\nfor\nthe\nh\nuge\nn\num\nb\ner\nof\nactions\nthat\ncan\nb\ne\nc\nhosen.\nIn\ncon\ntin\nuous\naction\nspaces,\nthe\nneed\nfor\ngeneralization\nis\nev\nen\nmore\npronounced.\nWhen\nestimating\nQ\nv\nalues\nusing\na\nneural\nnet\nw\nork,\nit\nis\np\nossible\nto\nuse\neither\na\ndistinct\nnet\nw\nork\nfor\neac\nh\naction,\nor\na\nnet\nw\nork\nwith\na\ndistinct\noutput\nfor\neac\nh\naction.\nWhen\nthe\naction\nspace\nis\ncon\ntin\nuous,\nneither\napproac\nh\nis\np\nossible.\nAn\nalternativ\ne\nstrategy\nis\nto\nuse\na\nsingle\nnet\nw\nork\nwith\nb\noth\nthe\nstate\nand\naction\nas\ninput\nand\nQ\nv\nalue\nas\nthe\noutput.\nT\nraining\nsuc\nh\na\nnet\nw\nork\nis\nnot\nconceptually\ndi\u000ecult,\nbut\nusing\nthe\nnet\nw\nork\nto\n\fnd\nthe\noptimal\naction\ncan\nb\ne\na\nc\nhallenge.\nOne\nmetho\nd\nis\nto\ndo\na\nlo\ncal\ngradien\nt-ascen\nt\nsearc\nh\non\nthe\naction\nin\norder\nto\n\fnd\none\nwith\nhigh\nv\nalue\n(Baird\n&\nKlopf,\n\u0001\t\t\u0003).\nGullapalli\n(\u0001\t\t0,\n\u0001\t\t\u0002)\nhas\ndev\nelop\ned\na\n\\neural\"\nreinforcemen\nt-learning\nunit\nfor\nuse\nin\ncon\ntin\nuous\naction\nspaces.\nThe\nunit\ngenerates\nactions\nwith\na\nnormal\ndistribution;\nit\nadjusts\nthe\nmean\nand\nv\nariance\nbased\non\nprevious\nexp\nerience.\nWhen\nthe\nc\nhosen\nactions\nare\nnot\np\nerforming\nw\nell,\nthe\nv\nariance\nis\nhigh,\nresulting\nin\nexploration\nof\nthe\nrange\nof\nc\nhoices.\nWhen\nan\naction\np\nerforms\nw\nell,\nthe\nmean\nis\nmo\nv\ned\nin\nthat\ndirection\nand\nthe\nv\nariance\ndecreased,\nresulting\nin\na\ntendency\nto\ngenerate\nmore\naction\nv\nalues\nnear\nthe\nsuccessful\none.\nThis\nmetho\nd\nw\nas\nsuccessfully\nemplo\ny\ned\nto\nlearn\nto\ncon\ntrol\na\nrob\not\narm\nwith\nman\ny\ncon\ntin\nuous\ndegrees\nof\nfreedom.\n\u0006.\u0003\nHierarc\nhical\nMetho\nds\nAnother\nstrategy\nfor\ndealing\nwith\nlarge\nstate\nspaces\nis\nto\ntreat\nthem\nas\na\nhierarc\nh\ny\nof\nlearning\nproblems.\nIn\nman\ny\ncases,\nhierarc\nhical\nsolutions\nin\ntro\nduce\nsligh\nt\nsub-optimalit\ny\nin\np\nerformance,\nbut\np\noten\ntially\ngain\na\ngo\no\nd\ndeal\nof\ne\u000eciency\nin\nexecution\ntime,\nlearning\ntime,\nand\nspace.\nHierarc\nhical\nlearners\nare\ncommonly\nstructured\nas\ngate\nd\nb\nehaviors,\nas\nsho\nwn\nin\nFigure\n\b.\nThere\nis\na\ncollection\nof\nb\nehaviors\nthat\nmap\nen\nvironmen\nt\nstates\nin\nto\nlo\nw-lev\nel\nactions\nand\na\ngating\nfunction\nthat\ndecides,\nbased\non\nthe\nstate\nof\nthe\nen\nvironmen\nt,\nwhic\nh\nb\neha\nvior's\nactions\nshould\nb\ne\nswitc\nhed\nthrough\nand\nactually\nexecuted.\nMaes\nand\nBro\noks\n(\u0001\t\t0)\nused\na\nv\nersion\nof\nthis\narc\nhitecture\nin\nwhic\nh\nthe\nindividual\nb\neha\nviors\nw\nere\n\fxed\na\npriori\nand\nthe\ngating\nfunction\nw\nas\nlearned\nfrom\nreinforcemen\nt.\nMahadev\nan\nand\nConnell\n(\u0001\t\t\u0001b)\nused\nthe\ndual\napproac\nh:\nthey\n\fxed\nthe\ngating\nfunction,\nand\nsupplied\nreinforcemen\nt\nfunctions\nfor\nthe\nindividual\nb\neha\nviors,\nwhic\nh\nw\nere\nlearned.\nLin\n(\u0001\t\t\u0003a)\nand\nDorigo\nand\nColom\nb\netti\n(\u0001\t\t\u0005,\n\u0001\t\t\u0004)\nb\noth\nused\nthis\napproac\nh,\n\frst\ntraining\nthe\nb\neha\nviors\nand\nthen\ntraining\nthe\ngating\nfunction.\nMan\ny\nof\nthe\nother\nhierarc\nhical\nlearning\nmetho\nds\ncan\nb\ne\ncast\nin\nthis\nframew\nork.\n\u0006.\u0003.\u0001\nFeud\nal\nQ-learning\nF\neudal\nQ-learning\n(Da\ny\nan\n&\nHin\nton,\n\u0001\t\t\u0003;\nW\natkins,\n\u0001\t\b\t)\nin\nv\nolv\nes\na\nhierarc\nh\ny\nof\nlearning\nmo\ndules.\nIn\nthe\nsimplest\ncase,\nthere\nis\na\nhigh-lev\nel\nmaster\nand\na\nlo\nw-lev\nel\nsla\nv\ne.\nThe\nmaster\nreceiv\nes\nreinforcemen\nt\nfrom\nthe\nexternal\nen\nvironmen\nt.\nIts\nactions\nconsist\nof\ncommands\nthat\n\u0002\u0006\u0005\nKaelbling,\nLittman,\n&\nMoore\ns\nb1\nb2\nb3\ng\na\nFigure\n\b:\nA\nstructure\nof\ngated\nb\neha\nviors.\nit\ncan\ngiv\ne\nto\nthe\nlo\nw-lev\nel\nlearner.\nWhen\nthe\nmaster\ngenerates\na\nparticular\ncommand\nto\nthe\nsla\nv\ne,\nit\nm\nust\nrew\nard\nthe\nsla\nv\ne\nfor\ntaking\nactions\nthat\nsatisfy\nthe\ncommand,\nev\nen\nif\nthey\ndo\nnot\nresult\nin\nexternal\nreinforcemen\nt.\nThe\nmaster,\nthen,\nlearns\na\nmapping\nfrom\nstates\nto\ncommands.\nThe\nsla\nv\ne\nlearns\na\nmapping\nfrom\ncommands\nand\nstates\nto\nexternal\nactions.\nThe\nset\nof\n\\commands\"\nand\ntheir\nasso\nciated\nreinforcemen\nt\nfunctions\nare\nestablished\nin\nadv\nance\nof\nthe\nlearning.\nThis\nis\nreally\nan\ninstance\nof\nthe\ngeneral\n\\gated\nb\neha\nviors\"\napproac\nh,\nin\nwhic\nh\nthe\nsla\nv\ne\ncan\nexecute\nan\ny\nof\nthe\nb\neha\nviors\ndep\nending\non\nits\ncommand.\nThe\nreinforcemen\nt\nfunctions\nfor\nthe\nindividual\nb\neha\nviors\n(commands)\nare\ngiv\nen,\nbut\nlearning\ntak\nes\nplace\nsim\nultaneously\nat\nb\noth\nthe\nhigh\nand\nlo\nw\nlev\nels.\n\u0006.\u0003.\u0002\nCompositional\nQ-learning\nSingh's\ncomp\nositional\nQ-learning\n(\u0001\t\t\u0002b,\n\u0001\t\t\u0002a)\n(C-QL)\nconsists\nof\na\nhierarc\nh\ny\nbased\non\nthe\ntemp\noral\nsequencing\nof\nsubgoals.\nThe\nelemental\ntasks\nare\nb\neha\nviors\nthat\nac\nhiev\ne\nsome\nrecognizable\ncondition.\nThe\nhigh-lev\nel\ngoal\nof\nthe\nsystem\nis\nto\nac\nhiev\ne\nsome\nset\nof\ncondi-\ntions\nin\nsequen\ntial\norder.\nThe\nac\nhiev\nemen\nt\nof\nthe\nconditions\npro\nvides\nreinforcemen\nt\nfor\nthe\nelemen\ntal\ntasks,\nwhic\nh\nare\ntrained\n\frst\nto\nac\nhiev\ne\nindividual\nsubgoals.\nThen,\nthe\ngating\nfunction\nlearns\nto\nswitc\nh\nthe\nelemen\ntal\ntasks\nin\norder\nto\nac\nhiev\ne\nthe\nappropriate\nhigh-lev\nel\nsequen\ntial\ngoal.\nThis\nmetho\nd\nw\nas\nused\nb\ny\nTham\nand\nPrager\n(\u0001\t\t\u0004)\nto\nlearn\nto\ncon\ntrol\na\nsim\nulated\nm\nulti-link\nrob\not\narm.\n\u0006.\u0003.\u0003\nHierar\nchical\nDist\nance\nto\nGo\nal\nEsp\necially\nif\nw\ne\nconsider\nreinforcemen\nt\nlearning\nmo\ndules\nto\nb\ne\npart\nof\nlarger\nagen\nt\narc\nhi-\ntectures,\nit\nis\nimp\nortan\nt\nto\nconsider\nproblems\nin\nwhic\nh\ngoals\nare\ndynamically\ninput\nto\nthe\nlearner.\nKaelbling's\nHDG\nalgorithm\n(\u0001\t\t\u0003a)\nuses\na\nhierarc\nhical\napproac\nh\nto\nsolving\nprob-\nlems\nwhen\ngoals\nof\nac\nhiev\nemen\nt\n(the\nagen\nt\nshould\nget\nto\na\nparticular\nstate\nas\nquic\nkly\nas\np\nossible)\nare\ngiv\nen\nto\nan\nagen\nt\ndynamically\n.\nThe\nHDG\nalgorithm\nw\norks\nb\ny\nanalogy\nwith\nna\nvigation\nin\na\nharb\nor.\nThe\nen\nvironmen\nt\nis\npartitioned\n(a\npriori,\nbut\nmore\nrecen\nt\nw\nork\n(Ashar,\n\u0001\t\t\u0004)\naddresses\nthe\ncase\nof\nlearning\nthe\npartition)\nin\nto\na\nset\nof\nregions\nwhose\ncen\nters\nare\nkno\nwn\nas\n\\landmarks.\"\nIf\nthe\nagen\nt\nis\n\u0002\u0006\u0006\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n2/5\n1/5\n2/5\nprinter\nofﬁce\n+100\nhall\nhall\nFigure\n\t:\nAn\nexample\nof\na\npartially\nobserv\nable\nen\nvironmen\nt.\ncurren\ntly\nin\nthe\nsame\nregion\nas\nthe\ngoal,\nthen\nit\nuses\nlo\nw-lev\nel\nactions\nto\nmo\nv\ne\nto\nthe\ngoal.\nIf\nnot,\nthen\nhigh-lev\nel\ninformation\nis\nused\nto\ndetermine\nthe\nnext\nlandmark\non\nthe\nshortest\npath\nfrom\nthe\nagen\nt's\nclosest\nlandmark\nto\nthe\ngoal's\nclosest\nlandmark.\nThen,\nthe\nagen\nt\nuses\nlo\nw-lev\nel\ninformation\nto\naim\nto\nw\nard\nthat\nnext\nlandmark.\nIf\nerrors\nin\naction\ncause\ndeviations\nin\nthe\npath,\nthere\nis\nno\nproblem;\nthe\nb\nest\naiming\np\noin\nt\nis\nrecomputed\non\nev\nery\nstep.\n\u0007.\nP\nartially\nObserv\nable\nEn\nvironmen\nts\nIn\nman\ny\nreal-w\norld\nen\nvironmen\nts,\nit\nwill\nnot\nb\ne\np\nossible\nfor\nthe\nagen\nt\nto\nha\nv\ne\np\nerfect\nand\ncomplete\np\nerception\nof\nthe\nstate\nof\nthe\nen\nvironmen\nt.\nUnfortunately\n,\ncomplete\nobserv\nabilit\ny\nis\nnecessary\nfor\nlearning\nmetho\nds\nbased\non\nMDPs.\nIn\nthis\nsection,\nw\ne\nconsider\nthe\ncase\nin\nwhic\nh\nthe\nagen\nt\nmak\nes\nobservations\nof\nthe\nstate\nof\nthe\nen\nvironmen\nt,\nbut\nthese\nobserv\nations\nma\ny\nb\ne\nnoisy\nand\npro\nvide\nincomplete\ninformation.\nIn\nthe\ncase\nof\na\nrob\not,\nfor\ninstance,\nit\nmigh\nt\nobserv\ne\nwhether\nit\nis\nin\na\ncorridor,\nan\nop\nen\nro\nom,\na\nT-junction,\netc.,\nand\nthose\nobserv\nations\nmigh\nt\nb\ne\nerror-prone.\nThis\nproblem\nis\nalso\nreferred\nto\nas\nthe\nproblem\nof\n\\incomplete\np\nerception,\"\n\\p\nerceptual\naliasing,\"\nor\n\\hidden\nstate.\"\nIn\nthis\nsection,\nw\ne\nwill\nconsider\nextensions\nto\nthe\nbasic\nMDP\nframew\nork\nfor\nsolving\npartially\nobserv\nable\nproblems.\nThe\nresulting\nformal\nmo\ndel\nis\ncalled\na\np\nartial\nly\nobservable\nMarkov\nde\ncision\npr\no\nc\ness\nor\nPOMDP\n.\n\u0007.\u0001\nState-F\nree\nDeterministic\nP\nolicies\nThe\nmost\nnaiv\ne\nstrategy\nfor\ndealing\nwith\npartial\nobserv\nabilit\ny\nis\nto\nignore\nit.\nThat\nis,\nto\ntreat\nthe\nobserv\nations\nas\nif\nthey\nw\nere\nthe\nstates\nof\nthe\nen\nvironmen\nt\nand\ntry\nto\nlearn\nto\nb\neha\nv\ne.\nFigure\n\t\nsho\nws\na\nsimple\nen\nvironmen\nt\nin\nwhic\nh\nthe\nagen\nt\nis\nattempting\nto\nget\nto\nthe\nprin\nter\nfrom\nan\no\u000ece.\nIf\nit\nmo\nv\nes\nfrom\nthe\no\u000ece,\nthere\nis\na\ngo\no\nd\nc\nhance\nthat\nthe\nagen\nt\nwill\nend\nup\nin\none\nof\nt\nw\no\nplaces\nthat\nlo\nok\nlik\ne\n\\hall\",\nbut\nthat\nrequire\ndi\u000beren\nt\nactions\nfor\ngetting\nto\nthe\nprin\nter.\nIf\nw\ne\nconsider\nthese\nstates\nto\nb\ne\nthe\nsame,\nthen\nthe\nagen\nt\ncannot\np\nossibly\nb\neha\nv\ne\noptimally\n.\nBut\nho\nw\nw\nell\ncan\nit\ndo?\nThe\nresulting\nproblem\nis\nnot\nMark\no\nvian,\nand\nQ-learning\ncannot\nb\ne\nguaran\nteed\nto\ncon-\nv\nerge.\nSmall\nbreac\nhes\nof\nthe\nMark\no\nv\nrequiremen\nt\nare\nw\nell\nhandled\nb\ny\nQ-learning,\nbut\nit\nis\np\nossible\nto\nconstruct\nsimple\nen\nvironmen\nts\nthat\ncause\nQ-learning\nto\noscillate\n(Chrisman\n&\n\u0002\u0006\u0007\nKaelbling,\nLittman,\n&\nMoore\nLittman,\n\u0001\t\t\u0003).\nIt\nis\np\nossible\nto\nuse\na\nmo\ndel-based\napproac\nh,\nho\nw\nev\ner;\nact\naccording\nto\nsome\np\nolicy\nand\ngather\nstatistics\nab\nout\nthe\ntransitions\nb\net\nw\neen\nobserv\nations,\nthen\nsolv\ne\nfor\nthe\noptimal\np\nolicy\nbased\non\nthose\nobserv\nations.\nUnfortunately\n,\nwhen\nthe\nen\nvironmen\nt\nis\nnot\nMark\no\nvian,\nthe\ntransition\nprobabilities\ndep\nend\non\nthe\np\nolicy\nb\neing\nexecuted,\nso\nthis\nnew\np\nolicy\nwill\ninduce\na\nnew\nset\nof\ntransition\nprobabilities.\nThis\napproac\nh\nma\ny\nyield\nplausible\nresults\nin\nsome\ncases,\nbut\nagain,\nthere\nare\nno\nguaran\ntees.\nIt\nis\nreasonable,\nthough,\nto\nask\nwhat\nthe\noptimal\np\nolicy\n(mapping\nfrom\nobserv\nations\nto\nactions,\nin\nthis\ncase)\nis.\nIt\nis\nNP-hard\n(Littman,\n\u0001\t\t\u0004b)\nto\n\fnd\nthis\nmapping,\nand\nev\nen\nthe\nb\nest\nmapping\ncan\nha\nv\ne\nv\nery\np\no\nor\np\nerformance.\nIn\nthe\ncase\nof\nour\nagen\nt\ntrying\nto\nget\nto\nthe\nprin\nter,\nfor\ninstance,\nan\ny\ndeterministic\nstate-free\np\nolicy\ntak\nes\nan\nin\fnite\nn\num\nb\ner\nof\nsteps\nto\nreac\nh\nthe\ngoal\non\na\nv\nerage.\n\u0007.\u0002\nState-F\nree\nSto\nc\nhastic\nP\nolicies\nSome\nimpro\nv\nemen\nt\ncan\nb\ne\ngained\nb\ny\nconsidering\nsto\nc\nhastic\np\nolicies;\nthese\nare\nmappings\nfrom\nobserv\nations\nto\nprobabilit\ny\ndistributions\no\nv\ner\nactions.\nIf\nthere\nis\nrandomness\nin\nthe\nagen\nt's\nactions,\nit\nwill\nnot\nget\nstuc\nk\nin\nthe\nhall\nforev\ner.\nJaakk\nola,\nSingh,\nand\nJordan\n(\u0001\t\t\u0005)\nha\nv\ne\ndev\nelop\ned\nan\nalgorithm\nfor\n\fnding\nlo\ncally-optimal\nsto\nc\nhastic\np\nolicies,\nbut\n\fnding\na\nglobally\noptimal\np\nolicy\nis\nstill\nNP\nhard.\nIn\nour\nexample,\nit\nturns\nout\nthat\nthe\noptimal\nsto\nc\nhastic\np\nolicy\nis\nfor\nthe\nagen\nt,\nwhen\nin\na\nstate\nthat\nlo\noks\nlik\ne\na\nhall,\nto\ngo\neast\nwith\nprobabilit\ny\n\u0002\n\u0000p\n\u0002\n\u0019\n0:\u0006\nand\nw\nest\nwith\nprobabilit\ny\np\n\u0002\n\u0000\u0001\n\u0019\n0:\u0004.\nThis\np\nolicy\ncan\nb\ne\nfound\nb\ny\nsolving\na\nsimple\n(in\nthis\ncase)\nquadratic\nprogram.\nThe\nfact\nthat\nsuc\nh\na\nsimple\nexample\ncan\npro\nduce\nirrational\nn\num\nb\ners\ngiv\nes\nsome\nindication\nthat\nit\nis\na\ndi\u000ecult\nproblem\nto\nsolv\ne\nexactly\n.\n\u0007.\u0003\nP\nolicies\nwith\nIn\nternal\nState\nThe\nonly\nw\na\ny\nto\nb\neha\nv\ne\ntruly\ne\u000bectiv\nely\nin\na\nwide-range\nof\nen\nvironmen\nts\nis\nto\nuse\nmemory\nof\nprevious\nactions\nand\nobserv\nations\nto\ndisam\nbiguate\nthe\ncurren\nt\nstate.\nThere\nare\na\nv\nariet\ny\nof\napproac\nhes\nto\nlearning\np\nolicies\nwith\nin\nternal\nstate.\nRecurren\nt\nQ-learning\nOne\nin\ntuitiv\nely\nsimple\napproac\nh\nis\nto\nuse\na\nrecurren\nt\nneural\nnet-\nw\nork\nto\nlearn\nQ\nv\nalues.\nThe\nnet\nw\nork\ncan\nb\ne\ntrained\nusing\nbac\nkpropagation\nthrough\ntime\n(or\nsome\nother\nsuitable\ntec\nhnique)\nand\nlearns\nto\nretain\n\\history\nfeatures\"\nto\npredict\nv\nalue.\nThis\napproac\nh\nhas\nb\neen\nused\nb\ny\na\nn\num\nb\ner\nof\nresearc\nhers\n(Meeden,\nMcGra\nw,\n&\nBlank,\n\u0001\t\t\u0003;\nLin\n&\nMitc\nhell,\n\u0001\t\t\u0002;\nSc\nhmidh\nub\ner,\n\u0001\t\t\u0001b).\nIt\nseems\nto\nw\nork\ne\u000bectiv\nely\non\nsimple\nproblems,\nbut\ncan\nsu\u000ber\nfrom\ncon\nv\nergence\nto\nlo\ncal\noptima\non\nmore\ncomplex\nproblems.\nClassi\fer\nSystems\nClassi\fer\nsystems\n(Holland,\n\u0001\t\u0007\u0005;\nGoldb\nerg,\n\u0001\t\b\t)\nw\nere\nexplicitly\ndev\nelop\ned\nto\nsolv\ne\nproblems\nwith\ndela\ny\ned\nrew\nard,\nincluding\nthose\nrequiring\nshort-term\nmemory\n.\nThe\nin\nternal\nmec\nhanism\nt\nypically\nused\nto\npass\nrew\nard\nbac\nk\nthrough\nc\nhains\nof\ndecisions,\ncalled\nthe\nbucket\nbrigade\nalgorithm,\nb\nears\na\nclose\nresem\nblance\nto\nQ-learning.\nIn\nspite\nof\nsome\nearly\nsuccesses,\nthe\noriginal\ndesign\ndo\nes\nnot\napp\near\nto\nhandle\npartially\nob-\nserv\ned\nen\nvironmen\nts\nrobustly\n.\nRecen\ntly\n,\nthis\napproac\nh\nhas\nb\neen\nreexamined\nusing\ninsigh\nts\nfrom\nthe\nreinforcemen\nt-\nlearning\nliterature,\nwith\nsome\nsuccess.\nDorigo\ndid\na\ncomparativ\ne\nstudy\nof\nQ-learning\nand\nclassi\fer\nsystems\n(Dorigo\n&\nBersini,\n\u0001\t\t\u0004).\nCli\u000b\nand\nRoss\n(\u0001\t\t\u0004)\nstart\nwith\nWilson's\nzeroth-\n\u0002\u0006\b\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\ni\nb\na\nSE\nπ\nFigure\n\u00010:\nStructure\nof\na\nPOMDP\nagen\nt.\nlev\nel\nclassi\fer\nsystem\n(Wilson,\n\u0001\t\t\u0005)\nand\nadd\none\nand\nt\nw\no-bit\nmemory\nregisters.\nThey\n\fnd\nthat,\nalthough\ntheir\nsystem\ncan\nlearn\nto\nuse\nshort-term\nmemory\nregisters\ne\u000bectiv\nely\n,\nthe\napproac\nh\nis\nunlik\nely\nto\nscale\nto\nmore\ncomplex\nen\nvironmen\nts.\nDorigo\nand\nColom\nb\netti\napplied\nclassi\fer\nsystems\nto\na\nmo\nderately\ncomplex\nproblem\nof\nlearning\nrob\not\nb\neha\nvior\nfrom\nimmediate\nreinforcemen\nt\n(Dorigo,\n\u0001\t\t\u0005;\nDorigo\n&\nColom\nb\netti,\n\u0001\t\t\u0004).\nFinite-history-windo\nw\nApproac\nh\nOne\nw\na\ny\nto\nrestore\nthe\nMark\no\nv\nprop\nert\ny\nis\nto\nallo\nw\ndecisions\nto\nb\ne\nbased\non\nthe\nhistory\nof\nrecen\nt\nobserv\nations\nand\np\nerhaps\nactions.\nLin\nand\nMitc\nhell\n(\u0001\t\t\u0002)\nused\na\n\fxed-width\n\fnite\nhistory\nwindo\nw\nto\nlearn\na\np\nole\nbalancing\ntask.\nMcCallum\n(\u0001\t\t\u0005)\ndescrib\nes\nthe\n\\utile\nsu\u000ex\nmemory\"\nwhic\nh\nlearns\na\nv\nariable-width\nwindo\nw\nthat\nserv\nes\nsim\nultaneously\nas\na\nmo\ndel\nof\nthe\nen\nvironmen\nt\nand\na\n\fnite-memory\np\nolicy\n.\nThis\nsystem\nhas\nhad\nexcellen\nt\nresults\nin\na\nv\nery\ncomplex\ndriving-sim\nulation\ndomain\n(McCallum,\n\u0001\t\t\u0005).\nRing\n(\u0001\t\t\u0004)\nhas\na\nneural-net\nw\nork\napproac\nh\nthat\nuses\na\nv\nariable\nhistory\nwindo\nw,\nadding\nhistory\nwhen\nnecessary\nto\ndisam\nbiguate\nsituations.\nPOMDP\nApproac\nh\nAnother\nstrategy\nconsists\nof\nusing\nhidden\nMark\no\nv\nmo\ndel\n(HMM)\ntec\nhniques\nto\nlearn\na\nmo\ndel\nof\nthe\nen\nvironmen\nt,\nincluding\nthe\nhidden\nstate,\nthen\nto\nuse\nthat\nmo\ndel\nto\nconstruct\na\np\nerfe\nct\nmemory\ncon\ntroller\n(Cassandra,\nKaelbling,\n&\nLittman,\n\u0001\t\t\u0004;\nLo\nv\nejo\ny\n,\n\u0001\t\t\u0001;\nMonahan,\n\u0001\t\b\u0002).\nChrisman\n(\u0001\t\t\u0002)\nsho\nw\ned\nho\nw\nthe\nforw\nard-bac\nkw\nard\nalgorithm\nfor\nlearning\nHMMs\ncould\nb\ne\nadapted\nto\nlearning\nPOMDPs.\nHe,\nand\nlater\nMcCallum\n(\u0001\t\t\u0003),\nalso\nga\nv\ne\nheuristic\nstate-\nsplitting\nrules\nto\nattempt\nto\nlearn\nthe\nsmallest\np\nossible\nmo\ndel\nfor\na\ngiv\nen\nen\nvironmen\nt.\nThe\nresulting\nmo\ndel\ncan\nthen\nb\ne\nused\nto\nin\ntegrate\ninformation\nfrom\nthe\nagen\nt's\nobserv\nations\nin\norder\nto\nmak\ne\ndecisions.\nFigure\n\u00010\nillustrates\nthe\nbasic\nstructure\nfor\na\np\nerfect-memory\ncon\ntroller.\nThe\ncomp\nonen\nt\non\nthe\nleft\nis\nthe\nstate\nestimator,\nwhic\nh\ncomputes\nthe\nagen\nt's\nb\nelief\nstate,\nb\nas\na\nfunction\nof\nthe\nold\nb\nelief\nstate,\nthe\nlast\naction\na,\nand\nthe\ncurren\nt\nobserv\nation\ni.\nIn\nthis\ncon\ntext,\na\nb\nelief\nstate\nis\na\nprobabilit\ny\ndistribution\no\nv\ner\nstates\nof\nthe\nen\nvironmen\nt,\nindicating\nthe\nlik\neliho\no\nd,\ngiv\nen\nthe\nagen\nt's\npast\nexp\nerience,\nthat\nthe\nen\nvironmen\nt\nis\nactually\nin\neac\nh\nof\nthose\nstates.\nThe\nstate\nestimator\ncan\nb\ne\nconstructed\nstraigh\ntforw\nardly\nusing\nthe\nestimated\nw\norld\nmo\ndel\nand\nBa\ny\nes'\nrule.\nNo\nw\nw\ne\nare\nleft\nwith\nthe\nproblem\nof\n\fnding\na\np\nolicy\nmapping\nb\nelief\nstates\nin\nto\naction.\nThis\nproblem\ncan\nb\ne\nform\nulated\nas\nan\nMDP,\nbut\nit\nis\ndi\u000ecult\nto\nsolv\ne\nusing\nthe\ntec\nhniques\ndescrib\ned\nearlier,\nb\necause\nthe\ninput\nspace\nis\ncon\ntin\nuous.\nChrisman's\napproac\nh\n(\u0001\t\t\u0002)\ndo\nes\nnot\ntak\ne\nin\nto\naccoun\nt\nfuture\nuncertain\nt\ny\n,\nbut\nyields\na\np\nolicy\nafter\na\nsmall\namoun\nt\nof\ncom-\nputation.\nA\nstandard\napproac\nh\nfrom\nthe\nop\nerations-researc\nh\nliterature\nis\nto\nsolv\ne\nfor\nthe\n\u0002\u0006\t\nKaelbling,\nLittman,\n&\nMoore\noptimal\np\nolicy\n(or\na\nclose\nappro\nximation\nthereof\n)\nbased\non\nits\nrepresen\ntation\nas\na\npiecewise-\nlinear\nand\ncon\nv\nex\nfunction\no\nv\ner\nthe\nb\nelief\nspace.\nThis\nmetho\nd\nis\ncomputationally\nin\ntractable,\nbut\nma\ny\nserv\ne\nas\ninspiration\nfor\nmetho\nds\nthat\nmak\ne\nfurther\nappro\nximations\n(Cassandra\net\nal.,\n\u0001\t\t\u0004;\nLittman,\nCassandra,\n&\nKaelbling,\n\u0001\t\t\u0005a).\n\b.\nReinforcemen\nt\nLearning\nApplications\nOne\nreason\nthat\nreinforcemen\nt\nlearning\nis\np\nopular\nis\nthat\nis\nserv\nes\nas\na\ntheoretical\nto\nol\nfor\nstudying\nthe\nprinciples\nof\nagen\nts\nlearning\nto\nact.\nBut\nit\nis\nunsurprising\nthat\nit\nhas\nalso\nb\neen\nused\nb\ny\na\nn\num\nb\ner\nof\nresearc\nhers\nas\na\npractical\ncomputational\nto\nol\nfor\nconstructing\nautonomous\nsystems\nthat\nimpro\nv\ne\nthemselv\nes\nwith\nexp\nerience.\nThese\napplications\nha\nv\ne\nranged\nfrom\nrob\notics,\nto\nindustrial\nman\nufacturing,\nto\ncom\nbinatorial\nsearc\nh\nproblems\nsuc\nh\nas\ncomputer\ngame\npla\nying.\nPractical\napplications\npro\nvide\na\ntest\nof\nthe\ne\u000ecacy\nand\nusefulness\nof\nlearning\nalgorithms.\nThey\nare\nalso\nan\ninspiration\nfor\ndeciding\nwhic\nh\ncomp\nonen\nts\nof\nthe\nreinforcemen\nt\nlearning\nframew\nork\nare\nof\npractical\nimp\nortance.\nF\nor\nexample,\na\nresearc\nher\nwith\na\nreal\nrob\notic\ntask\ncan\npro\nvide\na\ndata\np\noin\nt\nto\nquestions\nsuc\nh\nas:\n\u000f\nHo\nw\nimp\nortan\nt\nis\noptimal\nexploration?\nCan\nw\ne\nbreak\nthe\nlearning\np\nerio\nd\nin\nto\nexplo-\nration\nphases\nand\nexploitation\nphases?\n\u000f\nWhat\nis\nthe\nmost\nuseful\nmo\ndel\nof\nlong-term\nrew\nard:\nFinite\nhorizon?\nDiscoun\nted?\nIn\fnite\nhorizon?\n\u000f\nHo\nw\nm\nuc\nh\ncomputation\nis\na\nv\nailable\nb\net\nw\neen\nagen\nt\ndecisions\nand\nho\nw\nshould\nit\nb\ne\nused?\n\u000f\nWhat\nprior\nkno\nwledge\ncan\nw\ne\nbuild\nin\nto\nthe\nsystem,\nand\nwhic\nh\nalgorithms\nare\ncapable\nof\nusing\nthat\nkno\nwledge?\nLet\nus\nexamine\na\nset\nof\npractical\napplications\nof\nreinforcemen\nt\nlearning,\nwhile\nb\nearing\nthese\nquestions\nin\nmind.\n\b.\u0001\nGame\nPla\nying\nGame\npla\nying\nhas\ndominated\nthe\nArti\fcial\nIn\ntelligence\nw\norld\nas\na\nproblem\ndomain\nev\ner\nsince\nthe\n\feld\nw\nas\nb\norn.\nTw\no-pla\ny\ner\ngames\ndo\nnot\n\ft\nin\nto\nthe\nestablished\nreinforcemen\nt-learning\nframew\nork\nsince\nthe\noptimalit\ny\ncriterion\nfor\ngames\nis\nnot\none\nof\nmaximizing\nrew\nard\nin\nthe\nface\nof\na\n\fxed\nen\nvironmen\nt,\nbut\none\nof\nmaximizing\nrew\nard\nagainst\nan\noptimal\nadv\nersary\n(minimax).\nNonetheless,\nreinforcemen\nt-learning\nalgorithms\ncan\nb\ne\nadapted\nto\nw\nork\nfor\na\nv\nery\ngeneral\nclass\nof\ngames\n(Littman,\n\u0001\t\t\u0004a)\nand\nman\ny\nresearc\nhers\nha\nv\ne\nused\nreinforcemen\nt\nlearning\nin\nthese\nen\nvironmen\nts.\nOne\napplication,\nsp\nectacularly\nfar\nahead\nof\nits\ntime,\nw\nas\nSam\nuel's\nc\nhec\nk\ners\npla\nying\nsystem\n(Sam\nuel,\n\u0001\t\u0005\t).\nThis\nlearned\na\nv\nalue\nfunction\nrepresen\nted\nb\ny\na\nlinear\nfunction\nappro\nximator,\nand\nemplo\ny\ned\na\ntraining\nsc\nheme\nsimilar\nto\nthe\nup\ndates\nused\nin\nv\nalue\niteration,\ntemp\noral\ndi\u000berences\nand\nQ-learning.\nMore\nrecen\ntly\n,\nT\nesauro\n(\u0001\t\t\u0002,\n\u0001\t\t\u0004,\n\u0001\t\t\u0005)\napplied\nthe\ntemp\noral\ndi\u000berence\nalgorithm\nto\nbac\nkgammon.\nBac\nkgammon\nhas\nappro\nximately\n\u00010\n\u00020\nstates,\nmaking\ntable-based\nrein-\nforcemen\nt\nlearning\nimp\nossible.\nInstead,\nT\nesauro\nused\na\nbac\nkpropagation-based\nthree-la\ny\ner\n\u0002\u00070\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nT\nraining\nGames\nHidden\nUnits\nResults\nBasic\nP\no\nor\nTD\n\u0001.0\n\u000300,000\n\b0\nLost\nb\ny\n\u0001\u0003\np\noin\nts\nin\n\u0005\u0001\ngames\nTD\n\u0002.0\n\b00,000\n\u00040\nLost\nb\ny\n\u0007\np\noin\nts\nin\n\u0003\b\ngames\nTD\n\u0002.\u0001\n\u0001,\u000500,000\n\b0\nLost\nb\ny\n\u0001\np\noin\nt\nin\n\u00040\ngames\nT\nable\n\u0002:\nTD-Gammon's\np\nerformance\nin\ngames\nagainst\nthe\ntop\nh\numan\nprofessional\npla\ny\ners.\nA\nbac\nkgammon\ntournamen\nt\nin\nv\nolv\nes\npla\nying\na\nseries\nof\ngames\nfor\np\noin\nts\nun\ntil\none\npla\ny\ner\nreac\nhes\na\nset\ntarget.\nTD-Gammon\nw\non\nnone\nof\nthese\ntournamen\nts\nbut\ncame\nsu\u000ecien\ntly\nclose\nthat\nit\nis\nno\nw\nconsidered\none\nof\nthe\nb\nest\nfew\npla\ny\ners\nin\nthe\nw\norld.\nneural\nnet\nw\nork\nas\na\nfunction\nappro\nximator\nfor\nthe\nv\nalue\nfunction\nBo\nar\nd\nPosition\n!\nPr\nob\nability\nof\nvictory\nfor\ncurr\nent\nplayer:\nTw\no\nv\nersions\nof\nthe\nlearning\nalgorithm\nw\nere\nused.\nThe\n\frst,\nwhic\nh\nw\ne\nwill\ncall\nBasic\nTD-\nGammon,\nused\nv\nery\nlittle\nprede\fned\nkno\nwledge\nof\nthe\ngame,\nand\nthe\nrepresen\ntation\nof\na\nb\noard\np\nosition\nw\nas\nvirtually\na\nra\nw\nenco\nding,\nsu\u000ecien\ntly\np\no\nw\nerful\nonly\nto\np\nermit\nthe\nneural\nnet\nw\nork\nto\ndistinguish\nb\net\nw\neen\nconceptually\ndi\u000beren\nt\np\nositions.\nThe\nsecond,\nTD-Gammon,\nw\nas\npro\nvided\nwith\nthe\nsame\nra\nw\nstate\ninformation\nsupplemen\nted\nb\ny\na\nn\num\nb\ner\nof\nhand-\ncrafted\nfeatures\nof\nbac\nkgammon\nb\noard\np\nositions.\nPro\nviding\nhand-crafted\nfeatures\nin\nthis\nmanner\nis\na\ngo\no\nd\nexample\nof\nho\nw\ninductiv\ne\nbiases\nfrom\nh\numan\nkno\nwledge\nof\nthe\ntask\ncan\nb\ne\nsupplied\nto\na\nlearning\nalgorithm.\nThe\ntraining\nof\nb\noth\nlearning\nalgorithms\nrequired\nsev\neral\nmon\nths\nof\ncomputer\ntime,\nand\nw\nas\nac\nhiev\ned\nb\ny\nconstan\nt\nself-pla\ny\n.\nNo\nexploration\nstrategy\nw\nas\nused|the\nsystem\nalw\na\nys\ngreedily\nc\nhose\nthe\nmo\nv\ne\nwith\nthe\nlargest\nexp\nected\nprobabilit\ny\nof\nvictory\n.\nThis\nnaiv\ne\nexplo-\nration\nstrategy\npro\nv\ned\nen\ntirely\nadequate\nfor\nthis\nen\nvironmen\nt,\nwhic\nh\nis\np\nerhaps\nsurprising\ngiv\nen\nthe\nconsiderable\nw\nork\nin\nthe\nreinforcemen\nt-learning\nliterature\nwhic\nh\nhas\npro\nduced\nn\numerous\ncoun\nter-examples\nto\nsho\nw\nthat\ngreedy\nexploration\ncan\nlead\nto\np\no\nor\nlearning\np\ner-\nformance.\nBac\nkgammon,\nho\nw\nev\ner,\nhas\nt\nw\no\nimp\nortan\nt\nprop\nerties.\nFirstly\n,\nwhatev\ner\np\nolicy\nis\nfollo\nw\ned,\nev\nery\ngame\nis\nguaran\nteed\nto\nend\nin\n\fnite\ntime,\nmeaning\nthat\nuseful\nrew\nard\ninformation\nis\nobtained\nfairly\nfrequen\ntly\n.\nSecondly\n,\nthe\nstate\ntransitions\nare\nsu\u000ecien\ntly\nsto\nc\nhastic\nthat\nindep\nenden\nt\nof\nthe\np\nolicy\n,\nall\nstates\nwill\no\nccasionally\nb\ne\nvisited|a\nwrong\ninitial\nv\nalue\nfunction\nhas\nlittle\ndanger\nof\nstarving\nus\nfrom\nvisiting\na\ncritical\npart\nof\nstate\nspace\nfrom\nwhic\nh\nimp\nortan\nt\ninformation\ncould\nb\ne\nobtained.\nThe\nresults\n(T\nable\n\u0002)\nof\nTD-Gammon\nare\nimpressiv\ne.\nIt\nhas\ncomp\neted\nat\nthe\nv\nery\ntop\nlev\nel\nof\nin\nternational\nh\numan\npla\ny\n.\nBasic\nTD-Gammon\npla\ny\ned\nresp\nectably\n,\nbut\nnot\nat\na\nprofessional\nstandard.\n\u0002\u0007\u0001\nKaelbling,\nLittman,\n&\nMoore\n \nτ3\nτ1\nτ2\nα\nθ\nx, y\np\nτ1\nτ2\nFigure\n\u0001\u0001:\nSc\nhaal\nand\nA\ntk\neson's\ndevil-stic\nking\nrob\not.\nThe\ntap\nered\nstic\nk\nis\nhit\nalternately\nb\ny\neac\nh\nof\nthe\nt\nw\no\nhand\nstic\nks.\nThe\ntask\nis\nto\nk\neep\nthe\ndevil\nstic\nk\nfrom\nfalling\nfor\nas\nman\ny\nhits\nas\np\nossible.\nThe\nrob\not\nhas\nthree\nmotors\nindicated\nb\ny\ntorque\nv\nectors\n\u001c\n\u0001\n;\n\u001c\n\u0002\n;\n\u001c\n\u0003\n.\nAlthough\nexp\nerimen\nts\nwith\nother\ngames\nha\nv\ne\nin\nsome\ncases\npro\nduced\nin\nteresting\nlearning\nb\neha\nvior,\nno\nsuccess\nclose\nto\nthat\nof\nTD-Gammon\nhas\nb\neen\nrep\neated.\nOther\ngames\nthat\nha\nv\ne\nb\neen\nstudied\ninclude\nGo\n(Sc\nhraudolph,\nDa\ny\nan,\n&\nSejno\nwski,\n\u0001\t\t\u0004)\nand\nChess\n(Thrun,\n\u0001\t\t\u0005).\nIt\nis\nstill\nan\nop\nen\nquestion\nas\nto\nif\nand\nho\nw\nthe\nsuccess\nof\nTD-Gammon\ncan\nb\ne\nrep\neated\nin\nother\ndomains.\n\b.\u0002\nRob\notics\nand\nCon\ntrol\nIn\nrecen\nt\ny\nears\nthere\nha\nv\ne\nb\neen\nman\ny\nrob\notics\nand\ncon\ntrol\napplications\nthat\nha\nv\ne\nused\nreinforcemen\nt\nlearning.\nHere\nw\ne\nwill\nconcen\ntrate\non\nthe\nfollo\nwing\nfour\nexamples,\nalthough\nman\ny\nother\nin\nteresting\nongoing\nrob\notics\nin\nv\nestigations\nare\nunderw\na\ny\n.\n\u0001.\nSc\nhaal\nand\nA\ntk\neson\n(\u0001\t\t\u0004)\nconstructed\na\nt\nw\no-armed\nrob\not,\nsho\nwn\nin\nFigure\n\u0001\u0001,\nthat\nlearns\nto\njuggle\na\ndevice\nkno\nwn\nas\na\ndevil-stic\nk.\nThis\nis\na\ncomplex\nnon-linear\ncon\ntrol\ntask\nin\nv\nolving\na\nsix-dimensional\nstate\nspace\nand\nless\nthan\n\u000200\nmsecs\np\ner\ncon\ntrol\ndeci-\nsion.\nAfter\nab\nout\n\u00040\ninitial\nattempts\nthe\nrob\not\nlearns\nto\nk\neep\njuggling\nfor\nh\nundreds\nof\nhits.\nA\nt\nypical\nh\numan\nlearning\nthe\ntask\nrequires\nan\norder\nof\nmagnitude\nmore\npractice\nto\nac\nhiev\ne\npro\fciency\nat\nmere\ntens\nof\nhits.\nThe\njuggling\nrob\not\nlearned\na\nw\norld\nmo\ndel\nfrom\nexp\nerience,\nwhic\nh\nw\nas\ngeneralized\nto\nun\nvisited\nstates\nb\ny\na\nfunction\nappro\nximation\nsc\nheme\nkno\nwn\nas\nlo\ncally\nw\neigh\nted\nregression\n(Clev\neland\n&\nDelvin,\n\u0001\t\b\b;\nMo\nore\n&\nA\ntk\neson,\n\u0001\t\t\u0002).\nBet\nw\neen\neac\nh\ntrial,\na\nform\nof\ndynamic\nprogramming\nsp\neci\fc\nto\nlinear\ncon\ntrol\np\nolicies\nand\nlo\ncally\nlinear\ntransitions\nw\nas\nused\nto\nimpro\nv\ne\nthe\np\nolicy\n.\nThe\nform\nof\ndynamic\nprogramming\nis\nkno\nwn\nas\nlinear-quadratic-regulator\ndesign\n(Sage\n&\nWhite,\n\u0001\t\u0007\u0007).\n\u0002\u0007\u0002\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\n\u0002.\nMahadev\nan\nand\nConnell\n(\u0001\t\t\u0001a)\ndiscuss\na\ntask\nin\nwhic\nh\na\nmobile\nrob\not\npushes\nlarge\nb\no\nxes\nfor\nextended\np\nerio\nds\nof\ntime.\nBo\nx-pushing\nis\na\nw\nell-kno\nwn\ndi\u000ecult\nrob\notics\nproblem,\nc\nharacterized\nb\ny\nimmense\nuncertain\nt\ny\nin\nthe\nresults\nof\nactions.\nQ-learning\nw\nas\nused\nin\nconjunction\nwith\nsome\nno\nv\nel\nclustering\ntec\nhniques\ndesigned\nto\nenable\na\nhigher-dimensional\ninput\nthan\na\ntabular\napproac\nh\nw\nould\nha\nv\ne\np\nermitted.\nThe\nrob\not\nlearned\nto\np\nerform\ncomp\netitiv\nely\nwith\nthe\np\nerformance\nof\na\nh\numan-programmed\nso-\nlution.\nAnother\nasp\nect\nof\nthis\nw\nork,\nmen\ntioned\nin\nSection\n\u0006.\u0003,\nw\nas\na\npre-programmed\nbreakdo\nwn\nof\nthe\nmonolithic\ntask\ndescription\nin\nto\na\nset\nof\nlo\nw\ner\nlev\nel\ntasks\nto\nb\ne\nlearned.\n\u0003.\nMataric\n(\u0001\t\t\u0004)\ndescrib\nes\na\nrob\notics\nexp\nerimen\nt\nwith,\nfrom\nthe\nviewp\noin\nt\nof\ntheoret-\nical\nreinforcemen\nt\nlearning,\nan\nun\nthink\nably\nhigh\ndimensional\nstate\nspace,\ncon\ntaining\nman\ny\ndozens\nof\ndegrees\nof\nfreedom.\nF\nour\nmobile\nrob\nots\ntra\nv\neled\nwithin\nan\nenclo-\nsure\ncollecting\nsmall\ndisks\nand\ntransp\norting\nthem\nto\na\ndestination\nregion.\nThere\nw\nere\nthree\nenhancemen\nts\nto\nthe\nbasic\nQ-learning\nalgorithm.\nFirstly\n,\npre-programmed\nsig-\nnals\ncalled\npr\no\ngr\ness\nestimators\nw\nere\nused\nto\nbreak\nthe\nmonolithic\ntask\nin\nto\nsubtasks.\nThis\nw\nas\nac\nhiev\ned\nin\na\nrobust\nmanner\nin\nwhic\nh\nthe\nrob\nots\nw\nere\nnot\nforced\nto\nuse\nthe\nestimators,\nbut\nhad\nthe\nfreedom\nto\npro\ft\nfrom\nthe\ninductiv\ne\nbias\nthey\npro\nvided.\nSecondly\n,\ncon\ntrol\nw\nas\ndecen\ntralized.\nEac\nh\nrob\not\nlearned\nits\no\nwn\np\nolicy\nindep\nenden\ntly\nwithout\nexplicit\ncomm\nunication\nwith\nthe\nothers.\nThirdly\n,\nstate\nspace\nw\nas\nbrutally\nquan\ntized\nin\nto\na\nsmall\nn\num\nb\ner\nof\ndiscrete\nstates\naccording\nto\nv\nalues\nof\na\nsmall\nn\num-\nb\ner\nof\npre-programmed\nb\no\nolean\nfeatures\nof\nthe\nunderlying\nsensors.\nThe\np\nerformance\nof\nthe\nQ-learned\np\nolicies\nw\nere\nalmost\nas\ngo\no\nd\nas\na\nsimple\nhand-crafted\ncon\ntroller\nfor\nthe\njob.\n\u0004.\nQ-learning\nhas\nb\neen\nused\nin\nan\nelev\nator\ndispatc\nhing\ntask\n(Crites\n&\nBarto,\n\u0001\t\t\u0006).\nThe\nproblem,\nwhic\nh\nhas\nb\neen\nimplemen\nted\nin\nsim\nulation\nonly\nat\nthis\nstage,\nin\nv\nolv\ned\nfour\nelev\nators\nservicing\nten\n\ro\nors.\nThe\nob\njectiv\ne\nw\nas\nto\nminimize\nthe\na\nv\nerage\nsquared\nw\nait\ntime\nfor\npassengers,\ndiscoun\nted\nin\nto\nfuture\ntime.\nThe\nproblem\ncan\nb\ne\np\nosed\nas\na\ndiscrete\nMark\no\nv\nsystem,\nbut\nthere\nare\n\u00010\n\u0002\u0002\nstates\nev\nen\nin\nthe\nmost\nsimpli\fed\nv\nersion\nof\nthe\nproblem.\nCrites\nand\nBarto\nused\nneural\nnet\nw\norks\nfor\nfunction\nappro\nximation\nand\npro\nvided\nan\nexcellen\nt\ncomparison\nstudy\nof\ntheir\nQ-learning\napproac\nh\nagainst\nthe\nmost\np\nopular\nand\nthe\nmost\nsophisticated\nelev\nator\ndispatc\nhing\nalgorithms.\nThe\nsquared\nw\nait\ntime\nof\ntheir\ncon\ntroller\nw\nas\nappro\nximately\n\u0007%\nless\nthan\nthe\nb\nest\nalternativ\ne\nalgorithm\n(\\Empt\ny\nthe\nSystem\"\nheuristic\nwith\na\nreceding\nhorizon\ncon\ntroller)\nand\nless\nthan\nhalf\nthe\nsquared\nw\nait\ntime\nof\nthe\ncon\ntroller\nmost\nfrequen\ntly\nused\nin\nreal\nelev\nator\nsystems.\n\u0005.\nThe\n\fnal\nexample\nconcerns\nan\napplication\nof\nreinforcemen\nt\nlearning\nb\ny\none\nof\nthe\nauthors\nof\nthis\nsurv\ney\nto\na\npac\nk\naging\ntask\nfrom\na\nfo\no\nd\npro\ncessing\nindustry\n.\nThe\nproblem\nin\nv\nolv\nes\n\flling\ncon\ntainers\nwith\nv\nariable\nn\num\nb\ners\nof\nnon-iden\ntical\npro\nducts.\nThe\npro\nduct\nc\nharacteristics\nalso\nv\nary\nwith\ntime,\nbut\ncan\nb\ne\nsensed.\nDep\nending\non\nthe\ntask,\nv\narious\nconstrain\nts\nare\nplaced\non\nthe\ncon\ntainer-\flling\npro\ncedure.\nHere\nare\nthree\nexamples:\n\u000f\nThe\nmean\nw\neigh\nt\nof\nall\ncon\ntainers\npro\nduced\nb\ny\na\nshift\nm\nust\nnot\nb\ne\nb\nelo\nw\nthe\nman\nufacturer's\ndeclared\nw\neigh\nt\nW\n.\n\u0002\u0007\u0003\nKaelbling,\nLittman,\n&\nMoore\n\u000f\nThe\nn\num\nb\ner\nof\ncon\ntainers\nb\nelo\nw\nthe\ndeclared\nw\neigh\nt\nm\nust\nb\ne\nless\nthan\nP\n%.\n\u000f\nNo\ncon\ntainers\nma\ny\nb\ne\npro\nduced\nb\nelo\nw\nw\neigh\nt\nW\n0\n.\nSuc\nh\ntasks\nare\ncon\ntrolled\nb\ny\nmac\nhinery\nwhic\nh\nop\nerates\naccording\nto\nv\narious\nsetp\noints.\nCon\nv\nen\ntional\npractice\nis\nthat\nsetp\noin\nts\nare\nc\nhosen\nb\ny\nh\numan\nop\nerators,\nbut\nthis\nc\nhoice\nis\nnot\neasy\nas\nit\nis\ndep\nenden\nt\non\nthe\ncurren\nt\npro\nduct\nc\nharacteristics\nand\nthe\ncurren\nt\ntask\nconstrain\nts.\nThe\ndep\nendency\nis\noften\ndi\u000ecult\nto\nmo\ndel\nand\nhighly\nnon-linear.\nThe\ntask\nw\nas\np\nosed\nas\na\n\fnite-horizon\nMark\no\nv\ndecision\ntask\nin\nwhic\nh\nthe\nstate\nof\nthe\nsystem\nis\na\nfunction\nof\nthe\npro\nduct\nc\nharacteristics,\nthe\namoun\nt\nof\ntime\nremaining\nin\nthe\npro\nduction\nshift\nand\nthe\nmean\nw\nastage\nand\np\nercen\nt\nb\nelo\nw\ndeclared\nin\nthe\nshift\nso\nfar.\nThe\nsystem\nw\nas\ndiscretized\nin\nto\n\u000200,000\ndiscrete\nstates\nand\nlo\ncal\nw\neigh\nted\nregression\nw\nas\nused\nto\nlearn\nand\ngeneralize\na\ntransition\nmo\ndel.\nPrioritized\nsw\neep-\ning\nw\nas\nused\nto\nmain\ntain\nan\noptimal\nv\nalue\nfunction\nas\neac\nh\nnew\npiece\nof\ntransition\ninformation\nw\nas\nobtained.\nIn\nsim\nulated\nexp\nerimen\nts\nthe\nsa\nvings\nw\nere\nconsiderable,\nt\nypically\nwith\nw\nastage\nreduced\nb\ny\na\nfactor\nof\nten.\nSince\nthen\nthe\nsystem\nhas\nb\neen\ndeplo\ny\ned\nsuccessfully\nin\nsev\neral\nfactories\nwithin\nthe\nUnited\nStates.\nSome\nin\nteresting\nasp\nects\nof\npractical\nreinforcemen\nt\nlearning\ncome\nto\nligh\nt\nfrom\nthese\nexamples.\nThe\nmost\nstriking\nis\nthat\nin\nall\ncases,\nto\nmak\ne\na\nreal\nsystem\nw\nork\nit\npro\nv\ned\nnecessary\nto\nsupplemen\nt\nthe\nfundamen\ntal\nalgorithm\nwith\nextra\npre-programmed\nkno\nwledge.\nSupplying\nextra\nkno\nwledge\ncomes\nat\na\nprice:\nmore\nh\numan\ne\u000bort\nand\ninsigh\nt\nis\nrequired\nand\nthe\nsystem\nis\nsubsequen\ntly\nless\nautonomous.\nBut\nit\nis\nalso\nclear\nthat\nfor\ntasks\nsuc\nh\nas\nthese,\na\nkno\nwledge-free\napproac\nh\nw\nould\nnot\nha\nv\ne\nac\nhiev\ned\nw\north\nwhile\np\nerformance\nwithin\nthe\n\fnite\nlifetime\nof\nthe\nrob\nots.\nWhat\nforms\ndid\nthis\npre-programmed\nkno\nwledge\ntak\ne?\nIt\nincluded\nan\nassumption\nof\nlinearit\ny\nfor\nthe\njuggling\nrob\not's\np\nolicy\n,\na\nman\nual\nbreaking\nup\nof\nthe\ntask\nin\nto\nsubtasks\nfor\nthe\nt\nw\no\nmobile-rob\not\nexamples,\nwhile\nthe\nb\no\nx-pusher\nalso\nused\na\nclustering\ntec\nhnique\nfor\nthe\nQ\nv\nalues\nwhic\nh\nassumed\nlo\ncally\nconsisten\nt\nQ\nv\nalues.\nThe\nfour\ndisk-collecting\nrob\nots\nadditionally\nused\na\nman\nually\ndiscretized\nstate\nspace.\nThe\npac\nk\naging\nexample\nhad\nfar\nfew\ner\ndimensions\nand\nso\nrequired\ncorresp\nondingly\nw\neak\ner\nassumptions,\nbut\nthere,\nto\no,\nthe\nas-\nsumption\nof\nlo\ncal\npiecewise\ncon\ntin\nuit\ny\nin\nthe\ntransition\nmo\ndel\nenabled\nmassiv\ne\nreductions\nin\nthe\namoun\nt\nof\nlearning\ndata\nrequired.\nThe\nexploration\nstrategies\nare\nin\nteresting\nto\no.\nThe\njuggler\nused\ncareful\nstatistical\nanal-\nysis\nto\njudge\nwhere\nto\npro\ftably\nexp\nerimen\nt.\nHo\nw\nev\ner,\nb\noth\nmobile\nrob\not\napplications\nw\nere\nable\nto\nlearn\nw\nell\nwith\ngreedy\nexploration|alw\na\nys\nexploiting\nwithout\ndelib\nerate\nex-\nploration.\nThe\npac\nk\naging\ntask\nused\noptimism\nin\nthe\nface\nof\nuncertain\nt\ny\n.\nNone\nof\nthese\nstrategies\nmirrors\ntheoretically\noptimal\n(but\ncomputationally\nin\ntractable)\nexploration,\nand\ny\net\nall\npro\nv\ned\nadequate.\nFinally\n,\nit\nis\nalso\nw\north\nconsidering\nthe\ncomputational\nregimes\nof\nthese\nexp\nerimen\nts.\nThey\nw\nere\nall\nv\nery\ndi\u000beren\nt,\nwhic\nh\nindicates\nthat\nthe\ndi\u000bering\ncomputational\ndemands\nof\nv\narious\nreinforcemen\nt\nlearning\nalgorithms\ndo\nindeed\nha\nv\ne\nan\narra\ny\nof\ndi\u000bering\napplications.\nThe\njuggler\nneeded\nto\nmak\ne\nv\nery\nfast\ndecisions\nwith\nlo\nw\nlatency\nb\net\nw\neen\neac\nh\nhit,\nbut\nhad\nlong\np\nerio\nds\n(\u00030\nseconds\nand\nmore)\nb\net\nw\neen\neac\nh\ntrial\nto\nconsolidate\nthe\nexp\neriences\ncollected\non\nthe\nprevious\ntrial\nand\nto\np\nerform\nthe\nmore\naggressiv\ne\ncomputation\nnecessary\nto\npro\nduce\na\nnew\nreactiv\ne\ncon\ntroller\non\nthe\nnext\ntrial.\nThe\nb\no\nx-pushing\nrob\not\nw\nas\nmean\nt\nto\n\u0002\u0007\u0004\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nop\nerate\nautonomously\nfor\nhours\nand\nso\nhad\nto\nmak\ne\ndecisions\nwith\na\nuniform\nlength\ncon\ntrol\ncycle.\nThe\ncycle\nw\nas\nsu\u000ecien\ntly\nlong\nfor\nquite\nsubstan\ntial\ncomputations\nb\ney\nond\nsimple\nQ-\nlearning\nbac\nkups.\nThe\nfour\ndisk-collecting\nrob\nots\nw\nere\nparticularly\nin\nteresting.\nEac\nh\nrob\not\nhad\na\nshort\nlife\nof\nless\nthan\n\u00020\nmin\nutes\n(due\nto\nbattery\nconstrain\nts)\nmeaning\nthat\nsubstan\ntial\nn\num\nb\ner\ncrunc\nhing\nw\nas\nimpractical,\nand\nan\ny\nsigni\fcan\nt\ncom\nbinatorial\nsearc\nh\nw\nould\nha\nv\ne\nused\na\nsigni\fcan\nt\nfraction\nof\nthe\nrob\not's\nlearning\nlifetime.\nThe\npac\nk\naging\ntask\nhad\neasy\nconstrain\nts.\nOne\ndecision\nw\nas\nneeded\nev\nery\nfew\nmin\nutes.\nThis\npro\nvided\nopp\nortunities\nfor\nfully\ncomputing\nthe\noptimal\nv\nalue\nfunction\nfor\nthe\n\u000200,000-state\nsystem\nb\net\nw\neen\nev\nery\ncon\ntrol\ncycle,\nin\naddition\nto\np\nerforming\nmassiv\ne\ncross-v\nalidation-based\noptimization\nof\nthe\ntransition\nmo\ndel\nb\neing\nlearned.\nA\ngreat\ndeal\nof\nfurther\nw\nork\nis\ncurren\ntly\nin\nprogress\non\npractical\nimplemen\ntations\nof\nreinforcemen\nt\nlearning.\nThe\ninsigh\nts\nand\ntask\nconstrain\nts\nthat\nthey\npro\nduce\nwill\nha\nv\ne\nan\nimp\nortan\nt\ne\u000bect\non\nshaping\nthe\nkind\nof\nalgorithms\nthat\nare\ndev\nelop\ned\nin\nfuture.\n\t.\nConclusions\nThere\nare\na\nv\nariet\ny\nof\nreinforcemen\nt-learning\ntec\nhniques\nthat\nw\nork\ne\u000bectiv\nely\non\na\nv\nariet\ny\nof\nsmall\nproblems.\nBut\nv\nery\nfew\nof\nthese\ntec\nhniques\nscale\nw\nell\nto\nlarger\nproblems.\nThis\nis\nnot\nb\necause\nresearc\nhers\nha\nv\ne\ndone\na\nbad\njob\nof\nin\nv\nen\nting\nlearning\ntec\nhniques,\nbut\nb\necause\nit\nis\nv\nery\ndi\u000ecult\nto\nsolv\ne\narbitrary\nproblems\nin\nthe\ngeneral\ncase.\nIn\norder\nto\nsolv\ne\nhighly\ncomplex\nproblems,\nw\ne\nm\nust\ngiv\ne\nup\ntabula\nr\nasa\nlearning\ntec\nhniques\nand\nb\negin\nto\nincorp\norate\nbias\nthat\nwill\ngiv\ne\nlev\nerage\nto\nthe\nlearning\npro\ncess.\nThe\nnecessary\nbias\ncan\ncome\nin\na\nv\nariet\ny\nof\nforms,\nincluding\nthe\nfollo\nwing:\nshaping:\nThe\ntec\nhnique\nof\nshaping\nis\nused\nin\ntraining\nanimals\n(Hilgard\n&\nBo\nw\ner,\n\u0001\t\u0007\u0005);\na\nteac\nher\npresen\nts\nv\nery\nsimple\nproblems\nto\nsolv\ne\n\frst,\nthen\ngradually\nexp\noses\nthe\nlearner\nto\nmore\ncomplex\nproblems.\nShaping\nhas\nb\neen\nused\nin\nsup\nervised-learning\nsystems,\nand\ncan\nb\ne\nused\nto\ntrain\nhierarc\nhical\nreinforcemen\nt-learning\nsystems\nfrom\nthe\nb\nottom\nup\n(Lin,\n\u0001\t\t\u0001),\nand\nto\nalleviate\nproblems\nof\ndela\ny\ned\nreinforcemen\nt\nb\ny\ndecreasing\nthe\ndela\ny\nun\ntil\nthe\nproblem\nis\nw\nell\nundersto\no\nd\n(Dorigo\n&\nColom\nb\netti,\n\u0001\t\t\u0004;\nDorigo,\n\u0001\t\t\u0005).\nlo\ncal\nreinforcemen\nt\nsignals:\nWhenev\ner\np\nossible,\nagen\nts\nshould\nb\ne\ngiv\nen\nreinforcemen\nt\nsignals\nthat\nare\nlo\ncal.\nIn\napplications\nin\nwhic\nh\nit\nis\np\nossible\nto\ncompute\na\ngradien\nt,\nrew\narding\nthe\nagen\nt\nfor\ntaking\nsteps\nup\nthe\ngradien\nt,\nrather\nthan\njust\nfor\nac\nhieving\nthe\n\fnal\ngoal,\ncan\nsp\need\nlearning\nsigni\fcan\ntly\n(Mataric,\n\u0001\t\t\u0004).\nimitation:\nAn\nagen\nt\ncan\nlearn\nb\ny\n\\w\natc\nhing\"\nanother\nagen\nt\np\nerform\nthe\ntask\n(Lin,\n\u0001\t\t\u0001).\nF\nor\nreal\nrob\nots,\nthis\nrequires\np\nerceptual\nabilities\nthat\nare\nnot\ny\net\na\nv\nailable.\nBut\nanother\nstrategy\nis\nto\nha\nv\ne\na\nh\numan\nsupply\nappropriate\nmotor\ncommands\nto\na\nrob\not\nthrough\na\njo\nystic\nk\nor\nsteering\nwheel\n(P\nomerleau,\n\u0001\t\t\u0003).\nproblem\ndecomp\nosition:\nDecomp\nosing\na\nh\nuge\nlearning\nproblem\nin\nto\na\ncollection\nof\nsmaller\nones,\nand\npro\nviding\nuseful\nreinforcemen\nt\nsignals\nfor\nthe\nsubproblems\nis\na\nv\nery\np\no\nw\ner-\nful\ntec\nhnique\nfor\nbiasing\nlearning.\nMost\nin\nteresting\nexamples\nof\nrob\notic\nreinforcemen\nt\nlearning\nemplo\ny\nthis\ntec\nhnique\nto\nsome\nexten\nt\n(Connell\n&\nMahadev\nan,\n\u0001\t\t\u0003).\nre\rexes:\nOne\nthing\nthat\nk\neeps\nagen\nts\nthat\nkno\nw\nnothing\nfrom\nlearning\nan\nything\nis\nthat\nthey\nha\nv\ne\na\nhard\ntime\nev\nen\n\fnding\nthe\nin\nteresting\nparts\nof\nthe\nspace;\nthey\nw\nander\n\u0002\u0007\u0005\nKaelbling,\nLittman,\n&\nMoore\naround\nat\nrandom\nnev\ner\ngetting\nnear\nthe\ngoal,\nor\nthey\nare\nalw\na\nys\n\\killed\"\nimmediately\n.\nThese\nproblems\ncan\nb\ne\nameliorated\nb\ny\nprogramming\na\nset\nof\n\\re\rexes\"\nthat\ncause\nthe\nagen\nt\nto\nact\ninitially\nin\nsome\nw\na\ny\nthat\nis\nreasonable\n(Mataric,\n\u0001\t\t\u0004;\nSingh,\nBarto,\nGrup\nen,\n&\nConnolly\n,\n\u0001\t\t\u0004).\nThese\nre\rexes\ncan\nev\nen\ntually\nb\ne\no\nv\nerridden\nb\ny\nmore\ndetailed\nand\naccurate\nlearned\nkno\nwledge,\nbut\nthey\nat\nleast\nk\neep\nthe\nagen\nt\naliv\ne\nand\np\noin\nted\nin\nthe\nrigh\nt\ndirection\nwhile\nit\nis\ntrying\nto\nlearn.\nRecen\nt\nw\nork\nb\ny\nMillan\n(\u0001\t\t\u0006)\nexplores\nthe\nuse\nof\nre\rexes\nto\nmak\ne\nrob\not\nlearning\nsafer\nand\nmore\ne\u000ecien\nt.\nWith\nappropriate\nbiases,\nsupplied\nb\ny\nh\numan\nprogrammers\nor\nteac\nhers,\ncomplex\nreinforcemen\nt-\nlearning\nproblems\nwill\nev\nen\ntually\nb\ne\nsolv\nable.\nThere\nis\nstill\nm\nuc\nh\nw\nork\nto\nb\ne\ndone\nand\nman\ny\nin\nteresting\nquestions\nremaining\nfor\nlearning\ntec\nhniques\nand\nesp\necially\nregarding\nmetho\nds\nfor\nappro\nximating,\ndecomp\nosing,\nand\nincorp\norating\nbias\nin\nto\nproblems.\nAc\nkno\nwledgemen\nts\nThanks\nto\nMarco\nDorigo\nand\nthree\nanon\nymous\nreview\ners\nfor\ncommen\nts\nthat\nha\nv\ne\nhelp\ned\nto\nimpro\nv\ne\nthis\npap\ner.\nAlso\nthanks\nto\nour\nman\ny\ncolleagues\nin\nthe\nreinforcemen\nt-learning\ncomm\nunit\ny\nwho\nha\nv\ne\ndone\nthis\nw\nork\nand\nexplained\nit\nto\nus.\nLeslie\nP\nac\nk\nKaelbling\nw\nas\nsupp\norted\nin\npart\nb\ny\nNSF\ngran\nts\nIRI-\t\u0004\u0005\u0003\u0003\b\u0003\nand\nIRI-\n\t\u0003\u0001\u0002\u0003\t\u0005.\nMic\nhael\nLittman\nw\nas\nsupp\norted\nin\npart\nb\ny\nBellcore.\nAndrew\nMo\nore\nw\nas\nsupp\norted\nin\npart\nb\ny\nan\nNSF\nResearc\nh\nInitiation\nAw\nard\nand\nb\ny\n\u0003M\nCorp\noration.\nReferences\nAc\nkley\n,\nD.\nH.,\n&\nLittman,\nM.\nL.\n(\u0001\t\t0).\nGeneralization\nand\nscaling\nin\nreinforcemen\nt\nlearn-\ning.\nIn\nT\nouretzky\n,\nD.\nS.\n(Ed.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0002,\npp.\n\u0005\u00050{\u0005\u0005\u0007\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nAlbus,\nJ.\nS.\n(\u0001\t\u0007\u0005).\nA\nnew\napproac\nh\nto\nmanipulator\ncon\ntrol:\nCereb\nellar\nmo\ndel\narticulation\ncon\ntroller\n(cmac).\nJournal\nof\nDynamic\nSystems,\nMe\nasur\nement\nand\nContr\nol,\n\t\u0007,\n\u0002\u00020{\n\u0002\u0002\u0007.\nAlbus,\nJ.\nS.\n(\u0001\t\b\u0001).\nBr\nains,\nBehavior,\nand\nR\nob\notics.\nBYTE\nBo\noks,\nSubsidiary\nof\nMcGra\nw-\nHill,\nP\neterb\norough,\nNew\nHampshire.\nAnderson,\nC.\nW.\n(\u0001\t\b\u0006).\nL\ne\narning\nand\nPr\noblem\nSolving\nwith\nMultilayer\nConne\nctionist\nSystems.\nPh.D.\nthesis,\nUniv\nersit\ny\nof\nMassac\nh\nusetts,\nAmherst,\nMA.\nAshar,\nR.\nR.\n(\u0001\t\t\u0004).\nHierarc\nhical\nlearning\nin\nsto\nc\nhastic\ndomains.\nMaster's\nthesis,\nBro\nwn\nUniv\nersit\ny\n,\nPro\nvidence,\nRho\nde\nIsland.\nBaird,\nL.\n(\u0001\t\t\u0005).\nResidual\nalgorithms:\nReinforcemen\nt\nlearning\nwith\nfunction\nappro\nxima-\ntion.\nIn\nPrieditis,\nA.,\n&\nRussell,\nS.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u00030{\u0003\u0007\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nBaird,\nL.\nC.,\n&\nKlopf,\nA.\nH.\n(\u0001\t\t\u0003).\nReinforcemen\nt\nlearning\nwith\nhigh-dimensional,\ncon-\ntin\nuous\nactions.\nT\nec\nh.\nrep.\nWL-TR-\t\u0003-\u0001\u0001\u0004\u0007,\nW\nrigh\nt-P\natterson\nAir\nF\norce\nBase\nOhio:\nWrigh\nt\nLab\noratory\n.\n\u0002\u0007\u0006\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nBarto,\nA.\nG.,\nBradtk\ne,\nS.\nJ.,\n&\nSingh,\nS.\nP\n.\n(\u0001\t\t\u0005).\nLearning\nto\nact\nusing\nreal-time\ndynamic\nprogramming.\nA\nrti\fcial\nIntel\nligenc\ne,\n\u0007\u0002\n(\u0001),\n\b\u0001{\u0001\u0003\b.\nBarto,\nA.\nG.,\nSutton,\nR.\nS.,\n&\nAnderson,\nC.\nW.\n(\u0001\t\b\u0003).\nNeuronlik\ne\nadaptiv\ne\nelemen\nts\nthat\ncan\nsolv\ne\ndi\u000ecult\nlearning\ncon\ntrol\nproblems.\nIEEE\nT\nr\nansactions\non\nSystems,\nMan,\nand\nCyb\nernetics,\nSMC-\u0001\u0003\n(\u0005),\n\b\u0003\u0004{\b\u0004\u0006.\nBellman,\nR.\n(\u0001\t\u0005\u0007).\nDynamic\nPr\no\ngr\namming.\nPrinceton\nUniv\nersit\ny\nPress,\nPrinceton,\nNJ.\nBerenji,\nH.\nR.\n(\u0001\t\t\u0001).\nArti\fcial\nneural\nnet\nw\norks\nand\nappro\nximate\nreasoning\nfor\nin\ntelligen\nt\ncon\ntrol\nin\nspace.\nIn\nA\nmeric\nan\nContr\nol\nConfer\nenc\ne,\npp.\n\u00010\u0007\u0005{\u00010\b0.\nBerry\n,\nD.\nA.,\n&\nF\nristedt,\nB.\n(\u0001\t\b\u0005).\nBandit\nPr\noblems:\nSe\nquential\nA\nl\nlo\nc\nation\nof\nExp\neriments.\nChapman\nand\nHall,\nLondon,\nUK.\nBertsek\nas,\nD.\nP\n.\n(\u0001\t\b\u0007).\nDynamic\nPr\no\ngr\namming:\nDeterministic\nand\nSto\nchastic\nMo\ndels.\nPren\ntice-Hall,\nEnglew\no\no\nd\nCli\u000bs,\nNJ.\nBertsek\nas,\nD.\nP\n.\n(\u0001\t\t\u0005).\nDynamic\nPr\no\ngr\namming\nand\nOptimal\nContr\nol.\nA\nthena\nScien\nti\fc,\nBelmon\nt,\nMassac\nh\nusetts.\nV\nolumes\n\u0001\nand\n\u0002.\nBertsek\nas,\nD.\nP\n.,\n&\nCasta\n~\nnon,\nD.\nA.\n(\u0001\t\b\t).\nAdaptiv\ne\naggregation\nfor\nin\fnite\nhorizon\ndynamic\nprogramming.\nIEEE\nT\nr\nansactions\non\nA\nutomatic\nContr\nol,\n\u0003\u0004\n(\u0006),\n\u0005\b\t{\u0005\t\b.\nBertsek\nas,\nD.\nP\n.,\n&\nTsitsiklis,\nJ.\nN.\n(\u0001\t\b\t).\nPar\nal\nlel\nand\nDistribute\nd\nComputation:\nNumer-\nic\nal\nMetho\nds.\nPren\ntice-Hall,\nEnglew\no\no\nd\nCli\u000bs,\nNJ.\nBo\nx,\nG.\nE.\nP\n.,\n&\nDrap\ner,\nN.\nR.\n(\u0001\t\b\u0007).\nEmpiric\nal\nMo\ndel-Building\nand\nR\nesp\nonse\nSurfac\nes.\nWiley\n.\nBo\ny\nan,\nJ.\nA.,\n&\nMo\nore,\nA.\nW.\n(\u0001\t\t\u0005).\nGeneralization\nin\nreinforcemen\nt\nlearning:\nSafely\nappro\nximating\nthe\nv\nalue\nfunction.\nIn\nT\nesauro,\nG.,\nT\nouretzky\n,\nD.\nS.,\n&\nLeen,\nT.\nK.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0007\nCam\nbridge,\nMA.\nThe\nMIT\nPress.\nBurghes,\nD.,\n&\nGraham,\nA.\n(\u0001\t\b0).\nIntr\no\nduction\nto\nContr\nol\nThe\nory\nincluding\nOptimal\nContr\nol.\nEllis\nHorw\no\no\nd.\nCassandra,\nA.\nR.,\nKaelbling,\nL.\nP\n.,\n&\nLittman,\nM.\nL.\n(\u0001\t\t\u0004).\nActing\noptimally\nin\npartially\nobserv\nable\nsto\nc\nhastic\ndomains.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne\nSeattle,\nW\nA.\nChapman,\nD.,\n&\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0001).\nInput\ngeneralization\nin\ndela\ny\ned\nreinforcemen\nt\nlearning:\nAn\nalgorithm\nand\np\nerformance\ncomparisons.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nInterna-\ntional\nJoint\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne\nSydney\n,\nAustralia.\nChrisman,\nL.\n(\u0001\t\t\u0002).\nReinforcemen\nt\nlearning\nwith\np\nerceptual\naliasing:\nThe\np\nerceptual\ndistinctions\napproac\nh.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nT\nenth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne,\npp.\n\u0001\b\u0003{\u0001\b\b\nSan\nJose,\nCA.\nAAAI\nPress.\n\u0002\u0007\u0007\nKaelbling,\nLittman,\n&\nMoore\nChrisman,\nL.,\n&\nLittman,\nM.\n(\u0001\t\t\u0003).\nHidden\nstate\nand\nshort-term\nmemory\n..\nPresen\ntation\nat\nReinforcemen\nt\nLearning\nW\norkshop,\nMac\nhine\nLearning\nConference.\nCic\nhosz,\nP\n.,\n&\nMula\nwk\na,\nJ.\nJ.\n(\u0001\t\t\u0005).\nF\nast\nand\ne\u000ecien\nt\nreinforcemen\nt\nlearning\nwith\ntrun-\ncated\ntemp\noral\ndi\u000berences.\nIn\nPrieditis,\nA.,\n&\nRussell,\nS.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\t\t{\u00010\u0007\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nClev\neland,\nW.\nS.,\n&\nDelvin,\nS.\nJ.\n(\u0001\t\b\b).\nLo\ncally\nw\neigh\nted\nregression:\nAn\napproac\nh\nto\nregression\nanalysis\nb\ny\nlo\ncal\n\ftting.\nJournal\nof\nthe\nA\nmeric\nan\nStatistic\nal\nAsso\nciation,\n\b\u0003\n(\u00040\u0003),\n\u0005\t\u0006{\u0006\u00010.\nCli\u000b,\nD.,\n&\nRoss,\nS.\n(\u0001\t\t\u0004).\nAdding\ntemp\norary\nmemory\nto\nZCS.\nA\ndaptive\nBehavior,\n\u0003\n(\u0002),\n\u00010\u0001{\u0001\u00050.\nCondon,\nA.\n(\u0001\t\t\u0002).\nThe\ncomplexit\ny\nof\nsto\nc\nhastic\ngames.\nInformation\nand\nComputation,\n\t\u0006\n(\u0002),\n\u00020\u0003{\u0002\u0002\u0004.\nConnell,\nJ.,\n&\nMahadev\nan,\nS.\n(\u0001\t\t\u0003).\nRapid\ntask\nlearning\nfor\nreal\nrob\nots.\nIn\nR\nob\not\nL\ne\narning.\nKlu\nw\ner\nAcademic\nPublishers.\nCrites,\nR.\nH.,\n&\nBarto,\nA.\nG.\n(\u0001\t\t\u0006).\nImpro\nving\nelev\nator\np\nerformance\nusing\nreinforcemen\nt\nlearning.\nIn\nT\nouretzky\n,\nD.,\nMozer,\nM.,\n&\nHasselmo,\nM.\n(Eds.),\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\b.\nDa\ny\nan,\nP\n.\n(\u0001\t\t\u0002).\nThe\ncon\nv\nergence\nof\nTD(\u0015)\nfor\ngeneral\n\u0015.\nMachine\nL\ne\narning,\n\b\n(\u0003),\n\u0003\u0004\u0001{\n\u0003\u0006\u0002.\nDa\ny\nan,\nP\n.,\n&\nHin\nton,\nG.\nE.\n(\u0001\t\t\u0003).\nF\neudal\nreinforcemen\nt\nlearning.\nIn\nHanson,\nS.\nJ.,\nCo\nw\nan,\nJ.\nD.,\n&\nGiles,\nC.\nL.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0005\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nDa\ny\nan,\nP\n.,\n&\nSejno\nwski,\nT.\nJ.\n(\u0001\t\t\u0004).\nTD(\u0015)\ncon\nv\nerges\nwith\nprobabilit\ny\n\u0001.\nMachine\nL\ne\narn-\ning,\n\u0001\u0004\n(\u0003).\nDean,\nT.,\nKaelbling,\nL.\nP\n.,\nKirman,\nJ.,\n&\nNic\nholson,\nA.\n(\u0001\t\t\u0003).\nPlanning\nwith\ndeadlines\nin\nsto\nc\nhastic\ndomains.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne\nW\nashington,\nDC.\nD'Ep\nenoux,\nF.\n(\u0001\t\u0006\u0003).\nA\nprobabilistic\npro\nduction\nand\nin\nv\nen\ntory\nproblem.\nManagement\nScienc\ne,\n\u00010,\n\t\b{\u00010\b.\nDerman,\nC.\n(\u0001\t\u00070).\nFinite\nState\nMarkovian\nDe\ncision\nPr\no\nc\nesses.\nAcademic\nPress,\nNew\nY\nork.\nDorigo,\nM.,\n&\nBersini,\nH.\n(\u0001\t\t\u0004).\nA\ncomparison\nof\nq-learning\nand\nclassi\fer\nsystems.\nIn\nF\nr\nom\nA\nnimals\nto\nA\nnimats:\nPr\no\nc\ne\ne\ndings\nof\nthe\nThir\nd\nInternational\nConfer\nenc\ne\non\nthe\nSimulation\nof\nA\ndaptive\nBehavior\nBrigh\nton,\nUK.\nDorigo,\nM.,\n&\nColom\nb\netti,\nM.\n(\u0001\t\t\u0004).\nRob\not\nshaping:\nDev\neloping\nautonomous\nagen\nts\nthrough\nlearning.\nA\nrti\fcial\nIntel\nligenc\ne,\n\u0007\u0001\n(\u0002),\n\u0003\u0002\u0001{\u0003\u00070.\n\u0002\u0007\b\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nDorigo,\nM.\n(\u0001\t\t\u0005).\nAlecsys\nand\nthe\nAutonoMouse:\nLearning\nto\ncon\ntrol\na\nreal\nrob\not\nb\ny\ndistributed\nclassi\fer\nsystems.\nMachine\nL\ne\narning,\n\u0001\t.\nFiec\nh\nter,\nC.-N.\n(\u0001\t\t\u0004).\nE\u000ecien\nt\nreinforcemen\nt\nlearning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nSeventh\nA\nnnual\nA\nCM\nConfer\nenc\ne\non\nComputational\nL\ne\narning\nThe\nory,\npp.\n\b\b{\t\u0007.\nAsso\nciation\nof\nComputing\nMac\nhinery\n.\nGittins,\nJ.\nC.\n(\u0001\t\b\t).\nMulti-arme\nd\nBandit\nA\nl\nlo\nc\nation\nIndic\nes.\nWiley-In\nterscience\nseries\nin\nsystems\nand\noptimization.\nWiley\n,\nChic\nhester,\nNY.\nGoldb\nerg,\nD.\n(\u0001\t\b\t).\nGenetic\nalgorithms\nin\nse\nar\nch,\noptimization,\nand\nmachine\nle\narning.\nAddison-W\nesley\n,\nMA.\nGordon,\nG.\nJ.\n(\u0001\t\t\u0005).\nStable\nfunction\nappro\nximation\nin\ndynamic\nprogramming.\nIn\nPriedi-\ntis,\nA.,\n&\nRussell,\nS.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0002\u0006\u0001{\u0002\u0006\b\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nGullapalli,\nV.\n(\u0001\t\t0).\nA\nsto\nc\nhastic\nreinforcemen\nt\nlearning\nalgorithm\nfor\nlearning\nreal-v\nalued\nfunctions.\nNeur\nal\nNetworks,\n\u0003,\n\u0006\u0007\u0001{\u0006\t\u0002.\nGullapalli,\nV.\n(\u0001\t\t\u0002).\nR\neinfor\nc\nement\nle\narning\nand\nits\napplic\nation\nto\nc\nontr\nol.\nPh.D.\nthesis,\nUniv\nersit\ny\nof\nMassac\nh\nusetts,\nAmherst,\nMA.\nHilgard,\nE.\nR.,\n&\nBo\nw\ner,\nG.\nH.\n(\u0001\t\u0007\u0005).\nThe\nories\nof\nL\ne\narning\n(fourth\nedition).\nPren\ntice-Hall,\nEnglew\no\no\nd\nCli\u000bs,\nNJ.\nHo\u000bman,\nA.\nJ.,\n&\nKarp,\nR.\nM.\n(\u0001\t\u0006\u0006).\nOn\nnon\nterminating\nsto\nc\nhastic\ngames.\nManagement\nScienc\ne,\n\u0001\u0002,\n\u0003\u0005\t{\u0003\u00070.\nHolland,\nJ.\nH.\n(\u0001\t\u0007\u0005).\nA\ndaptation\nin\nNatur\nal\nand\nA\nrti\fcial\nSystems.\nUniv\nersit\ny\nof\nMic\nhigan\nPress,\nAnn\nArb\nor,\nMI.\nHo\nw\nard,\nR.\nA.\n(\u0001\t\u00060).\nDynamic\nPr\no\ngr\namming\nand\nMarkov\nPr\no\nc\nesses.\nThe\nMIT\nPress,\nCam\nbridge,\nMA.\nJaakk\nola,\nT.,\nJordan,\nM.\nI.,\n&\nSingh,\nS.\nP\n.\n(\u0001\t\t\u0004).\nOn\nthe\ncon\nv\nergence\nof\nsto\nc\nhastic\niterativ\ne\ndynamic\nprogramming\nalgorithms.\nNeur\nal\nComputation,\n\u0006\n(\u0006).\nJaakk\nola,\nT.,\nSingh,\nS.\nP\n.,\n&\nJordan,\nM.\nI.\n(\u0001\t\t\u0005).\nMon\nte-carlo\nreinforcemen\nt\nlearning\nin\nnon-Mark\no\nvian\ndecision\nproblems.\nIn\nT\nesauro,\nG.,\nT\nouretzky\n,\nD.\nS.,\n&\nLeen,\nT.\nK.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0007\nCam\nbridge,\nMA.\nThe\nMIT\nPress.\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0003a).\nHierarc\nhical\nlearning\nin\nsto\nc\nhastic\ndomains:\nPreliminary\nresults.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nT\nenth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning\nAmherst,\nMA.\nMorgan\nKaufmann.\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0003b).\nL\ne\narning\nin\nEmb\ne\ndde\nd\nSystems.\nThe\nMIT\nPress,\nCam\nbridge,\nMA.\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0004a).\nAsso\nciativ\ne\nreinforcemen\nt\nlearning:\nA\ngenerate\nand\ntest\nalgorithm.\nMachine\nL\ne\narning,\n\u0001\u0005\n(\u0003).\n\u0002\u0007\t\nKaelbling,\nLittman,\n&\nMoore\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0004b).\nAsso\nciativ\ne\nreinforcemen\nt\nlearning:\nFunctions\nin\nk\n-DNF.\nMachine\nL\ne\narning,\n\u0001\u0005\n(\u0003).\nKirman,\nJ.\n(\u0001\t\t\u0004).\nPr\ne\ndicting\nR\ne\nal-Time\nPlanner\nPerformanc\ne\nby\nDomain\nChar\nacterization.\nPh.D.\nthesis,\nDepartmen\nt\nof\nComputer\nScience,\nBro\nwn\nUniv\nersit\ny\n.\nKo\nenig,\nS.,\n&\nSimmons,\nR.\nG.\n(\u0001\t\t\u0003).\nComplexit\ny\nanalysis\nof\nreal-time\nreinforcemen\nt\nlearning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne,\npp.\n\t\t{\u00010\u0005\nMenlo\nP\nark,\nCalifornia.\nAAAI\nPress/MIT\nPress.\nKumar,\nP\n.\nR.,\n&\nV\naraiy\na,\nP\n.\nP\n.\n(\u0001\t\b\u0006).\nSto\nchastic\nSystems:\nEstimation,\nIdenti\fc\nation,\nand\nA\ndaptive\nContr\nol.\nPren\ntice\nHall,\nEnglew\no\no\nd\nCli\u000bs,\nNew\nJersey\n.\nLee,\nC.\nC.\n(\u0001\t\t\u0001).\nA\nself\nlearning\nrule-based\ncon\ntroller\nemplo\nying\nappro\nximate\nreasoning\nand\nneural\nnet\nconcepts.\nInternational\nJournal\nof\nIntel\nligent\nSystems,\n\u0006\n(\u0001),\n\u0007\u0001{\t\u0003.\nLin,\nL.-J.\n(\u0001\t\t\u0001).\nProgramming\nrob\nots\nusing\nreinforcemen\nt\nlearning\nand\nteac\nhing.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nNinth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne.\nLin,\nL.-J.\n(\u0001\t\t\u0003a).\nHierac\nhical\nlearning\nof\nrob\not\nskills\nb\ny\nreinforcemen\nt.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nInternational\nConfer\nenc\ne\non\nNeur\nal\nNetworks.\nLin,\nL.-J.\n(\u0001\t\t\u0003b).\nR\neinfor\nc\nement\nL\ne\narning\nfor\nR\nob\nots\nUsing\nNeur\nal\nNetworks.\nPh.D.\nthesis,\nCarnegie\nMellon\nUniv\nersit\ny\n,\nPittsburgh,\nP\nA.\nLin,\nL.-J.,\n&\nMitc\nhell,\nT.\nM.\n(\u0001\t\t\u0002).\nMemory\napproac\nhes\nto\nreinforcemen\nt\nlearning\nin\nnon-\nMark\no\nvian\ndomains.\nT\nec\nh.\nrep.\nCMU-CS-\t\u0002-\u0001\u0003\b,\nCarnegie\nMellon\nUniv\nersit\ny\n,\nSc\nho\nol\nof\nComputer\nScience.\nLittman,\nM.\nL.\n(\u0001\t\t\u0004a).\nMark\no\nv\ngames\nas\na\nframew\nork\nfor\nm\nulti-agen\nt\nreinforcemen\nt\nlearn-\ning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0001\u0005\u0007{\u0001\u0006\u0003\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nLittman,\nM.\nL.\n(\u0001\t\t\u0004b).\nMemoryless\np\nolicies:\nTheoretical\nlimitations\nand\npractical\nresults.\nIn\nCli\u000b,\nD.,\nHusbands,\nP\n.,\nMey\ner,\nJ.-A.,\n&\nWilson,\nS.\nW.\n(Eds.),\nF\nr\nom\nA\nnimals\nto\nA\nnimats\n\u0003:\nPr\no\nc\ne\ne\ndings\nof\nthe\nThir\nd\nInternational\nConfer\nenc\ne\non\nSimulation\nof\nA\ndaptive\nBehavior\nCam\nbridge,\nMA.\nThe\nMIT\nPress.\nLittman,\nM.\nL.,\nCassandra,\nA.,\n&\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0005a).\nLearning\np\nolicies\nfor\npartially\nobserv\nable\nen\nvironmen\nts:\nScaling\nup.\nIn\nPrieditis,\nA.,\n&\nRussell,\nS.\n(Eds.),\nPr\no\nc\ne\ne\nd-\nings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0003\u0006\u0002{\u0003\u00070\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nLittman,\nM.\nL.,\nDean,\nT.\nL.,\n&\nKaelbling,\nL.\nP\n.\n(\u0001\t\t\u0005b).\nOn\nthe\ncomplexit\ny\nof\nsolving\nMark\no\nv\ndecision\nproblems.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nA\nnnual\nConfer\nenc\ne\non\nUnc\nertainty\nin\nA\nrti\fcial\nIntel\nligenc\ne\n(UAI{\t\u0005)\nMon\ntreal,\nQu\n\u0013\neb\nec,\nCanada.\nLo\nv\nejo\ny\n,\nW.\nS.\n(\u0001\t\t\u0001).\nA\nsurv\ney\nof\nalgorithmic\nmetho\nds\nfor\npartially\nobserv\nable\nMark\no\nv\ndecision\npro\ncesses.\nA\nnnals\nof\nOp\ner\nations\nR\nese\nar\nch,\n\u0002\b,\n\u0004\u0007{\u0006\u0006.\n\u0002\b0\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nMaes,\nP\n.,\n&\nBro\noks,\nR.\nA.\n(\u0001\t\t0).\nLearning\nto\nco\nordinate\nb\neha\nviors.\nIn\nPr\no\nc\ne\ne\ndings\nEighth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne,\npp.\n\u0007\t\u0006{\b0\u0002.\nMorgan\nKaufmann.\nMahadev\nan,\nS.\n(\u0001\t\t\u0004).\nT\no\ndiscoun\nt\nor\nnot\nto\ndiscoun\nt\nin\nreinforcemen\nt\nlearning:\nA\ncase\nstudy\ncomparing\nR\nlearning\nand\nQ\nlearning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nInter-\nnational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0001\u0006\u0004{\u0001\u0007\u0002\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nMahadev\nan,\nS.\n(\u0001\t\t\u0006).\nAv\nerage\nrew\nard\nreinforcemen\nt\nlearning:\nFoundations,\nalgorithms,\nand\nempirical\nresults.\nMachine\nL\ne\narning,\n\u0002\u0002\n(\u0001).\nMahadev\nan,\nS.,\n&\nConnell,\nJ.\n(\u0001\t\t\u0001a).\nAutomatic\nprogramming\nof\nb\neha\nvior-based\nrob\nots\nusing\nreinforcemen\nt\nlearning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nNinth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne\nAnaheim,\nCA.\nMahadev\nan,\nS.,\n&\nConnell,\nJ.\n(\u0001\t\t\u0001b).\nScaling\nreinforcemen\nt\nlearning\nto\nrob\notics\nb\ny\nex-\nploiting\nthe\nsubsumption\narc\nhitecture.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEighth\nInternational\nWorkshop\non\nMachine\nL\ne\narning,\npp.\n\u0003\u0002\b{\u0003\u0003\u0002.\nMataric,\nM.\nJ.\n(\u0001\t\t\u0004).\nRew\nard\nfunctions\nfor\naccelerated\nlearning.\nIn\nCohen,\nW.\nW.,\n&\nHirsh,\nH.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning.\nMorgan\nKaufmann.\nMcCallum,\nA.\nK.\n(\u0001\t\t\u0005).\nR\neinfor\nc\nement\nL\ne\narning\nwith\nSele\nctive\nPer\nc\neption\nand\nHidden\nState.\nPh.D.\nthesis,\nDepartmen\nt\nof\nComputer\nScience,\nUniv\nersit\ny\nof\nRo\nc\nhester.\nMcCallum,\nR.\nA.\n(\u0001\t\t\u0003).\nOv\nercoming\nincomplete\np\nerception\nwith\nutile\ndistinction\nmemory\n.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nT\nenth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0001\t0{\n\u0001\t\u0006\nAmherst,\nMassac\nh\nusetts.\nMorgan\nKaufmann.\nMcCallum,\nR.\nA.\n(\u0001\t\t\u0005).\nInstance-based\nutile\ndistinctions\nfor\nreinforcemen\nt\nlearning\nwith\nhidden\nstate.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\nMachine\nL\ne\narn-\ning,\npp.\n\u0003\b\u0007{\u0003\t\u0005\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nMeeden,\nL.,\nMcGra\nw,\nG.,\n&\nBlank,\nD.\n(\u0001\t\t\u0003).\nEmergen\nt\ncon\ntrol\nand\nplanning\nin\nan\nau-\ntonomous\nv\nehicle.\nIn\nT\nouretsky\n,\nD.\n(Ed.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nFifte\nenth\nA\nnnual\nMe\neting\nof\nthe\nCo\ngnitive\nScienc\ne\nSo\nciety,\npp.\n\u0007\u0003\u0005{\u0007\u00040.\nLa\nw\nerence\nErlbaum\nAsso\nciates,\nHills-\ndale,\nNJ.\nMillan,\nJ.\nd.\nR.\n(\u0001\t\t\u0006).\nRapid,\nsafe,\nand\nincremen\ntal\nlearning\nof\nna\nvigation\nstrategies.\nIEEE\nT\nr\nansactions\non\nSystems,\nMan,\nand\nCyb\nernetics,\n\u0002\u0006\n(\u0003).\nMonahan,\nG.\nE.\n(\u0001\t\b\u0002).\nA\nsurv\ney\nof\npartially\nobserv\nable\nMark\no\nv\ndecision\npro\ncesses:\nTheory\n,\nmo\ndels,\nand\nalgorithms.\nManagement\nScienc\ne,\n\u0002\b,\n\u0001{\u0001\u0006.\nMo\nore,\nA.\nW.\n(\u0001\t\t\u0001).\nV\nariable\nresolution\ndynamic\nprogramming:\nE\u000ecien\ntly\nlearning\nac-\ntion\nmaps\nin\nm\nultiv\nariate\nreal-v\nalued\nspaces.\nIn\nPr\no\nc.\nEighth\nInternational\nMachine\nL\ne\narning\nWorkshop.\n\u0002\b\u0001\nKaelbling,\nLittman,\n&\nMoore\nMo\nore,\nA.\nW.\n(\u0001\t\t\u0004).\nThe\nparti-game\nalgorithm\nfor\nv\nariable\nresolution\nreinforcemen\nt\nlearn-\ning\nin\nm\nultidimensional\nstate-spaces.\nIn\nCo\nw\nan,\nJ.\nD.,\nT\nesauro,\nG.,\n&\nAlsp\nector,\nJ.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0006,\npp.\n\u0007\u0001\u0001{\u0007\u0001\b\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nMo\nore,\nA.\nW.,\n&\nA\ntk\neson,\nC.\nG.\n(\u0001\t\t\u0002).\nAn\nin\nv\nestigation\nof\nmemory-based\nfunction\nap-\npro\nximators\nfor\nlearning\ncon\ntrol.\nT\nec\nh.\nrep.,\nMIT\nArti\fcal\nIn\ntelligence\nLab\noratory\n,\nCam\nbridge,\nMA.\nMo\nore,\nA.\nW.,\n&\nA\ntk\neson,\nC.\nG.\n(\u0001\t\t\u0003).\nPrioritized\nsw\neeping:\nReinforcemen\nt\nlearning\nwith\nless\ndata\nand\nless\nreal\ntime.\nMachine\nL\ne\narning,\n\u0001\u0003.\nMo\nore,\nA.\nW.,\nA\ntk\neson,\nC.\nG.,\n&\nSc\nhaal,\nS.\n(\u0001\t\t\u0005).\nMemory-based\nlearning\nfor\ncon\ntrol.\nT\nec\nh.\nrep.\nCMU-RI-TR-\t\u0005-\u0001\b,\nCMU\nRob\notics\nInstitute.\nNarendra,\nK.,\n&\nThathac\nhar,\nM.\nA.\nL.\n(\u0001\t\b\t).\nL\ne\narning\nA\nutomata:\nAn\nIntr\no\nduction.\nPren\ntice-Hall,\nEnglew\no\no\nd\nCli\u000bs,\nNJ.\nNarendra,\nK.\nS.,\n&\nThathac\nhar,\nM.\nA.\nL.\n(\u0001\t\u0007\u0004).\nLearning\nautomata|a\nsurv\ney\n.\nIEEE\nT\nr\nansactions\non\nSystems,\nMan,\nand\nCyb\nernetics,\n\u0004\n(\u0004),\n\u0003\u0002\u0003{\u0003\u0003\u0004.\nP\neng,\nJ.,\n&\nWilliams,\nR.\nJ.\n(\u0001\t\t\u0003).\nE\u000ecien\nt\nlearning\nand\nplanning\nwithin\nthe\nDyna\nframe-\nw\nork.\nA\ndaptive\nBehavior,\n\u0001\n(\u0004),\n\u0004\u0003\u0007{\u0004\u0005\u0004.\nP\neng,\nJ.,\n&\nWilliams,\nR.\nJ.\n(\u0001\t\t\u0004).\nIncremen\ntal\nm\nulti-step\nQ-learning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0002\u0002\u0006{\u0002\u0003\u0002\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nP\nomerleau,\nD.\nA.\n(\u0001\t\t\u0003).\nNeur\nal\nnetwork\np\ner\nc\neption\nfor\nmobile\nr\nob\not\nguidanc\ne.\nKlu\nw\ner\nAcademic\nPublishing.\nPuterman,\nM.\nL.\n(\u0001\t\t\u0004).\nMarkov\nDe\ncision\nPr\no\nc\nesses|Discr\nete\nSto\nchastic\nDynamic\nPr\no-\ngr\namming.\nJohn\nWiley\n&\nSons,\nInc.,\nNew\nY\nork,\nNY.\nPuterman,\nM.\nL.,\n&\nShin,\nM.\nC.\n(\u0001\t\u0007\b).\nMo\ndi\fed\np\nolicy\niteration\nalgorithms\nfor\ndiscoun\nted\nMark\no\nv\ndecision\npro\ncesses.\nManagement\nScienc\ne,\n\u0002\u0004,\n\u0001\u0001\u0002\u0007{\u0001\u0001\u0003\u0007.\nRing,\nM.\nB.\n(\u0001\t\t\u0004).\nContinual\nL\ne\narning\nin\nR\neinfor\nc\nement\nEnvir\nonments.\nPh.D.\nthesis,\nUniv\nersit\ny\nof\nT\nexas\nat\nAustin,\nAustin,\nT\nexas.\nR\n\nude,\nU.\n(\u0001\t\t\u0003).\nMathematic\nal\nand\nc\nomputational\nte\nchniques\nfor\nmultilevel\nadaptive\nmeth-\no\nds.\nSo\nciet\ny\nfor\nIndustrial\nand\nApplied\nMathematics,\nPhiladelphi\na,\nP\nennsylv\nania.\nRumelhart,\nD.\nE.,\n&\nMcClelland,\nJ.\nL.\n(Eds.).\n(\u0001\t\b\u0006).\nPar\nal\nlel\nDistribute\nd\nPr\no\nc\nessing:\nExplor\nations\nin\nthe\nmicr\nostructur\nes\nof\nc\no\ngnition.\nV\nolume\n\u0001:\nFoundations.\nThe\nMIT\nPress,\nCam\nbridge,\nMA.\nRummery\n,\nG.\nA.,\n&\nNiranjan,\nM.\n(\u0001\t\t\u0004).\nOn-line\nQ-learning\nusing\nconnectionist\nsystems.\nT\nec\nh.\nrep.\nCUED/F-INFENG/TR\u0001\u0006\u0006,\nCam\nbridge\nUniv\nersit\ny.\n\u0002\b\u0002\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nRust,\nJ.\n(\u0001\t\t\u0006).\nNumerical\ndynamic\nprogramming\nin\neconomics.\nIn\nHandb\no\nok\nof\nComputa-\ntional\nEc\nonomics.\nElsevier,\nNorth\nHolland.\nSage,\nA.\nP\n.,\n&\nWhite,\nC.\nC.\n(\u0001\t\u0007\u0007).\nOptimum\nSystems\nContr\nol.\nPren\ntice\nHall.\nSalganico\u000b,\nM.,\n&\nUngar,\nL.\nH.\n(\u0001\t\t\u0005).\nActiv\ne\nexploration\nand\nlearning\nin\nreal-v\nalued\nspaces\nusing\nm\nulti-armed\nbandit\nallo\ncation\nindices.\nIn\nPrieditis,\nA.,\n&\nRussell,\nS.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\nTwelfth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0004\b0{\u0004\b\u0007\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nSam\nuel,\nA.\nL.\n(\u0001\t\u0005\t).\nSome\nstudies\nin\nmac\nhine\nlearning\nusing\nthe\ngame\nof\nc\nhec\nk\ners.\nIBM\nJournal\nof\nR\nese\nar\nch\nand\nDevelopment,\n\u0003,\n\u0002\u0001\u0001{\u0002\u0002\t.\nReprin\nted\nin\nE.\nA.\nF\neigen\nbaum\nand\nJ.\nF\neldman,\neditors,\nComputers\nand\nThought,\nMcGra\nw-Hill,\nNew\nY\nork\n\u0001\t\u0006\u0003.\nSc\nhaal,\nS.,\n&\nA\ntk\neson,\nC.\n(\u0001\t\t\u0004).\nRob\not\njuggling:\nAn\nimplemen\ntation\nof\nmemory-based\nlearning.\nContr\nol\nSystems\nMagazine,\n\u0001\u0004.\nSc\nhmidh\nub\ner,\nJ.\n(\u0001\t\t\u0006).\nA\ngeneral\nmetho\nd\nfor\nm\nulti-agen\nt\nlearning\nand\nincremen\ntal\nself-\nimpro\nv\nemen\nt\nin\nunrestricted\nen\nvironmen\nts.\nIn\nY\nao,\nX.\n(Ed.),\nEvolutionary\nComputa-\ntion:\nThe\nory\nand\nApplic\nations.\nScien\nti\fc\nPubl.\nCo.,\nSingap\nore.\nSc\nhmidh\nub\ner,\nJ.\nH.\n(\u0001\t\t\u0001a).\nCurious\nmo\ndel-buildi\nng\ncon\ntrol\nsystems.\nIn\nPr\no\nc.\nInternational\nJoint\nConfer\nenc\ne\non\nNeur\nal\nNetworks,\nSingap\nor\ne,\nV\nol.\n\u0002,\npp.\n\u0001\u0004\u0005\b{\u0001\u0004\u0006\u0003.\nIEEE.\nSc\nhmidh\nub\ner,\nJ.\nH.\n(\u0001\t\t\u0001b).\nReinforcemen\nt\nlearning\nin\nMark\no\nvian\nand\nnon-Mark\no\nvian\nen\nvironmen\nts.\nIn\nLippman,\nD.\nS.,\nMo\no\ndy\n,\nJ.\nE.,\n&\nT\nouretzky\n,\nD.\nS.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0003,\npp.\n\u000500{\u00050\u0006\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nSc\nhraudolph,\nN.\nN.,\nDa\ny\nan,\nP\n.,\n&\nSejno\nwski,\nT.\nJ.\n(\u0001\t\t\u0004).\nT\nemp\noral\ndi\u000berence\nlearning\nof\np\nosition\nev\naluation\nin\nthe\ngame\nof\nGo.\nIn\nCo\nw\nan,\nJ.\nD.,\nT\nesauro,\nG.,\n&\nAlsp\nector,\nJ.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0006,\npp.\n\b\u0001\u0007{\b\u0002\u0004\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nSc\nhrijv\ner,\nA.\n(\u0001\t\b\u0006).\nThe\nory\nof\nLine\nar\nand\nInte\nger\nPr\no\ngr\namming.\nWiley-In\nterscience,\nNew\nY\nork,\nNY.\nSc\nh\nw\nartz,\nA.\n(\u0001\t\t\u0003).\nA\nreinforcemen\nt\nlearning\nmetho\nd\nfor\nmaximizing\nundiscoun\nted\nre-\nw\nards.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nT\nenth\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning,\npp.\n\u0002\t\b{\u00030\u0005\nAmherst,\nMassac\nh\nusetts.\nMorgan\nKaufmann.\nSingh,\nS.\nP\n.,\nBarto,\nA.\nG.,\nGrup\nen,\nR.,\n&\nConnolly\n,\nC.\n(\u0001\t\t\u0004).\nRobust\nreinforcemen\nt\nlearning\nin\nmotion\nplanning.\nIn\nCo\nw\nan,\nJ.\nD.,\nT\nesauro,\nG.,\n&\nAlsp\nector,\nJ.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0006,\npp.\n\u0006\u0005\u0005{\u0006\u0006\u0002\nSan\nMateo,\nCA.\nMorgan\nKaufmann.\nSingh,\nS.\nP\n.,\n&\nSutton,\nR.\nS.\n(\u0001\t\t\u0006).\nReinforcemen\nt\nlearning\nwith\nreplacing\neligibili\nt\ny\ntraces.\nMachine\nL\ne\narning,\n\u0002\u0002\n(\u0001).\n\u0002\b\u0003\nKaelbling,\nLittman,\n&\nMoore\nSingh,\nS.\nP\n.\n(\u0001\t\t\u0002a).\nReinforcemen\nt\nlearning\nwith\na\nhierarc\nh\ny\nof\nabstract\nmo\ndels.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nT\nenth\nNational\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel\nligenc\ne,\npp.\n\u00020\u0002{\u00020\u0007\nSan\nJose,\nCA.\nAAAI\nPress.\nSingh,\nS.\nP\n.\n(\u0001\t\t\u0002b).\nT\nransfer\nof\nlearning\nb\ny\ncomp\nosing\nsolutions\nof\nelemen\ntal\nsequen\ntial\ntasks.\nMachine\nL\ne\narning,\n\b\n(\u0003),\n\u0003\u0002\u0003{\u0003\u00040.\nSingh,\nS.\nP\n.\n(\u0001\t\t\u0003).\nL\ne\narning\nto\nSolve\nMarkovian\nDe\ncision\nPr\no\nc\nesses.\nPh.D.\nthesis,\nDepart-\nmen\nt\nof\nComputer\nScience,\nUniv\nersit\ny\nof\nMassac\nh\nusetts.\nAlso,\nCMPSCI\nT\nec\nhnical\nRep\nort\n\t\u0003-\u0007\u0007.\nStengel,\nR.\nF.\n(\u0001\t\b\u0006).\nSto\nchastic\nOptimal\nContr\nol.\nJohn\nWiley\nand\nSons.\nSutton,\nR.\nS.\n(\u0001\t\t\u0006).\nGeneralization\nin\nReinforcemen\nt\nLearning:\nSuccessful\nExamples\nUsing\nSparse\nCoarse\nCo\nding.\nIn\nT\nouretzky\n,\nD.,\nMozer,\nM.,\n&\nHasselmo,\nM.\n(Eds.),\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\b.\nSutton,\nR.\nS.\n(\u0001\t\b\u0004).\nT\nemp\nor\nal\nCr\ne\ndit\nAssignment\nin\nR\neinfor\nc\nement\nL\ne\narning.\nPh.D.\nthesis,\nUniv\nersit\ny\nof\nMassac\nh\nusetts,\nAmherst,\nMA.\nSutton,\nR.\nS.\n(\u0001\t\b\b).\nLearning\nto\npredict\nb\ny\nthe\nmetho\nd\nof\ntemp\noral\ndi\u000berences.\nMachine\nL\ne\narning,\n\u0003\n(\u0001),\n\t{\u0004\u0004.\nSutton,\nR.\nS.\n(\u0001\t\t0).\nIn\ntegrated\narc\nhitectures\nfor\nlearning,\nplanning,\nand\nreacting\nbased\non\nappro\nximating\ndynamic\nprogramming.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nSeventh\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning\nAustin,\nTX.\nMorgan\nKaufmann.\nSutton,\nR.\nS.\n(\u0001\t\t\u0001).\nPlanning\nb\ny\nincremen\ntal\ndynamic\nprogramming.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEighth\nInternational\nWorkshop\non\nMachine\nL\ne\narning,\npp.\n\u0003\u0005\u0003{\u0003\u0005\u0007.\nMorgan\nKaufmann.\nT\nesauro,\nG.\n(\u0001\t\t\u0002).\nPractical\nissues\nin\ntemp\noral\ndi\u000berence\nlearning.\nMachine\nL\ne\narning,\n\b,\n\u0002\u0005\u0007{\u0002\u0007\u0007.\nT\nesauro,\nG.\n(\u0001\t\t\u0004).\nTD-Gammon,\na\nself-teac\nhing\nbac\nkgammon\nprogram,\nac\nhiev\nes\nmaster-\nlev\nel\npla\ny\n.\nNeur\nal\nComputation,\n\u0006\n(\u0002),\n\u0002\u0001\u0005{\u0002\u0001\t.\nT\nesauro,\nG.\n(\u0001\t\t\u0005).\nT\nemp\noral\ndi\u000berence\nlearning\nand\nTD-Gammon.\nCommunic\nations\nof\nthe\nA\nCM,\n\u0003\b\n(\u0003),\n\u0005\b{\u0006\u0007.\nTham,\nC.-K.,\n&\nPrager,\nR.\nW.\n(\u0001\t\t\u0004).\nA\nmo\ndular\nq-learning\narc\nhitecture\nfor\nmanipula-\ntor\ntask\ndecomp\nosition.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEleventh\nInternational\nConfer\nenc\ne\non\nMachine\nL\ne\narning\nSan\nF\nrancisco,\nCA.\nMorgan\nKaufmann.\nThrun,\nS.\n(\u0001\t\t\u0005).\nLearning\nto\npla\ny\nthe\ngame\nof\nc\nhess.\nIn\nT\nesauro,\nG.,\nT\nouretzky\n,\nD.\nS.,\n&\nLeen,\nT.\nK.\n(Eds.),\nA\ndvanc\nes\nin\nNeur\nal\nInformation\nPr\no\nc\nessing\nSystems\n\u0007\nCam\nbridge,\nMA.\nThe\nMIT\nPress.\n\u0002\b\u0004\nReinf\nor\ncement\nLearning:\nA\nSur\nvey\nThrun,\nS.,\n&\nSc\nh\nw\nartz,\nA.\n(\u0001\t\t\u0003).\nIssues\nin\nusing\nfunction\nappro\nximation\nfor\nreinforcemen\nt\nlearning.\nIn\nMozer,\nM.,\nSmolensky\n,\nP\n.,\nT\nouretzky\n,\nD.,\nElman,\nJ.,\n&\nW\neigend,\nA.\n(Eds.),\nPr\no\nc\ne\ne\ndings\nof\nthe\n\u0001\t\t\u0003\nConne\nctionist\nMo\ndels\nSummer\nScho\nol\nHillsdale,\nNJ.\nLa\nwrence\nErlbaum.\nThrun,\nS.\nB.\n(\u0001\t\t\u0002).\nThe\nrole\nof\nexploration\nin\nlearning\ncon\ntrol.\nIn\nWhite,\nD.\nA.,\n&\nSofge,\nD.\nA.\n(Eds.),\nHandb\no\nok\nof\nIntel\nligent\nContr\nol:\nNeur\nal,\nF\nuzzy,\nand\nA\ndaptive\nAppr\no\naches.\nV\nan\nNostrand\nReinhold,\nNew\nY\nork,\nNY.\nTsitsiklis,\nJ.\nN.\n(\u0001\t\t\u0004).\nAsync\nhronous\nsto\nc\nhastic\nappro\nximation\nand\nQ-learning.\nMachine\nL\ne\narning,\n\u0001\u0006\n(\u0003).\nTsitsiklis,\nJ.\nN.,\n&\nV\nan\nRo\ny\n,\nB.\n(\u0001\t\t\u0006).\nF\neature-based\nmetho\nds\nfor\nlarge\nscale\ndynamic\nprogramming.\nMachine\nL\ne\narning,\n\u0002\u0002\n(\u0001).\nV\nalian\nt,\nL.\nG.\n(\u0001\t\b\u0004).\nA\ntheory\nof\nthe\nlearnable.\nCommunic\nations\nof\nthe\nA\nCM,\n\u0002\u0007\n(\u0001\u0001),\n\u0001\u0001\u0003\u0004{\u0001\u0001\u0004\u0002.\nW\natkins,\nC.\nJ.\nC.\nH.\n(\u0001\t\b\t).\nL\ne\narning\nfr\nom\nDelaye\nd\nR\newar\nds.\nPh.D.\nthesis,\nKing's\nCollege,\nCam\nbridge,\nUK.\nW\natkins,\nC.\nJ.\nC.\nH.,\n&\nDa\ny\nan,\nP\n.\n(\u0001\t\t\u0002).\nQ-learning.\nMachine\nL\ne\narning,\n\b\n(\u0003),\n\u0002\u0007\t{\u0002\t\u0002.\nWhitehead,\nS.\nD.\n(\u0001\t\t\u0001).\nComplexit\ny\nand\nco\nop\neration\nin\nQ-learning.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nEighth\nInternational\nWorkshop\non\nMachine\nL\ne\narning\nEv\nanston,\nIL.\nMorgan\nKauf-\nmann.\nWilliams,\nR.\nJ.\n(\u0001\t\b\u0007).\nA\nclass\nof\ngradien\nt-estimating\nalgorithms\nfor\nreinforcemen\nt\nlearning\nin\nneural\nnet\nw\norks.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nIEEE\nFirst\nInternational\nConfer\nenc\ne\non\nNeur\nal\nNetworks\nSan\nDiego,\nCA.\nWilliams,\nR.\nJ.\n(\u0001\t\t\u0002).\nSimple\nstatistical\ngradien\nt-follo\nwing\nalgorithms\nfor\nconnectionist\nreinforcemen\nt\nlearning.\nMachine\nL\ne\narning,\n\b\n(\u0003),\n\u0002\u0002\t{\u0002\u0005\u0006.\nWilliams,\nR.\nJ.,\n&\nBaird,\nI\nI\nI,\nL.\nC.\n(\u0001\t\t\u0003a).\nAnalysis\nof\nsome\nincremen\ntal\nv\narian\nts\nof\np\nolicy\niteration:\nFirst\nsteps\nto\nw\nard\nunderstanding\nactor-critic\nlearning\nsystems.\nT\nec\nh.\nrep.\nNU-CCS-\t\u0003-\u0001\u0001,\nNortheastern\nUniv\nersit\ny\n,\nCollege\nof\nComputer\nScience,\nBoston,\nMA.\nWilliams,\nR.\nJ.,\n&\nBaird,\nI\nI\nI,\nL.\nC.\n(\u0001\t\t\u0003b).\nTigh\nt\np\nerformance\nb\nounds\non\ngreedy\np\nolicies\nbased\non\nimp\nerfect\nv\nalue\nfunctions.\nT\nec\nh.\nrep.\nNU-CCS-\t\u0003-\u0001\u0004,\nNortheastern\nUniv\ner-\nsit\ny\n,\nCollege\nof\nComputer\nScience,\nBoston,\nMA.\nWilson,\nS.\n(\u0001\t\t\u0005).\nClassi\fer\n\ftness\nbased\non\naccuracy\n.\nEvolutionary\nComputation,\n\u0003\n(\u0002),\n\u0001\u0004\u0007{\u0001\u0007\u0003.\nZhang,\nW.,\n&\nDietteric\nh,\nT.\nG.\n(\u0001\t\t\u0005).\nA\nreinforcemen\nt\nlearning\napproac\nh\nto\njob-shop\nsc\nheduling.\nIn\nPr\no\nc\ne\ne\ndings\nof\nthe\nInternational\nJoint\nConfer\nenc\ne\non\nA\nrti\fcial\nIntel-\nlienc\ne.\n\u0002\b\u0005\n",
  "categories": [
    "cs.AI"
  ],
  "published": "1996-05-01",
  "updated": "1996-05-01"
}