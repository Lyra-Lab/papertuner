{
  "id": "http://arxiv.org/abs/cmp-lg/9607017v1",
  "title": "Natural Language Processing: Structure and Complexity",
  "authors": [
    "Wlodek Zadrozny"
  ],
  "abstract": "We introduce a method for analyzing the complexity of natural language\nprocessing tasks, and for predicting the difficulty new NLP tasks.\n  Our complexity measures are derived from the Kolmogorov complexity of a class\nof automata --- {\\it meaning automata}, whose purpose is to extract relevant\npieces of information from sentences. Natural language semantics is defined\nonly relative to the set of questions an automaton can answer.\n  The paper shows examples of complexity estimates for various NLP programs and\ntasks, and some recipes for complexity management. It positions natural\nlanguage processing as a subdomain of software engineering, and lays down its\nformal foundation.",
  "text": "arXiv:cmp-lg/9607017v1  13 Jul 1996\nNatural Language Processing: Structure and Complexity\nWlodek Zadrozny\nIBM Research, T. J. Watson Research Center\nYorktown Heights, NY 10598\n∗wlodz@watson.ibm.com\nAbstract\nWe introduce a method for analyzing the com-\nplexity of natural language processing tasks,\nand for predicting the diﬃculty new NLP tasks.\nOur complexity measures are derived from the\nKolmogorov complexity of a class of automata\n— meaning automata, whose purpose is to ex-\ntract relevant pieces of information from sen-\ntences. Natural language semantics is deﬁned\nonly relative to the set of questions an automa-\nton can answer.\nThe paper shows examples of complexity esti-\nmates for various NLP programs and tasks, and\nsome recipes for complexity management.\nIt\npositions natural language processing as a sub-\ndomain of software engineering, and lays down\nits formal foundation.\n1\nIntroduction\nThis paper proposes a solution to the problem of mea-\nsuring and managing the complexity of natural language\nprocessing (NLP) systems.\nIdeally, before building a NLP application, such as\na phone dialog system, a translation system, or a text\nskimmer, we would like to know how diﬃcult the task\nis. This complexity measure should be expressed in a\nnumber, which, in turn, could be translated into an es-\ntimated program size (or other parameters), and even-\ntually into dollars.\nIf the task is too diﬃcult, such a\nmeasure would allow us to limit the task to subtasks\nof manageable complexity; for instance, if a full natural\nlanguage help system for Unix is not feasible, perhaps\nit is possible to provide it for the twenty most common\ncommands.\nHowever, NLP domains and tasks are very seldom an-\nalyzed before building a program, and typically very few\n∗In: ”Proc. SEKE’96, 8th Int. Conf. on Software En-\ngineering and Knowledge Engineering”, Lake Tahoe, 1996,\npages 595-602.\nnumbers are provided that would measure their com-\nplexity. For instance, we can get information about the\nvocabulary size and the number of grammar rules of a\nparser or a tagger, and subsequently some numbers mea-\nsuring its performance. But for NLP to become an en-\ngineering discipline, we have to be able provide numbers\ndescribing in advance the complexity of a task, e.g. of\nparsing computer manuals.\nFor speech recognition some of those numbers have\nbeen computed, e.g. the average perplexity of a corpus\n(roughly, the number of possible continuations of the\nstrings of words). But we have not yet seen an analysis\nthat would, for instance, compute the number of dif-\nferent sentences or dialogs expressing the command to\nschedule a meeting with Bob at 5 in his oﬃce.\nSuch an analysis requires a model of the phenomenon\nand data.\nFor instance, to measure the perplexity of\na corpus, we model the corpus as a set of sequences\nof words, and then the counting produces the required\nnumbers, cf.\n[19].\nObviously, we could collect a cor-\npus of data about meetings, analyze it, and come up\nwith the number. However, the data collection process is\ncostly, and thus cannot cover all relevant domains. Fur-\nthermore, instead of counting possible dialogs, it would\nclearly be easier to count language constructions that\nappear in dialogs, and obtain the number of dialogs by\ncombinatorics. This requires to model the language as\na set of constructions, and thus to abandon the simple\ncounting model.\nThe natural question arises whether we can do bet-\nter, and propose a model that provides a measure of the\ncomplexity of the task, but dispenses with, or limits, the\ndata collection eﬀort. We will propose such a model. It\nallows us to introduce several measures of the complexity\nof NLP tasks; it has a theoretical foundation in a modi-\nﬁcation of the theory of Kolmogorov complexity; and it\nsuggests ways of managing the complexity of NLP pro-\ngrams by more careful speciﬁcation of their objectives.\nA digression on vision\nInstead of proceeding directly to the description of our\nmodel, we will review some complexity estimates of prob-\nlems in vision. Our aim is to set up the stage for the\ndiscussion of complexity issues in NL.\nMoravec [11, 12] analyses the complexities of diﬀer-\nent tasks, and compares them with the processing ca-\npacity of computers, the human retina and the human\nbrain. Thus tracking a white line in perfect lighting con-\nditions requires about 1 MIP (million instructions per\nsecond); smart bombs and simple road navigation take\nabout 10 MIPS; chess playing at grandmaster level —\n10,000 MIPS. He estimates the computational capabil-\nity of the human retina at 1000 MIPS (which also is the\npower of an IBM RS6000 workstation in 1995); of the\nmonkey visual system at 100,000 MIPS; of the human\nvisual system at 1,000,000 MIPS; and ﬁnally of the hu-\nman brain at 10 million MIPS.\nThese numbers are interesting, but it is more impor-\ntant that this analysis is based on a speciﬁc model In\nthis model, the processing powers are measured in pix-\nels/frame/second.\nTo give an analogous complexity analysis of NLP, we\nneed a set of units in which we can measure the elements\nof NLP tasks.\n2\nSemantic complexity\nIn this section we introduce the mathematical apparatus\nto measure the complexity of NLP tasks.\nIn order to compare NLP systems, esp. if we want\nto compare systems performing diﬀerent tasks we need\na set of tools.\nOne of the tools for measuring com-\nplexity widely used in theoretical computer science is\nKolmogorov complexity.\nHowever, the concept of K.-\ncomplexity must be modiﬁed to work for our task. This\nwill be done in this and the next section.\nKolmogorov complexity of a string x is deﬁned as the\nsize of the shortest string y from which a certain univer-\nsal Turing machine produces x. Intuitively, y measures\nthe amount of information necessary to describe x, i.e.\nthe information content of x. (cf. [9] for details and a\nvery good survey of Kolmogorov complexity and related\nconcepts). For our purposes any of the related deﬁnitions\nof complexity will work. For example, the Minimum De-\nscription Length of Rissanen ([9] and [15]), or the size\nof a grammar (as in [17]), or the number of states of an\nautomaton.\nObviously, K.-complexity does not by itself tell us any-\nthing about natural language and semantics, so we will\nmodify it, so that we could deﬁne and measure the se-\nmantic complexity of natural language. To do that, we\nassume that the meaning of a sentence is encoded in the\nquestions it can answer (examples below), a view also\nheld by Bloomﬁeld [2].\nSo, let Q be a set of questions.\nWe will deﬁne the\nQ-complexity of a set of sentences S as the size of the\nsimplest model scheme M= MS, such that, for any sen-\ntence s in S, its semantics is given by M(s), and M(s)\ncorrectly answers all questions about s contained in Q.\nThe words ”model scheme” can stand for either ”Tur-\ning machine”, or ”Prolog program”, or ”description”,\nor a related notion. In this paper we think of M as a\nTuring machine that computes the semantics of the sen-\ntences in S, and measure its size by the product (Num-\nber of States) × (Number of Symbols).\nThus deﬁned,\nState × Symbol-complexity is a good approximation of\nK-complexity ([9]).\nTo deﬁne Q-complexity, we ﬁrst deﬁne the concept of\nmeaning automaton (M-automaton): Let, as above, Q be\na set of questions, and S set of sentences (e.g. accepted\nby a grammar). Formally, we treat each question as a\n(partial) function from sentences to a set of answers A:\nq : S →A\nIntuitively, each question examines a sentence for a piece\nof relevant information. Under this assumption the se-\nmantics of a sentence (i.e. a formal string) is not given\nby its truth conditions or denotation but by a set of an-\nswers:\n∥s∥= {< q, q(s) >: q ∈Q}\nNow, given a set of sentences S and a set of questions\nQ, their meaning automaton is a function\nM : S × Q →A\nwhich satisﬁes the constraint\nM(s, q) = q(s)\ni.e.\na function which gives a correct answer to every\nquestion. We call it a meaning automaton because for\nany sentence s\n∥s∥= {< q, M(s, q) >: q ∈Q}\nAs before, the Q-complexity of the set S is the size of the\nsmallest such M.\nNote that the idea of a meaning automaton as a ques-\ntion answer map allows us to bypass all subtle semantics\nquestions without doing violence to them.\nNote also that in practice we will deal not with the sim-\nplest model scheme, but with the simplest we are able to\nconstruct. Furthermore, to take care of the possible non-\ncomputability of the function computing Q-complexity\nof a set of sentences, we can put some restriction on the\nTuring machine, e.g. requiring it to be ﬁnite state or\na stack automaton. Finally, we will use the size of the\nTuring machine description — T-rule–complexity — as\nanother, related, measure (see Sections 4 and 5).\n3\nSome Q-complexity classes\nMeaning automata (M-automata) provide a foundation\nfor an analysis of a few NLP problems and systems we are\ngoing to present in this section. As a result of this anal-\nysis, in the next sections, we should be able to present a\nmethod of estimating the semantic complexities of NLP\nsystems, and to suggest ways of managing them.\nWe can measure the semantic complexity of a set of\nsentences by the size of the smallest model that answers\nall relevant questions about those sentences (in practice,\nof the simplest one we are able to construct). But what\nare the relevant questions? Let us ﬁrst look at the types\nof questions. A simple classiﬁcation of questions given\nby [16] (pp.191-2) is based on the type of answer they\nexpect: (1) those that expect aﬃrmation or rejection\n— yes-no questions; (2) those that expect a reply sup-\nplying an item of information — Wh questions; and (3)\nthose that expect as the reply one of two or more options\npresented in the question — alternative questions.\nWe will now examine a few simple Q-machines, and\nthen discuss the Q-complexities of a couple of programs.\n”what”-complexity\nLet U be a ﬁnite set of tokens. Consider the following\nsemantic machine MU: For any token u in U, if the input\nis ”what is u” the output is a deﬁnition defu of u, and the\nnext state is acc (accept). We assume that the output is\njust one token, defu, and the input also consists only of\none token, namely u, and the question is implicit. Then,\nthe size of MU is the measure of ”what”-complexity of\nU. MU can be described as a one state, two tape, Turing\nmachine consisting of the following set rules\n(1, u, defu, acc) for u ∈U\nBoth the T-rule– and the State × Symbol– complexity\nof MU are a simple function of the size of U.\nThis machine must be familiar to everyone; most\nkeyword-based help systems have this structure.\n”yes/no”-complexity\nIn [28] we present a machine for answering yes/no\nquestions in two simple domains, each containing 24 sen-\ntences. The machine simply compares the question with\na stored on another tape answer; it performs no analy-\nsis of the question except for extracting a piece of data,\nand no reasoning. That machine had the State × Symbol\ncomplexity of 6 = 2 × 3.\nObviously the size of the machine would grow if the\nnumber of questions increases, or if some analysis is re-\nquired.\nQ-Complexity of eliza\nWe can compute the semantic complexity of eliza [23]\nas Q-complexity. Namely, we can identify Q with the set\nof key-words for which eliza has rules for transforming\ninput sentences (including a rule for what to do if an\ninput sentence contains no key-word).\neliza had not\nmore than 2 key list structures for each of the about 50\nkeywords, and its control mechanism had 18 states.\nIf we assume that states and keywords corresponds\nto rules of a Turing machine, we can estimate eliza’s\ncomplexity at 118 rules.\nSince the machine could be\nrepresented as a 4-column table, its T-rule complexity\nwould be roughly 4 × 118 = 472. (Its State × Symbol\ncomplexity can be estimated in a similar way).\nHowever, eliza maintained the distinction between\nprogram and data. Thus we can distinguish between the\nrelative complexities of eliza database for the domain\nX, given eliza program (here about 4×100 for T-rules),\nand the complexity of the program itself (about 4 × 18).\nThe moral is that all discussions of complexity must as-\nsume some level of abstraction (see next section). (No-\ntice the parallels with the Lindenbaum algebras in logic).\nComplexity of extraction\nThe importance of discussing semantic complexity at\nsome level of abstraction is even better seen for more\ncomplex programs. We can turn our attention to the\narea of information extraction (e.g. [13]). For instance,\nHuﬀman, in a recent paper [7], uses about 60 extraction\npatterns generated from 150 examples to create an NLP\nsystem attuned to corporate management changes. Thus\nthe complexity of his system measured by the number of\npatterns is 60.\nThis is the most relevant complexity measure for the\nsystem. But at a diﬀerent level of abstraction, its com-\nplexity would be larger, since it would have to include the\ncomplexity(-ies) of the grammar that is needed to break\nthe text into noun and verb groups; or even the complex-\nity of determining the sentence boundaries.\nAnd the\nbinary representation of the whole extraction machine\nwould increase the complexity by orders of magnitude.\nHowever, those lower level complexities are irrelevant,\nbecause they do not directly reﬂect the semantic of the\ntask — they have little to do with the set Q.\nNote.\nIn addition to talking about Q-complexities\nat diﬀerent abstraction levels, we can also say that the\nnumber 60 reﬂects the complexity of the task at the level\nof 90% precision (as achieved by the program).\nThis\nis natural because of the possible translation between\nMinimum Description Length and K-complexity ([9]).\n4\nQ-complexities of new tasks\nIn our previous discussions we described the M-automata\nat a certain natural level of abstraction. For instance,\nwe have assumed that the output of the automaton an-\nswering a ”what” question consists of only one token; or\nthat the grammar rules used in the speciﬁcation of the\nHuﬀman’s extraction machine can be taken for granted.\nSince the intuitions about what constitutes such a nat-\nural level of abstraction are often shared by researchers,\nM-automata can be used to estimate the semantic com-\nplexity of existing programs. Our next step is to explain\nwhat might constitute such a ”natural level of abstrac-\ntion”, and to estimate its complexity as a function of\nQ, and some properties of the task, e.g. the size of the\ndomain. This will allow us to estimate the complexity\nof NLP programs in new domains.\nNatural levels of abstraction\nThe Kolmogorov complexity of a task, i.e. the size of\nthe smallest Turing machine that achieves some required\ninput-output behavior is not computable. This leaves us\nwith approximations, i.e. machines we are able to con-\nstruct. In building such machines, we choose an archi-\ntecture. Even though many architectural styles are pos-\nsible, in our case, with the experience of building NLP\nsystems as knowledge based programs, it is natural to as-\nsume that they can be decomposed into the main control\nprogram and the knowledge base. Thus the complexity\nof the whole system is a function of the complexities of\nthe two parts, the program and the knowledge base.\n4.1\nThe complexity of the knowledge base\nWe view the knowledge base as a set of facts about the\ncategories of the domain. In a limited domain, we ﬁnd\na relatively small number of semantic categories.\nFor\nexample, for the domain of calendars the number of con-\ncepts is less than 100; for room scheduling it is about\n20. Even for a relatively complex oﬃce application, say,\nLotus Notes, the number of semantic categories is be-\ntween 200 and 500 (the number depends what counts as\na category, and what is merely a feature).\nThose background knowledge facts describe properties\nthat are relevant for the task. E.g. if the task is schedul-\ning meetings, and one of the concepts is ”range”, spec-\nifying the ”beginning”, ”end”, and ”duration”, we also\nhave to know that the value of ”beginning” is smaller\nthan the value of ”end”. We should also know that two\ndiﬀerent meetings cannot occupy the same room in over-\nlapping periods of time, and the number of days in any\nmonth, and that meetings are typically scheduled after\nthe current date, etc., etc.\nThe complexity of such a knowledge base is described\nby its size. So, the natural question that arises is how\nmany such facts we need in the knowledge base.\nIs\nit billions? The next question is whether they can be\nstated in a language that can be computationally ma-\nnipulated. After all, in principle, any eﬃcient represen-\ntation of space could require sensors and eﬀectors.\nFortunately, background knowledge is bounded, and\nfor reasoning about small domains propositional repre-\nsentations work ﬁne; there is evidence ([6], [4], [27]) that\nthe ratio of the number of words to the number of facts\nnecessary to understand sentences with them is about\n10:1. Second, experience suggests that this bound works\nalso for concepts in general, that is, it is not restricted\nto linguistic knowledge.\nThis 10:1 bound makes the enterprise of building nat-\nural language understanding systems for small domains\nfeasible. The background knowledge about them can be\norganized, coded and tested (cf. e.g. [26]).\n4.2\nComplexity of the control programs\nDialog systems are a good example of the NLP archi-\ntecture in which the main task is decomposed into the\ncontrol program and the knowledge base. The main al-\ngorithm of such programs is quite simple: at each step\nthe following routine is called\nParse one sentence:\n1. Read sentence/string.\n2. Parse sentence using\n(a) grammar\n(b) background knowledge\n(c) contextual parameters\n3. Compute attributes important for application\n4. Update current context\n5. Reply\nThe parser may consult with background knowledge; the\nsemantic interpreter (point 3) uses the knowledge of the\napplication and current context (e.g. what the question\nwas about) to interpret the string (e.g. an answer that is\na fragment with otherwise multiple interpretations). In\npoint 4, contextual variables are updated (e.g. that the\ncontext does not contain a question, or that the current\naction is moving a meeting); and in 2c, contextual pa-\nrameters might include the state of the application, e.g.\nto choose one of many possible parses.\nWe refer to the choice of replies based on the computed\nattributes of the input as dialog management. Clearly, a\ndialog manager is the main control program of a dialog\nsystem. Thus the question of the complexity of control\nprogram is the question about the complexity of the di-\nalog manager. Some of the crucial things in the working\nof a dialog manager include:\n(a) take an order, and ﬁgure it out (”set up an appoint-\nment”);\n(b) deal with parameters of the order (”in the after-\nnoon”);\n(c) ask for parameters (”is it important?”);\n(d) deal with a change in the parameters (”make it 6”);\n(e) deal with a change in the order (”no, show my\nemail”);\n(f) answer simple questions (”do I have any meetings to-\nday?”);\n(g) recover from speech recognition errors (”at what\ntime, two or eight?”).\nThis list could be extended. Also, it is clear, that none\nof the points (a)-(g) are among the properties of eliza.\nThe question now is how complex a dialog machine we\nneed for a useful behavior. How many constraints should\nit satisfy? How many states are required?\nFortunately, the complexity of task oriented dia-\nlog machines is low.\nWinograd and Flores ([25],\np.64ﬀ)\nargue\nthat\nthe\nbasic\nconversation for\nac-\ntion\nmachine\nneeds\nonly\n9\nstates,\nand\n8\nba-\nsic actions (request, promise, counter, declare,\nassert, reject, withdraw, renege)\nthat\ndescribe\ntransitions between those states. This number is not uni-\nversally agreed upon, but other estimates are low too.\nFor instance, Bunt [3] in his classiﬁcation lists 18 ba-\nsic dialog control functions and dialog acts. One can of\ncourse argue about the adequacy of either model, but the\nfact remains that, for simple tasks, dialog complexity is\nlimited by a small number of basic states.\n4.3\nComplexities of NLP systems in small\ndomains\nThe above remarks about the simplicity of the control\nprograms apply to many other NLP systems, e.g. those\nfor information extraction. Our experience suggests that\nthese estimates should work in small domains. But what\nis a small domain?\nA small domain should have less than 300 concepts,\nbecause, given the 10:1 ratio of axioms to concepts, and\n3,000 axioms as the upper limit of the expert system\ntechnology, we cannot expect to be able to handle more\nknowledge about the domain. (Of course, we can imag-\nine decomposing a larger domain into weakly interact-\ning small domains). Notice that this postulate does not\nmean that we have to restrict ourselves e.g. to less that\n300 words in NLP applications — such applications usu-\nally deal with classes of words, i.e. concepts. For in-\nstance in syntactic tagging, the number of categories is\nusually less than 50 while the vocabulary exceeds 100,000\ntokens; eliza has no limitation on the vocabulary, but\nits list of concepts is limited to a few dozens.\nGiven the above constraints, in a small domain, there\nshould be no need to go beyond the 300 or so concepts.\nWe can express this postulate in terms of Q-complexity.\n1\n5\nEstimating the complexity of NLP\ntasks\nIn this section we discuss the semantic complexities of\nbackground knowledge, grammars and control programs\nfor NLP tasks. The second problem seems to be to large\nextent open, but, as we point out, there is a hope for a\nsolution based on Zipf’s laws.\n5.1\nAn analysis of a dialog system\nWe have started our work in dialog systems with min-\ncal, a meeting scheduler, which we discuss brieﬂy in\nthe next section. Our latest system is a simulated ATM\nmachine, connected to a speech recognizer and a text to\nspeech system. The interesting fact about this system is\nthat we have actually tried to estimate the complexity\n1 Note that domains are small only relative to a task. For\nexample, the domain of simple banking transactions (bal-\nances, payments, transfers) is small if the task is to be able\nto order the execution of a transaction; but it is large if we\nwere to provide explanations how the execution takes place,\nand why this way and not the other.\nof the domain before building the application.\nSemantic analysis of linguistic representation\nThe system currently has three categories of actions:\nexecutable, recognized, and unknown.\nThe ﬁrst cat-\negory comprises of 9 actions, each with 0-3 param-\neters: transfer/3, withdraw/2, deposit/2, pay/1,\ninquire/1, summary/1, ok/1 (possibly, a quitting ac-\ntion) quit/0, and help/0.\nThe category of non-\nexecutable but recognized action consists of: cancel,\npast (anything about events), and time (anything re-\nferring to temporal data). Everything else is unknown.\nThe parameters of actions are among:\naccount,\naccount to, account from,\nand 5 types of bills\n(mastercard, electricity, insurance, visa,\nand\nphone).\nThe semantic engine, which maps linguistic represen-\ntations into executable actions and their parameters con-\nsists of 110 prolog facts and 20 rules. The facts are used\nmostly to categorize linguistic constructions, situations,\nand names of actions and parameters. For instance, in\nthis domain declarative sentences are all interpreted as\npotential commands; ”checking” is interpreted as a re-\nquest for information (in the absence of other context);\nand one action can be represented by diﬀerent verbs\n(synonyms).\nThe 20 rules describe how to get the actions names\nand their attributes from linguistic representations. The\naverage size of a rule is about 12 predicates.\nClearly, the size of this knowledge base seems to con-\nﬁrm the 10:1 ratio of axioms to concepts we discussed in\nprevious sections.\nLinguistic analysis\nLinguistic analysis is performed by a chart parser,\nwhich has been used without any changes in all our ap-\nplications. Thus, its complexity can be factored out from\nour current discussion.\nThe grammar of the ATM dialog machine consists of\nabout 600 lexical entries and about 200 productions In\naddition, there are 22 domain speciﬁc rules used to con-\nstrain the parser (e.g. that nouns describing ”utilities”\ndo not modify each other, in sentences such as ”pay\nphone visa and insurance”).\nOur ﬁrst rough estimate of the complexity of gram-\nmars for such small domains was based on the 10:1 ratio.\nThat is we assumed that we would have 10 synonyms\nfor each lexical entry in a given class, and that that\nthe grammar would require adding about 40 new con-\nstructions to the calendar grammar we used before, and\nthat we would need about 120 productions altogether.\nIt turned out that indeed we had to add about 40 new\nproductions, but in addition, we had to add about 40\ndomain dependent entries that behave like idioms.\nThus our initial estimate was not correct; and we hy-\npothesize in Section 5.3.\nthat Zipf’s laws could be a\nbetter way of estimating the size of a grammar.\nDialog management\nThere are 7 main states in the dialog management pro-\ngram (the 5 states described in Section 4.2., plus ”ini-\ntialize” and ”end”). Each state has on average about\n10 sub-states. These numbers agree with the estimates\ngiven in Section 4.2.\nThe knowledge base associated with the applications\nspeciﬁes how to ask for a parameter depending on a situ-\nation. E.g. whether to ask for a value, or suggest a value,\nor assume a default. Thus its size is proportional to the\nnumber of diﬀerent situations, and its upper bound is\nthe sum of the powers 2pa of parameters of actions pa,\nfor all actions a (not a large number if the number of\nparameters is small).\nExecuting actions\nAn action to be executed must have the required pa-\nrameters present.\nFor the actions in our domain the\nparameters are given either as a list (”phone and insur-\nance”), or as an attribute (e.g. ”all”, coming from ”pay\nall my bills”). The handling of the two cases is diﬀer-\nent, for instance, because the reply to the user should\nbe diﬀerent in each case. Altogether we have 21 cases of\nexecute action, each described by an average of 8 clauses.\n5.2\nA comparison of two dialog systems\nUsing the tools introduced above we can analyze other\nNLP systems. In this section, we compare two dialog sys-\ntems: mincal ([29]), a calendar management program,\nand boris ([5], [8]), whose domain of expertise was a\ndivorce case:\n-- Why did Sarah lose her divorce case?\n-- She cheated on Paul.\nIf we compare boris ([5], [8]) with mincal we no-\ntice some clear parallels.\nFirst, they have an almost\nidentical vocabulary size of about 350 words. Secondly,\nthey have a similar number of background knowledge\nfacts. Namely, boris uses around 50 major knowledge\nstructures such as Scripts, TAUs, MOPs, Settings, Re-\nlationships etc.; on average, the size of each such struc-\nture would not exceed 10 Prolog clauses/axioms, with\nno more than 4 predicates with 2-3 variables each per\nclause, if it were implemented in Prolog. If we apply a\nsimilar metrics to mincal, we get about 200 facts ex-\npressing background knowledge about time, events and\nthe calendar, plus about 100 grammatical constructions,\nmany of them dealing with temporal expressions, others\nwith agents, actions etc. Clearly then the two systems\nare of about the same size. Finally, the main algorithms\ndo not diﬀer much in their complexities (as measured by\nsize and what they do).\nWe now can explain the diﬀerence in the apparent\nsemantic complexities of their respective domains. First,\nthe vocabulary sizes and the sizes of knowledge bases\nof boris and mincal are almost identical. Thus, their\n”what”-complexities are roughly the same.\nBut we should note that for dialog systems it makes\nsense to talk about iterated Q-complexity.\nThat is, if\nthe Q-complexity of one round of dialog is X, then the\ncomplexity of 2 rounds might be larger than X; e.g. if\nwhat-complexity of a help system is 200, measured by\nthe number of items we can ask about at the very be-\nginning of a conversation, and if we are permitted to\nask what-questions about any item in the answers, the\n”twice iterated what-complexity” of the system might be\n3000.\nNow, our theory can give an explanation of why the\nsentence The meeting is at 5 seems simpler than Sarah\ncheated on Paul. Namely, for the last sentence we assume\nnot only the ability to derive and discuss the immediate\nconsequences of that fact such as ”broken obligation ”\nor ”is Paul aware of it?”, but also such related topics as\n”Sarah’s emotional life” , ”diseases”, ”antibiotics”, and\n”death of grandmother”. In other words, the real com-\nplexity of discussing a narrative is at least the complexity\nof ”iterated-what” combined with ”iterated-why” (and\nmight as well include alternative questions).\nBy the arguments of the preceding section, this would\nrequire really extensive background knowledge, and the\nQ-complexity (measured by the number of facts) would\nrange between 105 and 107; assuming each round of di-\nalog requires an iteration of ”why” and ”what”, with at\nleast 5 rounds of dialog, and an average of 10 facts per\ntoken in the database of background knowledge. Hence,\nthe domain of the marital relations, deﬁned by the abil-\nity to discuss any relevant and related topic, is not a\nsmall domain.\nIn contrast, the Q-complexity of mincal is less than\n1, 000 (measured again by the number of facts), there\nare no restrictions on the number of exchanges; but\nthere is an implicit assumption that all dialogs are re-\nstricted to the tasks of scheduling, canceling and moving\nmeetings, and there is no expectation of discussing the\ncontent of the meetings with the machine (the meet-\nings could have topics though). Here, the set Q consists\nof questions about parameters of calendar events: ac-\ntion name?, event time?, ....\nThus, the domain of calendar action is manageable,\nthe domain of divorce not. Because the control programs\nfor both domains are of roughly the same complexity,\nthis diﬀerence has to attributed to the size of the required\ndatabase of background facts. The key to controlling the\nsemantic complexity lies in limiting the interaction with\nthe user to a clearly deﬁned set of (wh-) questions. And\nthis is the topic of the next section.\n5.3\nEstimating the grammar size\nAlthough our initial estimate of the size of the grammar\nwas wrong, the number of constructions needed for the\napplication was relatively small.\nThis is also true relative to a class of constructions.\nFor instance, for prepositional phrases constructions,\nonly a small percent of constructions with each prepo-\nsition is needed. For the task of scheduling a room we\nneed 5 out of 30 constructions with ”for” mentioned in\nCollins-Cobuild dictionary ([22]); for ”from” the ratio is\n3 out of 26; for ”with” it is 3/30; and for ”at” 2/24.\nThis observation is not limited\nto prepositional\nphrases. The same pattern holds for constructions with\nthe verb ”to be”, ”to have”, and many phrasal construc-\ntions. But notice that while the domain selects construc-\ntions which makes sense there, the constructions do not\nexplicitly mention the domain. Thus they are reusable;\nthey encode general linguistic knowledge.\nIt seems that estimating the size of the grammar can\nbe based on Zipf’s laws ([30, 10, 20]), which specify dis-\ntributions of linguistic items. According to Zipf, if we\nrank words in a given corpus by their frequencies (i.e.\nhow often they appear in the corpus), with the most fre-\nquent word receiving rank 1, the next one rank 2, and\nso on, until the least frequent, then it can be observed\nthat the following equation holds:\nrank × frequency = constant\nZipf checked that this law (with diﬀerent constants) ap-\nplies to words in many corpora and diﬀerent languages,\nto the number of meanings per word in a dictionary, to\nlengths of articles in diﬀerent editions of Encyclopedia\nBritannica, etc. That is, not only, remarkably enough,\nthe same functional relationship between ranks and fre-\nquencies holds for words in diﬀerent languages and diﬀer-\nent corpora, but it also holds for other linguistic entities.\nZipf’s laws appear in other situations and domains,\nincluding software engineering (e.g. [20] [14]). Further-\nmore, they apply both to syntactic and semantic classi-\nﬁcations. Thus we can conjecture that they would apply\nto English constructions in restricted domains.\nUnder these assumptions it should be possible to esti-\nmate the size of the grammar for a NLP task given the\nnumber of concepts in the task domain, required accu-\nracy of processing, and some domain-independent lan-\nguage parameters. How exactly one would do it seems\nto be an open problem, but Chapter 3 of [20] seems like\na natural starting point.\n6\nComplexity management\nFrom the point of view of building a natural language\ndialog system, the commonsense domain of divorce is\ntoo complex. But the divorce domain can be managed,\ne.g. by restricting the conversation to questions about\nconcrete parameters of divorce settlements.\nHow do we, in general, control the complexity of an\nNLP application? Given the model we have introduced,\nthere are four elements in an NLP system: the concep-\ntual domain, the user, the NLP program, and the appli-\ncation controlled by the program (e.g. a database). The\nmanagement of complexity starts with the deﬁnition of\nthe domain. The number of conceptual entities deter-\nmines the size of the database of background knowledge,\nthe size and complexity of the grammar, and the com-\nplexity of the control program.\nIn part this is accomplished by specifying the types\nof meanings that can be dealt with by the system. The\nconcept of Q-semantics and meaning automata should\nhelp with that part of the analysis. The result should\nbe a bound on the combinatorics of interaction between\nthe user and the system.\nThe next element has to do with the user understand-\ning the capabilities of the system. This includes, in its\nstatic part, deﬁning for the user the range of acceptable\ntopics and the types of NL constructions, e.g. by forbid-\nding all ”why” questions. The dynamic part requires the\nrecognition and control of the borderline between what\nis acceptable and what is not.\nFor instance, a meet-\ning scheduling system can recognize a question about a\npast room assignment only to reply that it does not keep\nthe database of old assignments. The dynamic part must\nthus include the recognition of the intentions of the user,\nand for dialog systems, the control of dialog (e.g. asking\nquestions that require simple yes/no replies).\nSince State × Symbol complexity applies to machines\nat diﬀerent level of abstraction, we can measure at least\nfour diﬀerent types of complexities:\nA. Relative to a task (”what”-, ”why-”, Q-);\nB. Of the main engine;\nC. Of the whole program;\nD. Relative to desired performance.\nArchitectures are there to resolve the constraints of\nrepresentation, complexity, and performance; e.g. there\nare trade-oﬀs between the pipeline architecture for NLP\nand the integrated syntax and semantics. The choice of\nright representations (abstract structures) allows us to\nkeep the value of A smaller than 10K even if the complex-\nity C of the whole program is greater than 100M states.\nThus the management of complexity requires thinking\nabout the best architecture. Furthermore, since it is pos-\nsible to ﬁnd examples where small diﬀerences in D may\ncorrespond to large diﬀerences in B and C (consider e.g.\nthe diﬃculty of improving accuracy of a search engine or\na speech recognition program), the management of the\nexpectations of users and a clear deﬁnition of the task is\nof great importance.\nFinally, it should be noted that although the tools we\nhave introduced to analyze problems allow us to estimate\nthe complexity of NLP programs in new domains, we\ncannot make exact predictions of their complexity. The\ncomplexity measures apply to mathematical models, and\nthey come close to the reality only if those models closely\nresemble the reality. This situation is an exact parallel\nof other engineering disciplines.\n7\nDiscussion\nWe have introduced a methodology for predicting the\ndiﬃculty of coding new natural language processing pro-\ngrams and for analyzing existing ones. Our methods can\nalso be used in managing complexity of NLP applications\nby guiding possible reformulations of the tasks.\nThe method is based on the idea of deﬁning semantic\ncomplexity with respect to a set of (abstract) questions,\ni.e. types of information that the program is supposed\nto compute.\nSince those types of information depend\non a task, the complexity of the task can be computed\nfrom the complexity of the question automaton. Fur-\nthermore, this complexity is invariant with respect to\nthe representation language (because of the invariance\nof the Kolmogorov complexity on which it is based). 2\nThe paper deﬁnes (for NLP) a set of complexity mea-\nsures, such as the size of background knowledge, and\ndiﬀerent types of wh-complexity (based on the types of\nquestions). The complexities can be measured in (Num-\nber of States) × (Number of Symbols), and by the Num-\nber of Axioms, assuming a constant (equal to 10) ratio\nof the number of axioms per concept. The ﬁrst measure\nis used for the complexity of the control; the second for\nbackground knowledge.\nThis kind of analysis can be applied to other related\nprograms: both those documented in the literature, e.g.\n[1], [24], [8], [18], and the small application domains\nin which we tested our approach to dialog management\n(calendar, banking transactions, fast food, insurance and\nemail).\nWe have also shown how the complexities of dialog\nmachines and background knowledge can be computed\nfor knowledge-based NLP systems. We have speculated\nthat the complexity of the NLP grammar can be com-\nputed based on Zipf’s laws.\nHowever, the paper is only the ﬁrst step in the direc-\ntion of making quantitative assessments of the diﬃculty\nof building NLP systems; and many of natural important\nquestions remain open. For instance, the question about\nthe size of the NL grammar per task; or how to measure\nthe complexity of NLP tasks based on diﬀerent architec-\ntures, such as a cascade of ﬁnite state automata, or the\ntraditional morphology-syntax-semantics-pragmatics.\nThe ﬁnal point in our discussion of the methods intro-\nduced in the paper is their possible relevance for software\nengineering. Here we would like to make the following\npoints.\n2The existence of such invariant complexity measures is\nnot obvious, for example, Simon in [21], p.228, wrote ”How\ncomplex or simple a structure is depends critically upon the\nway in which we describe it. Most of the complex structures\nfound in the world are enormously redundant, and we can\nuse this redundancy to simplify their description.\nBut to\nuse it, to achieve this simpliﬁcation, we must ﬁnd the right\nrepresentation”.\n1. We have shown how NLP can be thought of as a\nsubdomain of software engineering.\n2. The method of complexity analysis based on Q-\nautomata should apply to other programming tasks.\n3. It is possible that the theory of Q-complexity can\nprovide the theoretical justiﬁcation for some soft-\nware metrics.\n4. We have shown that the complexity of NLP tasks\ncan be estimated at diﬀerent levels of abstraction,\nand the same should hold for any other program-\nming task.\nReferences\n[1] E. Bilange and J-Y. Magadur. A robust approach\nfor handling oral dialogues. Proc. Coling’92, pages\n799–805, 1992.\n[2] L. Bloomﬁeld. A set of postulates for the science\nof language. Language, vol.I, pages 799–805, 1926.\nReprinted in: Hayden, D.E. et al. (Eds.). Classics in\nLinguistics. Philosophical Library Inc., NY, 1967.\n[3] H. Bunt.\nContext and dialogue control.\nThink,\n3(May):19–31, 1994.\n[4] E.J. Crothers. Paragraph Structure Inference. Ablex\nPublishing, Norwood, New Jersey, 1979.\n[5] M.G. Dyer. In-Depth Understanding. MIT Press,\nCambridge, MA, 1983.\n[6] A.C. Graesser. Prose Comprehension Beyond the\nWord. Springer, New York, NY, 1981.\n[7] S. B. Huﬀman.\nLearning information extraction\npatterns from examples. Proc. IJCAI-95 workshop\non New Approaches to Learning for Natural Lan-\nguage Processing, 1995.\n[8] W. Lehnert, M.G.Dyer, P.N.Johnson, C.J.Yang,\nand S. Harley. Boris – an experiment in in-depth\nunderstanding of narratives. Artiﬁcial Intelligence,\n20(1):15–62, 1983.\n[9] M.Li. and P.Vitanyi.\nAn Introduction to Kol-\nmogorov Complexity and Its Applications. Springer,\nNew York, 1993.\n[10] B. Mandelbrot.\nOn the theory of word frequen-\ncies and on related Markovian models of discourse.\nIn R. Jakobson, editor, Structure of Language and\nits Mathematical Aspects. Proc. Symp. in Applied\nMath. vol. XII, pages 190–219, Providence, RI,\n1961. American Mathematical Society.\n[11] H.Moravec.\nMind Children.\nHarvard University\nPress, Cambridge, MA, 1988.\n[12] H. Moravec. Caution! robot vehicle!\nIn V. Lifs-\nchitz, editor, Artiﬁcial Intelligence and mathemati-\ncal theory of of computation. Papers in honor of J.\nMcCarthy, pages 331–344. 1991.\n[13] Proc. of the 5th Message Understanding Confer-\nence. Morgan Kaufman, 1993.\n[14] K. Nakanishi and T. Arano. Class library maturity\nmetric. NTT R&D, 44(1):9–14, 1995.\n[15] J. Rissanen. A universal prior for integers and esti-\nmation by minimum description length. Annals of\nStatistics, 11:416–431, 1982.\n[16] R.Quirk and S.Greenbaum. A Concise Grammar of\nContemporary English. Harcourt Brace Jovanovich,\nInc., New York, NY, 1973.\n[17] W. J. Savitch. Why it might pay to assume that\nlanguages are inﬁnite. Annals of Mathematics and\nArtiﬁcial Intelligence, 8(1,2):17–26, 1993.\n[18] R. C. Schank, editor. Conceptual Information Pro-\ncessing. Americal Elsevier, New York, NY, 1975.\n[19] S. Seneﬀ. Tina: A natural language system for spo-\nken language application. Computational Linguis-\ntics, 18(1):61–86, 1992.\n[20] M.L. Shooman.\nSoftware Engineering.\nMcGraw-\nHill, New York, 1983.\n[21] H. Simon. The Sciences of the Artiﬁcial. MIT Press,\nCambridge, MA, 1981.\n[22] J. Sinclair (ed.). Collins-Cobuild English Language\nDictionary. Collins ELT., London, 1987.\n[23] J. Weizenbaum.\nEliza.\nCommunications of the\nACM, 9(1):36–45, 1966.\n[24] R. Wilensky, D.N. Chin, M. Luria, J. Martin,\nJ. Mayﬁeld, and D. Wu. The Berkeley Unix consul-\ntant project. Computational Linguistics, 14(4):35–\n84, 1988.\n[25] T. Winograd and F. Flores. Understanding Com-\nputers and Cognition. Ablex, Norwood, NJ, 1986.\n[26] W. Zadrozny. Reasoning with background knowl-\nedge – a three-level theory. Computational Intelli-\ngence, 10(2):150–184, 1994.\n[27] W.Zadrozny and K.Jensen.\nSemantics of para-\ngraphs. Computational Linguistics, 17(2):171–210,\n1991.\n[28] W. Zadrozny.\nOn the complexities of NLP sys-\ntems. In Proc. FLAIRS’96, The 9th Florida Arti-\nﬁcial Intelligence Research Symposium. Key West,\nFL., 1996.\n[29] W. Zadrozny, M. Szummer, S. Jarecki, D. E. John-\nson, and L. Morgenstern. NL understanding with a\ngrammar of constructions. Proc. Coling’94, 1994.\n[30] G.K. Zipf.\nHuman Behavior and the Princi-\nple of Least Eﬀort.\nAddison-Wesley Press, Cam-\nbridge,MA, 1949.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1996-07-13",
  "updated": "1996-07-13"
}