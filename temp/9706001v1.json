{
  "id": "http://arxiv.org/abs/cmp-lg/9706001v1",
  "title": "Assigning Grammatical Relations with a Back-off Model",
  "authors": [
    "Erika F. de Lima"
  ],
  "abstract": "This paper presents a corpus-based method to assign grammatical\nsubject/object relations to ambiguous German constructs. It makes use of an\nunsupervised learning procedure to collect training and test data, and the\nback-off model to make assignment decisions.",
  "text": "arXiv:cmp-lg/9706001v1  4 Jun 1997\nAssigning Grammatical Relations with a Back-oﬀModel\nErika F. de Lima\nGMD - German National Research Center for Information Technology\nDolivostrasse 15\n64293 Darmstadt, Germany\ndelima@darmstadt.gmd.de\nAbstract\nThis paper presents a corpus-based method\nto assign grammatical subject/object re-\nlations to ambiguous German constructs.\nIt makes use of an unsupervised learning\nprocedure to collect training and test data,\nand the back-oﬀmodel to make assignment\ndecisions.\n1\nIntroduction\nAssigning a parse structure to the German sentence\n(1) involves addressing the fact that it is syntacti-\ncally ambiguous:\n(1) Eine hohe Inﬂationsrate erwartet die ¨Okonomin.\na high inﬂation rate\nexpects the economist\n‘The economist expects a high inﬂation rate.’\nIn this sentence it must be determined which nom-\ninal phrase is the subject of the verb. The verb er-\nwarten (‘to expect’) takes, in one reading, a nom-\ninative NP as its subject and an accusative NP as\nits object. The nominal phrases preceding and fol-\nlowing the verb in (1) are both ambiguous with\nrespect to case; they may be nominative or ac-\ncusative. Further, both NPs agree in number with\nthe verb, and since in German any major con-\nstituent may be fronted in a verb-second clause,\nboth NPs may be the subject/object of the verb.\nIn this example, morpho-syntactical information is\nnot suﬃcient to determine that the nominal phrase\n[NP die ¨Okonomin] (‘the economist’) is the subject of\nthe verb, and [NP Eine hohe Inﬂationsrate] (‘a high\ninﬂation rate’) its object.\nDetermining the subject/object of an ambiguous\nconstruct such as (1) with a knowledge-based ap-\nproach requires (at least) a lexical representation\nspecifying the classes of entities which may serve as\narguments in the relation(s) denoted by each verb\nin the vocabulary, as well as membership informa-\ntion with respect to these classes for all entities de-\nnoted by nouns in the vocabulary. One problem with\nthis approach is that it is usually not available for a\nbroad-coverage system.\nThis paper proposes an approximation, similar to\nthe empirical approaches to PP attachment decision\n(Hindle and Rooth, 1993; Ratnaparkhi, Reynar, and\nRoukos, 1994; Collins and Brooks, 1995).\nThese\nmake use of unambiguous examples provided by a\ntreebank or a learning procedure in order to train a\nmodel to decide the attachment of ambiguous con-\nstructs.\nIn the current setting, this approach in-\nvolves learning the classes of nouns occurring unam-\nbiguously as subject/object of a verb in sample text,\nand using the classes thus obtained to disambiguate\nambiguous constructs.\nUnambiguous examples are provided by sentences\nin which morpho-syntactical information suﬃces to\ndetermine the subject and object of the verb. For in-\nstance in (2), the nominal phrase [NP der ¨Okonom]\nwith a masculine head noun is unambiguously nom-\ninative, identifying it as the subject of the verb. In\n(3), both NPs are ambiguous with respect to case;\nhowever, the nominal phrase [NP Die ¨Okonomen]\nwith a plural head noun is the only one to agree in\nnumber with the verb, identifying it as its subject.\n(2) Eine hohe Inﬂationsrate erwartet der ¨Okonom.\na high inﬂation rate\nexpects the economist\n‘The economist expects a high inﬂation rate.’\n(3) Die ¨Okonomen erwarten eine hohe Inﬂationsrate.\nthe economists expect\na high inﬂation rate\n‘The economists expect a high inﬂation rate.’\nThis paper describes a procedure to determine the\nsubject and object in ambiguous German constructs\nautomatically. It is based on shallow parsing tech-\nniques employed to collect training and test data\nfrom (un)ambiguous examples in a text corpus,\nand the back-oﬀmodel to determine which NP in\na morpho-syntactically ambiguous construct is the\nsubject/object of the verb, based on the evidence\nprovided by the collected training data.\n2\nCollecting Training and Test Data\nShallow parsing techniques are used to collect train-\ning and test data from a text corpus. The corpus\nis tokenized, morphologically analyzed, lemmatized,\nand parsed using a standard CFG parser with a\nhand-written grammar to identify clauses containing\na ﬁnite verb taking a nominative NP as its subject\nand an accusative NP as its object.\nConstructs covered by the grammar include verb-\nsecond and verb-ﬁnal clauses. Each clause is seg-\nmented into phrase-like constituents, including nom-\ninative (NC), prepositional (PC), and verbal (VC)\nconstituents. Their deﬁnition is non-standard; for\ninstance, all prepositional phrases, whether comple-\nment or not, are left unattached. As an example,\nthe shallow parse structure for the sentence in (4) is\nshown in (4′) below.\n(4) Die Gesellschaft erwartet in diesem Jahr\nthe society\nexpects in this year\nin S¨udostasien\neinen Umsatz\nin southeast Asia a turnover\nvon 125 Millionen DM.\nfrom 125 million DM\n‘The society expects this year in southeast Asia\na turnover of 125 million DM.’\n(4′)\n[S [NC3,s,{nom,acc} Die Gesellschaft]\n[V C3,s erwartet]\n[P C in diesem Jahr]\n[P C in S¨udostasien]\n[NC3,s,acc einen Umsatz]\n[P C von 125 Millionen DM]\n]\nNominal and verbal constituents display person and\nnumber information; nominal constituents also dis-\nplay case information. For instance in the structure\nabove, 3 denotes third person, s denotes singular\nnumber, nom and acc denote nominative and ac-\ncusative case, respectively. The set {nom, acc} indi-\ncates that the ﬁrst nominal constituent in the struc-\nture is ambiguous with respect to case; it may be\nnominative or accusative.\nTest and training tuples are obtained from shallow\nstructures containing a verbal constituent and two\nnominative/accusative nominal constituents. Note\nthat no subcategorization information is used; it suf-\nﬁces for a verb to occur in a clause with two nom-\ninative/accusative NCs for it to be considered test-\ning/training data.\nTraining data consists of tuples (n1, v, n2, x),\nwhere v is a verb, n1 and n2 are nouns, and\nx\n∈\n{1, 0} indicates whether n1 is the subject\nof the verb.\nTest data consists of ambiguous tu-\nples (n1, v, n2) for which it cannot be established\nwhich noun is the subject/object of the verb based\non morpho-syntactical information alone.\nThe set of training and test tuples for a given cor-\npus is obtained as follows. For each shallow structure\ns in the corpus containing one verbal and two nomi-\nnative/accusative nominal constituents, let n1, v, n2\nbe such that v is the main verb in s, and n1 and n2\nare the heads of the nominative/accusative NCs in\ns such that n1 precedes n2 in s. In the rules below,\ni, j ∈{1, 2}, j ̸= i, and g(i) = 1 if i = 1, and 0\notherwise. Note that the last element in a training\ntuple indicates whether the ﬁrst NC in the structure\nis the subject of the verb (1 if so, 0 otherwise).\nCase Nominative Rule. If ni is masculine, and\nthe NC headed by ni is unambiguously nominative1,\nthen (n1, v, n2, g(i)) is a training tuple,\nCase Accusative Rule. If ni is masculine, and the\nNC headed by ni is unambiguously accusative, then\n(n1, v, n2, g(j)) is a training tuple,\nAgreement Rule.\nIf ni but not nj agrees with\nv in person and number, then (n1, v, n2, g(i)) is a\ntraining tuple,\nHeuristic Rule. If the shallow structure consists\nof a verb-second clause with an adverbial in the ﬁrst\nposition, or of a verb-ﬁnal clause introduced by a\nconjunction or a complementizer, then (n1, v, n2, 1)\nis a training tuple (see below for examples),\nDefault Rule. (n1, v, n2) is a test triple.\nFor instance, the training tuple (Gesellschaft, er-\nwarten, Umsatz, 1) (‘society, expect, turnover’) is\nobtained from the structure (4′) above with the\nCase Accusative Rule, since the NC headed by\nthe masculine noun Umsatz (‘turnover’) is unam-\nbiguously accusative and hence the object of the\nverb. The training tuple (Inﬂationsrate, erwarten,\n¨Okonom, 0) (‘inﬂation rate, expect, economist’) and\n( ¨Okonom, erwarten, Inﬂationsrate, 1) (‘economist,\nexpect, inﬂation rate’) are obtained from sentences\n(2) and (3) with the Case Nominative and Agree-\nment Rules, respectively, and the test tuple (Inﬂa-\ntionsrate, erwarten, ¨Okonomin) (‘inﬂation rate, ex-\npect, economist’) from the ambiguous sentence in\n(1) by the Default Rule.\n1Only NCs with a masculine head noun may be un-\nambiguous with respect to nominative/accusative case\nin German.\nThe Heuristic Rule is based on the observation\nthat in the constructs stipulated by the rule, al-\nthough the object may potentially precede the sub-\nject of the verb, this does not (usually) occur in writ-\nten text. (5) and (6) are sentences to which this rule\napplies.\n(5) In diesem Jahr erwartet die ¨Okonomin\nin this year\nexpects the economist\neine hohe Inﬂationsrate.\na high inﬂation rate\n‘This year the economist expects a high\ninﬂation rate.”\n(6) Weil\ndie ¨Okonomin eine hohe Inﬂationsrate\nbecause the economist a high inﬂation rate\nerwartet, . . .\nexpects\n‘Because the economist expects a high inﬂation\nrate, . . . ’\nNote that the Heuristic Rule does not apply to verb-\nﬁnal clauses introduced by a relative or interrogative\nitem, such as in (7):\n(7) Die Rate, die\ndie ¨Okonomin erwartet, . . .\nthe rate\nwhich the economist expects, . . .\n3\nTesting\nThe testing algorithm makes use of the back-oﬀ\nmodel (Katz, 1987) in order to determine the sub-\nject/object in an ambiguous test tuple. The model,\ndeveloped within the context of speech recognition,\nconsists of a recursive procedure to estimate n-gram\nprobabilities from sparse data. Its generality makes\nit applicable to other areas; the method has been\nused, for instance, to solve prepositional phrase at-\ntachment in (Collins and Brooks, 1995).\n3.1\nKatz’s back-oﬀmodel\nLet wn\n1 denote the n-gram w1, . . . , wn, and f(wn\n1 )\ndenote the number of times it occurred in a sample\ntext. The back-oﬀestimate computes the probabil-\nity of a word given the n −1 preceding words. It\nis deﬁned recursively as follows.\n(In the formulae\nbelow, α(wn−1\n1\n) is a normalizing factor and dr a dis-\ncount coeﬃcient.\nSee (Katz, 1987) for a detailed\naccount of the model.)\nPbo(wn|wn−1\n1\n)=\n\u001a˜P(wn|wn−1\n1\n), if ˜P(wn|wn−1\n1\n) > 0\nα(wn−1\n1\n)Pbo(wn|wn−1\n2\n), otherwise,\nwhere ˜P(wn|wn−1\n1\n) is deﬁned as follows:\n˜P(wn|wn−1\n1\n) =\n(\ndf(wn\n1 )\nf(wn\n1 )\nf(wn−1\n1\n),\nif f(wn−1\n1\n) ̸= 0\n0,\notherwise.\n3.2\nThe Revised Model\nIn the current context, instead of estimating the\nprobability of a word given the n−1 preceding words,\nwe estimate the probability that the ﬁrst noun n1 in\na test triple (n1, v, n2) is the subject of the verb v,\ni.e., P(S = 1|N1 = n1, V = v, N2 = n2) where S is\nan indicator random variable (S = 1 if the ﬁrst noun\nin the triple is the subject of the verb, 0 otherwise).\nIn the estimate Pbo(wn|wn−1\n1\n) only one relation—\nthe precedence relation—is relevant to the problem;\nin the current setting, one would like to make use of\ntwo implicit relations in the training tuple—subject\nand object—in order to produce an estimate for\nP(1|n1, v, n2). The model below is similar to that\nin (Collins and Brooks, 1995).\nLet L be the set of lemmata occurring in the\ntraining triples obtained from a sample text, and let\nc(n1, v, n2, x) denote the frequency count obtained\nfor the training tuple (n1, v, n2, x) (x ∈{0, 1}). We\ndeﬁne the count fso(n1, v, n2) = c(n1, v, n2, 1) +\nc(n2, v, n1, 0) of n1 as the subject and n2 as the ob-\nject of v. Further, we deﬁne the count fs(n1, v) =\nP\nn2∈L fso(n1, v, n2) of n1 as the subject of v with\nany object, and analogously, the count fo(n1, v) of\nn1 as the object of v with any subject.\nFurther,\nwe deﬁne the counts fs(v) = P\nn1,n2∈L c(n1, v, n2, 1)\nand fo(v) = P\nn1,n2∈L c(n1, v, n2, 0). The estimate\nPi(1|n1, v, n2) (0 ≤i ≤3) is deﬁned recursively as\nfollows:\nP0(1|n1, v, n2) = 1.0\nPi(1|n1, v, n2) =\n(\nci(n1,v,n2)\nti(n1,v,n2), if ti(n1, v, n2) > 0\nP(i−1)(1|n1, v, n2), otherwise,\nwhere the counts ci(n1, v, n2), and ti(n1, v, n2) are\ndeﬁned as follows:\nci(n1, v, n2) =\n\n\n\nfso(n1, v, n2),\nif i = 3\nfs(n1, v) + fo(n2, v),\nif i = 2\nfs(v),\nif i = 1\nti(n1, v, n2) =\n\n\n\nfso(n1, v, n2) + fso(n2, v, n1),\nif i = 3\nfs(n1, v)+fo(n1, v)+fs(n2, v)+fo(n2, v), if i = 2\nfs(v) + fo(v),\nif i = 1\nThe deﬁnition of P3(1|n1, v, n2) is analogous to that\nof Pbo(wn|wn−1\n1\n). In the case where the counts are\npositive, the numerator in the latter is the number\nof times the word wn followed the n-gram wn−1\n1\nin\ntraining data, and in the former, the number of times\nn1 occurred as the subject with n2 as the object of v.\nThis count is divided, in the latter, by the number\nof times the n-gram wn−1\n1\nwas seen in training data,\nand in the former, by the number of times n1 was\nseen as the subject or object of v with n2 as its\nobject/subject respectively.\nHowever, the deﬁnition of P2(1|n1, v, n2) is some-\nwhat diﬀerent; it makes use of both the subject\nand object relations implicit in the tuple.\nIn\nP2(1|n1, v, n2), one combines the evidence for n1 as\nthe subject of v (with any object) with that of n2 as\nthe object of v (with any subject).\nAt the P1 level, only the counts obtained for the\nverb are used in the estimate; although for certain\nverbs some nouns may have deﬁnite preferences for\nappearing in the subject or object position, this in-\nformation was deemed on empirical grounds not to\nbe appropriate for all verbs.\nWhen the verb v in a test tuple (n1, v, n2)\ndoes not occur in any training tuple, the default\nP0(1|n1, v, n2) = 1.0 is used; it reﬂects the fact that\nconstructs in which the ﬁrst noun is the subject of\nthe verb are more common.\n3.3\nDecision Algorithm\nThe decision algorithm determines for a given test\ntuple (n1, v, n2), which noun is the subject of the\nverb v.\nIn case one of the nouns in the tuple is\na pronoun, it does not make sense to predict that\nit is subject/object of a verb based on how often it\noccurred unambiguously as such in a sample text. In\nthis case, only the information provided by training\ndata for the noun in the test tuple is used. Further,\nin case both heads in a test tuple are pronouns, the\ntuple is not considered. The algorithm is as follows.\nIf n1 and n2 are both nouns, then n1 is the subject\nof v if P3(1|n1, v, n2) ≥0.5, else its object.\nIn case n2 (but not n1) is a pronoun, redeﬁne ci and\nti as follows:\nci(n1, v, n2) =\n\u001a\nfs(n1, v),\nif i = 2\nfs(v),\nif i = 1\nti(n1, v, n2) =\n\u001a\nfs(n1, v) + fo(n1, v),\nif i = 2\nfs(v) + fo(v),\nif i = 1\nand calculate P2(1|n1, v, n2) with these new deﬁni-\ntions. If P2(1|n1, v, n2) ≥0.5, then n1 is the subject\nof the verb v, else its object. We proceed analogously\nin case n1 (but not n2) is a pronoun.\n3.4\nRelated Work\nIn (Collins and Brooks, 1995) the back-oﬀmodel\nis used to decide PP attachment given a tuple\n(v, n1, p, n2), where v is a verb, n1 and n2 are nouns,\nand p a preposition such that the PP headed by p\nmay be attached either to the verb phrase headed\nby v or to the NP headed by n1, and n2 is the head\nof the NP governed by p.\nThe model presented in section 3.2 is similar to\nthat in (Collins and Brooks, 1995), however, unlike\n(Collins and Brooks, 1995), who use examples from\na treebank to train their model, the procedure de-\nscribed in this paper uses training data automati-\ncally obtained from sample text. Accordingly, the\nmodel must cope with the fact that training data is\nmuch more likely to contain errors. The next sec-\ntion evaluates the decision algorithm as well as the\ntraining data obtained by the learning procedure.\n4\nResults\nThe method described in the previous section was\napplied to a text corpus consisting of 5 months of the\nnewspaper Frankfurter Allgemeine Zeitung with ap-\nproximately 15 million word-like tokens. The learn-\ning procedure produced a total of 24,178 test tuples\nand 47,547 training triples.\n4.1\nLearning procedure\nIn order to evaluate the data used to train the model,\n1000 training tuples were examined. Of these tuples,\n127 were considered to be (partially) incorrect based\non the judgments of a single judge given the original\nsentence. Errors in training and test data may stem\nfrom the morphology component, from the grammar\nspeciﬁcation, from the heuristic rule, or from actual\nerrors in the text.\n4.1.1\nSubcategorization Information\nThe system works without subcategorization in-\nformation; it suﬃces for a verb to occur with a possi-\nbly nominative and a possibly accusative NC for it to\nbe considered training/test data. Lack of subcatego-\nrization leads to errors when verbs occurring with an\n(ambiguous) dative NC are mistaken for verbs which\nsubcategorize for an accusative nominal phrase. For\ninstance in (7) below, the verb geh¨oren (‘to belong’)\ntakes, in one reading, a dative NP as its object and\na nominative NP as its subject.\nSince the nomi-\nnal constituent [NC Bill] is ambiguous with respect\nto case and possibly accusative, the erroneous tu-\nple (Wagen, geh¨oren, Bill, 1) (‘car, belong, Bill’) is\nproduced for this sentence.\n(7) Der Wagen geh¨ort Bill.\nthe car\nbelongs Bill\n‘The car belongs to Bill.’\nAnother source of errors is the fact that any ac-\ncusative NC is considered an object of the verb.\nFor instance in sentence (8), the verb trainieren (‘to\ntrain’) occurs with two NCs. Since the NC preced-\ning the verb is unambiguously nominative and the\none following the verb possibly accusative, the train-\ning tuple (Tennisspieler, trainieren, Jahr, 1) (‘ten-\nnis player, train, year’) is produced for this sentence,\nalthough the second NC is not an object of the verb.\n(8) Der Tennisspieler trainiert das ganze Jahr.\nthe tennis player trains\nthe whole year\n4.1.2\nHomographs\nIn sentence (9) below, the word morgen (‘to-\nmorrow’) is an adverb.\nHowever, its capitalized\nform may also be a noun, leading in this case to\nthe erroneous training tuple (Morgen, trainieren,\nTennisspieler, 0) (since [NC der Tennisspieler] is un-\nambiguously nominative).\n(9) Morgen\ntrainiert der Tennisspieler.\ntomorrow trains\nthe tennis player\n‘The tennis player will train tomorrow.’\n4.1.3\nSeparable Preﬁxes\nIn German, verb preﬁxes can be separated from\nthe verb. When a ﬁnite (separable preﬁx) main verb\noccupies the second position in the clause, its preﬁx\ntakes the last position in the clause core. For exam-\nple in sentence (10) below, the preﬁx zur¨uck of the\nverb zur¨uckweisen (‘to reject’) follows the object of\nthe verb and a subordinate clause with a subjunc-\ntive main verb. This construct is not covered by the\ncurrent version of the grammar. However, due to\nthe grammar deﬁnition, and since weisen is also a\nverb (without a separable preﬁx) in German, [C Er\nweist die Kritik der Prinzessin] is still accepted as a\nvalid clause, leading to the erroneous training tuple\n(er, weisen, Kritik, 1) (‘he, point, criticism’). Such\nerrors may be avoided with further development of\nthe grammar.\n(10) Er weist\ndie Kritik\nder Prinzessin, seine\nhe rejects the criticism the princess\nhis\nOhren seien zu groß, zur¨uck.\nears\nare\ntoo big PRT\n‘He rejects the princess’ criticism that his ears\nare too big.”\n4.1.4\nConstituent Heads\nThe system is not always able to determine con-\nstituent heads correctly.\nFor instance in sentence\n(11), all words in the name Mexikanische Verband\nf¨ur Menschenrechte are capitalized. Upon encoun-\ntering the adjective Mexikanische, the system takes\nit to be a noun (nouns are capitalized in German),\nfollowed by the noun Verband “in apposition”. Sen-\ntence (11) is the source of the erroneous training tu-\nple (Mexikanisch, beschuldigen, Beh¨orde, 1) (‘Mexi-\ncan, blame, public authorities’).\n(11) Der Mexikanische Verband f¨ur Menschen-\nthe Mexican Association\nfor Human\nrechte beschuldigt die Beh¨orden.\nRights blames\nthe public authorities\n‘The Mexican Association for Human Rights\nblames the public authorities.’\n4.1.5\nMulti-word lexical units\nThe learning procedure has no access to multi-\nword lexical units. For instance in sentence (12), the\nﬁrst word in the expression Hand in Hand is consid-\nered the object of the verb, leading to the training\ntuple (Architekten, arbeiten, Hand, 1) (‘architect,\nwork, hand’). Given the information the system has\naccess to, such errors cannot be avoided.\n(12) Alle Architekten sollen Hand in Hand arbeiten.\nall architects\nshould hand in hand work\n‘All architects should work hand in hand.’\n4.1.6\nSource Text\nNot only spelling errors in the source text are the\nsource of incorrect tuples. For instance in sentence\n(13), the verb suchen (‘to seek’) is erroneously in the\nthird person plural. Since Reihe (‘series’) in German\nis a singular noun, and Kontakte (‘contacts’) plu-\nral, the actual object, but not the subject, agrees in\nnumber with the verb, so the incorrect tuple (Reihe,\nsuchen, Kontakt, 0) (‘series, seek, contact’) is ob-\ntained from this sentence.\n(13) *Eine Reihe von Staaten suchen gesch¨aftliche\na series\nfrom states seek\nbusiness\nKontakte zu der Region.\ncontacts to the region\n‘*A series of states seek contacts to the region.’\nFinally, a large number of errors, specially in test\ntuples, stems from the fact that soft constraints are\nused for words unknown to the morphology.\n4.2\nDecision Algorithm\nIn order to evaluate the accuracy of the decision al-\ngorithm, 1000 triples were selected from the set of\ntest triples. Of these, 285 contained errors, based\nPn\nNumber\nPercent of test tuples\nNumber correct\nAccuracy\nP3\n2\n0.28\n2\n100.00\nP2\n204\n28.53\n194\n95.10\nP1\n486\n67.97\n431\n88.68\nP0\n23\n3.22\n20\n86.96\nTotal\n715\n100.00\n647\n90.49\nFigure 1: The accuracy of the system at each level\non the judgements of a single judge given the origi-\nnal sentence2. The results produced by the system\nfor the remaining 715 tuples were compared to the\njudgements of a single judge given the original text.\nThe system performed with an overall accuracy of\n90.49%.\nA lower bound for the accuracy of the decision al-\ngorithm can be deﬁned by considering the ﬁrst noun\nin every test tuple to be the subject of the verb (by\nfar the most common construct), yielding for these\n715 tuples an accuracy of 87.83%.\nThe above ﬁgure shows how many of the 715 eval-\nuated test tuples were assigned subject/object based\non the values Pn, and the accuracy of the system at\neach level.\nThe accuracy for P2 and P3 exceeds 95%. How-\never, their coverage is relatively low (28.81%). Since\nthe procedure used to collect training data runs\nwithout supervision, increasing the size of the train-\ning set depends only on the availability of sample\ntext and should be further pursued.\nOne reason for the relatively low coverage is\nthe fact that German compound nouns consider-\nably increase the size of the sample space. For in-\nstance, the head of the nominal constituent [NC Der\nTennisspieler] (‘the tennis player’) is considered by\nthe system to be the compound noun Tennisspieler\n(‘tennis player’), instead of its head noun Spieler\n(‘player’). Consistently considering the head of pu-\ntative compound nouns to be the head of nomi-\nnal constituents may in some cases lead to awk-\nward results. However, reducing the size of the sam-\nple space by morphological processing of compound\nnouns should be considered in order to increase cov-\nerage.\n4.2.1\nExamples\nFollowing are examples of test tuples for which a\ndecision was made based on values of P2. All sen-\ntences below stem from the corpus.\nSentence (14) was the source for the test tuple\n(Ausstellung, zeigen, Spektrum) (‘exhibition, show,\n2The higher error rate for test tuples is due to the soft\nconstraints used for words unknown to the morphology.\nspectrum’). This tuple was correctly disambiguated\nwith P2 = 0.87, with, among others, the training\ntuples (Ausstellung, zeigen, Bild, 1) (‘exhibition,\nshow, painting’), (Ausstellung, zeigen, Beispiel, 1)\n(‘exhibition, show, example’), and (Ausstellung,\nzeigen, Querschnitt, 1) (‘exhibition, show, cross-\nsection’) obtained with the Agreement (sentences\n(15) and (16)) and Case Rules (sentence (17)), re-\nspectively.\n(14) Die Ausstellung zeigt das Spektrum j¨udischer\nthe exhibition\nshows the spectrum jewish\nBuchkunst von den Anf¨angen [. . .]\nbook art\nfrom the beginnings\n‘The exhibition shows the spectrum of jewish\nbook art from the beginnings [. . .].’\n(15) die letzte Ausstellung vor der Sommerpause\nthe last exhibition\nbefore the summer pause\nzeigt Bilder und Zeichnungen von Petra\nshows paintings und drawings from Petra\nTrenkel zum Thema “Dorf”.\nTrenkel to the subject village\n‘The last exhibition before the summer pause\nshows paintings and drawings by Petra\nTrenkel on the subject “village”.’\n(16) Die Ausstellung im Museum\nf¨ur Kunst-\nthe exhibition\nin the museum for arts and\nhandwerk zeigt Beispiele seiner vielf¨altigen\ncrafts\nshows examples his manifold\nObjekt-Typen [. . .]\nobject types\n‘The exhibition in the museum for arts and\ncrafts shows examples of his manifold\nobject types [. . .]’\n(17) Eine vom franz¨osischen Kulturinstitut\na\nfrom the French culture institute\nmit Unterst¨utzung des B¨orsenvereins\nwith support the B¨orsenverein\nin der Zentralen Kinder- und Jugendbibliothek\nin the central children and youth library\nim B¨urgerhaus Bornheim\nin the community center Bornheim\neingerichtete Ausstellung zeigt\norganized exhibition\nshows\neinen interessanten Querschnitt.\nan interesting cross-section\n‘A exhibition in the central children’s and\nyouth library in the community center Born-\nheim, organized by the French culture\ninstitute with support of the B¨orsenverein,\nshows an interesting cross-section.’\nSentence (18) below was the source for the test tuple\n(Altersgrenze, nennen, Gesetz) (‘age limit, mention,\nlaw’). The system incorrectly considered the noun\nAltersgrenze to be the subject of the verb.\n(18) Eine Altersgrenze nennt\ndas Gesetz nicht.\nan age limit\nmentions the law\nnot\n‘The law does not mention an age limit.’\nThere were no training tuples in which the com-\npound noun Altersgrenze occurred as the sub-\nject/object of the verb. However, the noun Gesetz\noccurred more frequently as the object of the verb\nnennen than as its subject, leading to the erroneous\ndecision.\n5\nConclusion\nThis paper describes a procedure to automatically\nassign grammatical subject/object relations to am-\nbiguous German constructs. It is based on an unsu-\npervised learning procedure to collect test and train-\ning data and the back-oﬀmodel to make assignment\ndecisions. The system was implemented and tested\non a 15-million word newspaper corpus.\nThe overall accuracy of the decision algorithm was\nalmost 3% higher than the baseline of 87.83% es-\ntablished.\nThe accuracy of the procedure for tu-\nples for which a decision was made based on training\npairs/triples (P2 and P3) exceeded 95%.\nIn order to increase the coverage for these cases as\nwell as the overall performance of the procedure, the\nsample space should be reduced by morphologically\nprocessing German compound nouns, and the size of\nthe training set should be increased. Further, in the\nexperiment described in this paper, the model was\ntrained with data obtained by an unsupervised pro-\ncedure which performs with an accuracy of approxi-\nmately 87% for training data. Further development\nof the morphology component and grammar deﬁni-\ntion should lead to improved results.\n6\nAcknowledgments\nI would like to thank Michael K¨onyves-T´oth, who\ndeveloped the parser engine used in the experiment\ndescribed in this paper, for his support. I would also\nlike to thank Martin B¨ottcher and the anonymous\nreviewers for many helpful comments on an earlier\nversion of the paper.\nReferences\n[Collins and Brooks1995] Collins, Michael and James\nBrooks. 1995. Prepositional phrase attachment\nthrough a backed-oﬀmodel. In Proceedings of the\nThird Workshop on Very Large Corpora.\n[Hindle and Rooth1993] Hindle, Donald and Mats\nRooth.\n1993.\nStructural ambiguity and lexical\nrelations. Computational Linguistics, 19(1).\n[Katz1987] Katz, S. 1987. Estimation of probabili-\nties from sparse data for the language model com-\nponent of a speech recognizer.\nIEEE Transac-\ntions on Acoustics, Speech, and Signal Processing,\n35(3).\n[Ratnaparkhi, Reynar, and Roukos1994]\nRatnaparkhi, A., J. Reynar, and S. Roukos.\n1994.\nA maximum entropy model for preposi-\ntional phrase attachment. In Proceedings of the\nARPA Workshop on Human Language Technol-\nogy.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1997-06-04",
  "updated": "1997-06-04"
}