{
  "id": "http://arxiv.org/abs/cmp-lg/9709010v1",
  "title": "Message-Passing Protocols for Real-World Parsing -- An Object-Oriented Model and its Preliminary Evaluation",
  "authors": [
    "Udo Hahn",
    "Peter Neuhaus",
    "Norbert Broeker"
  ],
  "abstract": "We argue for a performance-based design of natural language grammars and\ntheir associated parsers in order to meet the constraints imposed by real-world\nNLP. Our approach incorporates declarative and procedural knowledge about\nlanguage and language use within an object-oriented specification framework. We\ndiscuss several message-passing protocols for parsing and provide reasons for\nsacrificing completeness of the parse in favor of efficiency based on a\npreliminary empirical evaluation.",
  "text": "arXiv:cmp-lg/9709010v1  23 Sep 1997\nMessage-Passing Protocols for Real-World Parsing —\nAn Object-Oriented Model and its Preliminary\nEvaluation\nUdo Hahn and Peter Neuhaus and Norbert Br¨oker\nComputational Linguistics Lab\nFreiburg University, Werthmannplatz, D-79085 Freiburg, Germany\n{hahn,neuhaus,nobi}@coling.uni-freiburg.de\nhttp://www.coling.uni-freiburg.de\nAbstract\nWe argue for a performance-based design of natural language grammars and their associated parsers in order to\nmeet the constraints imposed by real-world NLP. Our approach incorporates declarative and procedural knowledge\nabout language and language use within an object-oriented speciﬁcation framework. We discuss several message-\npassing protocols for parsing and provide reasons for sacriﬁcing completeness of the parse in favor of eﬃciency based\non a preliminary empirical evaluation.\n1\nIntroduction\nOver the past decades the design of natural language grammars and their parsers was almost entirely based\non competence considerations (Chomsky, 1965). These hailed pure declarativism (Shieber, 1986) and banned\nprocedural aspects of natural language use out of the domain of language theory proper. The major premises of\nthat approach were to consider sentences as the primary object of linguistic investigation, to focus on syntactic\ndescriptions, and to rely upon perfectly well-formed utterances for which complete grammar speciﬁcations of\narbitrary depth and sophistication were available. In fact, promising eﬃciency results can be achieved for parsers\noperating under corresponding optimal laboratory conditions. Considering, however, the requirements of natural\nlanguage understanding, i.e., the integration of syntax, semantics, and pragmatics, and taking ill-formed input\nor incomplete knowledge into consideration, their processing costs either tend to increase at excessive rates or\nlinguistic processing even fails completely.\nAs a consequence, the challenge to meet the speciﬁc requirements imposed by real-world texts has led many\nresearchers in the NLP community to re-engineer competence grammars and their parsers and to provide various\nadd-ons in terms of constraints (Uszkoreit, 1991), heuristics (Huyck & Lytinen, 1993), statistics-based weights\n(Charniak, 1993), etc. In contradistinction to these approaches, our principal goal has been to incorporate\nperformance conditions already in the design of natural language grammars, yielding so-called performance\ngrammars. Thus, not only declarative knowledge (as is common for competence grammars), but also procedural\nknowledge (about control and parsing strategies, resource limitations, etc.) has to be taken into consideration\nat the grammar speciﬁcation level proper. This is achieved by providing self-contained description primitives for\nthe expression of procedural knowledge. We have taken care to transparently separate declarative (structure-\noriented) from procedural (process-oriented) knowledge pieces. Hence, we have chosen a formally homogeneous,\nhighly modularized object-oriented grammar speciﬁcation framework, viz. the actor model of computation which\nis based on concurrently active objects that communicate by asynchronous message passing (Agha, 1990).\nThe parser whose design is based on these performance considerations forms part of a text knowledge acqui-\nsition system, operational in two domains, viz. the processing of test reports from the information technology\nﬁeld (Hahn & Schnattinger, 1997) and medical reports (Hahn et al., 1996b). The analysis of texts (instead\nof isolated sentences) requires, ﬁrst of all, the consideration of textual phenomena by a dedicated text gram-\nmar. Second, text understanding is based on drawing\ninferences by which text propositions are integrated\non the ﬂy into the text knowledge base with reference to a canonical representation of the underlying domain\nknowledge. This way, grammatical (language-speciﬁc) and conceptual (domain-speciﬁc) knowledge are closely\ncoupled. Third, text understanding in humans occurs immediately and at least within speciﬁc processing cycles\nin parallel (Thibadeau et al., 1982). These processing strategies we ﬁnd in human language processing are taken\nas hints how the complexity of natural language understanding can reasonably be overcome by machines. Thus,\ntext parsing devices should operate incrementally and concurrently. In addition, the consideration of real-world\ntexts forces us to supply mechanisms which allow for the robust processing of extra- and ungrammatical input.\nWe take an approach where — in the light of abundant speciﬁcation gaps at the grammar and domain repre-\nsentation level — the degree of underspeciﬁcation of the knowledge sources or the impact of grammar violations\ndirectly corresponds to a lessening of the precision and depth of text knowledge representations, thus aiming at\na sophisticated fail-soft model of partial text parsing.\n2\nThe Grammar\nThe performance grammar we consider contains fully lexicalized grammar speciﬁcations (Hahn et al., 1994).\nEach lexical item is subject to conﬁgurational constraints on word classes and morphological features as well\nas conditions on word order and conceptual compatibility a head places on possible modiﬁers. Grammatical\nconditions of these types are combined in terms of valency constraints (at the phrasal and clausal level) as well\nas textuality constraints (at the text level of consideration), which concrete dependency structures and local\nas well as global coherence relations must satisfy. The compatibility of grammatical features including order\nconstraints (encapsulated by methods we refer to as syntaxCheck) is computed by a uniﬁcation mechanism,\nwhile the evaluation of semantic and conceptual constraints (we here refer to as conceptCheck) relies upon\nthe terminological and rule-based construction of a consistent conceptual representation.\nThus, while the\ndependency relations represent the linguistic structure of the input, the conceptual relations yield the targeted\nrepresentation of the text content (for an illustration, cf. Fig. 7).\nIn order to structure the underlying lexicon, inheritance mechanisms are used. Lexical speciﬁcations are\norganized along the grammar hierarchy at various abstraction levels, e.g., with respect to generalizations on\nword classes. Lexicalization of this form already yields a ﬁne-grained decomposition of declarative grammar\nknowledge. It lacks, however, an equivalent description at the procedural level. We therefore provide lexicalized\ncommunication primitives to allow for heterogeneous and local forms of interaction among lexical items.\nFollowing the arguments brought forward, e.g., by Jackendoﬀ(1990) and Allen (1993), there is no distinction\nat the representational level between semantic and conceptual interpretations of texts. Hence, semantic and\ndomain knowledge speciﬁcations are based on a common hybrid classiﬁcation-based knowledge representation\nlanguage (for a survey, cf. Woods & Schmolze (1992)). Ambiguities which result in interpretation variants are\nmanaged by a context mechanism of the underlying knowledge base system.\nRobustness at the grammar level is achieved by several means.\nDependency grammars describe binary,\nfunctional relations between words rather than contiguous constituent structures. Thus, ill-formed input often\nhas an (incomplete) analysis in our grammar. Furthermore, it is possible to specify lexical items at diﬀerent\nlevels of syntactic or semantic granularity such that the speciﬁcity of constraints may vary. The main burden\nof robustness, however, is assigned to a dedicated message-passing protocol we will discuss in the next section.\n3\nThe Parser\nViewed from a parsing perspective, we represent lexical items as word actors which are acquainted with other\nactors representing the heads or modiﬁers in the current utterance. A specialized actor type, the phrase actor,\ngroups word actors which are connected by dependency relations and encapsulates administrative information\nabout each phrase. A message does not have to be sent directly to a speciﬁc word actor, but will be sent to the\nmediating phrase actor which forwards it to an appropriate word actor. Furthermore, the phrase actor holds the\ncommunication channel to the corresponding interpretation context in the domain knowledge base system. A\ncontainer actor encapsulates several phrase actors that constitute alternative analyses for the same part of the\ninput text (i.e., structural ambiguities). Container actors play a central role in controlling the parsing process,\nbecause they keep information about the textually related (preceding) container actors holding the left context\nand the chronologically related (previous) container actors holding a part of the head-oriented parse history.\nBasic Parsing Protocol (incl. Ambiguity Handling). We use a graphical description language to sketch\nthe message-passing protocol for establishing dependency relations as depicted in Fig. 1 (the phrase actor’s active\nFigure 1: Basic Mode (incl. Structural Ambiguities)\nhead is visualized by L). A searchHeadFor message (and vice versa a searchModiﬁerFor message if searchHeadFor\nfails) is sent to the textually preceding container actor (precedence relations are depicted by bold dashed lines),\nwhich simultaneously directs this message to its encapsulated phrase actors. At the level of a single phrase actor,\nthe distribution of the searchHeadFor message occurs for all word actors at the “right rim” of the dependency\ntree (depicted by\n). A word actor that receives a searchHeadFor message from another word actor concurrently\nforwards this message to its head (if any) and tests in its local site whether a dependency relation can be\nestablished by checking its corresponding valency constraints (applying syntaxCheck and conceptCheck).\nIn case of success, a headFound message is returned, the sender and the receiver are copied (to enable alternative\nattachments in the concurrent system, i.e., no destructive operations are carried out), and a dependency relation,\nindicated by a dotted line, is established between those copies which join into a phrasal relationship (for a more\ndetailed description of the underlying protocols, cf. Neuhaus & Hahn (1996)). For illustration purposes, consider\nthe analysis of a phrase like “Zenon sells this printer” covering the content of the phrase actor which textually\nprecedes the phrase actor holding the dependency structure for “for $2,000”. The latter actor requests its\nattachment as a modiﬁer of some head.\nThe resultant new container actor (encapsulating the dependency\nanalysis for “Zenon sells this printer for $2,000” in two phrase actors) is, at the same time, the historical\nsuccessor of the phrase actor covering the analysis for “Zenon sells this printer”.\nThe structural ambiguity inherent in the example is easily accounted for by this scheme. The criterion for\na structural ambiguity to emerge is the reception of at least two positive replies to a single searchHeadFor (or\nsearchModiﬁerFor) message by the initiator. The basic protocol already provides for the concurrent copying and\nfeature updates. In the example from Fig. 1, two alternative readings are parsed, one phrase actor holding the\nattachment to the verb (“sells”), the other holding that to the noun (“printer”). The crucial point about these\nambiguous syntactic structures is that they have conceptually diﬀerent representations in the domain knowledge\nbase. In the case of Fig. 1 verb attachment leads to the instantiation of the price slot of the corresponding\nSell action, while the noun attachment leads to the corresponding instantiation of the price slot of Printer.\nRobustness: Skipping Protocol. Skipping for robustnes purposes is a well known mechanism though limited\nin its reach (Lavie & Tomita, 1993). But in free word-order languages as German skipping is even vital for\nthe analysis of entirely well-formed structures, e.g., those involving scrambling or discontinuous constructions.\nFor brevity, we will base the following explanation on the robustness issue and refer the interested reader to\nNeuhaus & Br¨oker (1997). The incompleteness of linguistic and conceptual speciﬁcations is ubiquitous in real-\nworld applications and, therefore, requires mechanisms for a fail-soft parsing behavior. Fig. 2 illustrates a typical\nFigure 2: Skipping Mode\n“skipping” scenario. The currently active container addresses a searchHeadFor (or searchModiﬁerFor) message to\nits textually immediately preceding container actor. If both types of messages fail, the immediately preceding\ncontainer of the active container forwards these messages — in the canonical order — to its immediately\npreceding container actor. If any of these two message types succeeds after that mediation, a corresponding\n(discontinuous) dependency structure is built up. Furthermore, the skipped container is moved to the left of\nthe newly built container actor. Note that this behavior results in the reordering of the lexical items analyzed\nso far such that skipped containers are continuously moved to the left. As an example, consider the phrase\n“Zenon sells this printer” and let us further assume “totally” to be a grammatically unknown item which is\nfollowed by the occurrence of “over-priced” as the active container. Skipping yields a structural analysis for\n“Zenon sells this printer over-priced”, while “totally” is simply discarded from further consideration. This mode\nrequires an extension of the basic protocol in that searchHeadFor and searchModiﬁerFor messages are forwarded\nacross non-contiguous parts of the analysis when these messages do not yield a positive result for the requesting\nactor relative to the immediately adjacent container actor.\nBacktracking Protocol.\nBacktracking to which we still adhere in our model of constrained concurrency\naccounts for a state of the analysis where none of the aforementioned protocols have terminated successfully in\nany textually preceding container, i.e., several repeated skippings have occurred, until a linguistically plausible\nbarrier is encountered. In this case, backtracking takes place and messages are now directed to historically\nprevious containers, i.e., to containers holding fragments of the parse history. This is realized in terms of a\nprotocol extension by which searchHeadFor (or searchModiﬁerFor) messages, ﬁrst, are reissued to the textually\nimmediately preceding container actor which then forwards these messages to its historically previous container\nactor. This actor contains the head-centered results of the analysis of the left context prior to the structural\nextension held by the historical successor.1 Attachments for heads or modiﬁers are now checked referring to the\nhistorically preceding container and the active container as depicted in Fig. 3a.\nIf the valency constraints are met, a new phrase actor is formed (cf. Fig. 3b) necessarily yielding a discontinu-\nous analysis. A slightly modiﬁed protocol implements reanalysis, where the skipped items send reSearchHeadFor\n(or reSearchModiﬁerFor) messages to the new phrase actor, which forwards them directly to those word actors\nwhere the discontinuity occurs. As an example, consider the fragment “the customer bought the silver” (with\n“silver” in the noun reading, cf. Fig. 3a). This yields a perfect analysis which, however, cannot be further aug-\nmented when the word actor “notebook” asks for a possible attachment.2 Two intervening steps of reanalysis\n1 Any container which holds the modifying part of the structural analysis of the historical successor (in Fig. 3a this relates to\n“the” and “silver”) is deleted. Hence, this deletion renders the parser incomplete in spite of backtracking.\n2Being an arc-eager parsing system, a possible dependency relation will always be established. Hence, the adjective reading of\n“silver” will not be considered in the initial analysis.\nFigure 3: Backtracking Mode\n(cf. Fig. 3b and 3c) yield the ﬁnal structural conﬁguration depicted in Fig. 3d.\nPrediction Protocol. The depth-ﬁrst approach of the parser brings about a decision problem whenever a\nphrase cannot be integrated into (one of) the left-context analyses. Either, the left context and the current\nphrase are to be related by a word not yet read from the input and, thus, the analysis should proceed without\nan attachment.3 Or, depth-ﬁrst analysis was misguided and a backtrack should be invoked to revise a former\ndecision with respect to attachment information available by now.\nPrediction can be used to carry out a more informed selection between these alternatives. Words not yet read,\nbut required for a complete analysis, can be derived from the input analyzed so far, either top-down (predicting\na modiﬁer) or bottom-up (predicting a head). Both types of prediction are common in phrase-structure based\nparsers, e.g. Earley-style top-down prediction (Earley, 1970) or left-corner strategies with bottom-up prediction\n(Kay, 1986).\nSince dependency grammars, in general, do not employ non-lexical categories which can be\npredicted, so-called virtual words are constructed by the parser, which are later to be instantiated with lexical\ncontent as it becomes available when the analysis proceeds.\nWhenever an active phrase cannot attach itself to the left context, the head of this phrase may predict a\nvirtual word as tentative head of a new phrase under which it is subordinated. The virtual word is speciﬁed\nwith respect to its word class, morphosyntactic features, and order restrictions, but is left vacuous with respect\nto its lexeme and semantic speciﬁcation. In this way, a determiner immediately constructs an NP (cf. Fig. 4a),\nwhich can be attached to the left context and may incrementally incorporate additional attributive adjectives\nuntil the head noun is found (cf. Fig. 4b).4 The virtual word processes a searchPredictionFor protocol initiated\nby the next lexical item. The virtual word and this lexical item are merged iﬀthe lexical item is at least as\nspeciﬁc as the virtual word (concerning word class and features) and it is able to govern all modiﬁers of the\n3 This eﬀect occurs particularly often for verb-ﬁnal languages such as German.\n4 This procedure implements the notion of mother node constructing categories proposed by Hawkins (1994), which are a\ngeneralization of the notion head to all words which unambiguously determine their head. The linguistic puzzle about NP vs. DP\nis thus solved. In contrast to Hawkins, we also allow for multiple predictions.\nFigure 4: Predicting and merging a noun\nvirtual word (cf. Fig. 4c).\nThis last criterion may not always be met, although the prediction, in general, is correct. Consider the case\nof German verb-ﬁnal subclauses. A top-down prediction of the complementizer constructs a virtual ﬁnite verb\n(designated by\n), which may govern any number of NPs in the subclause (cf. Fig. 5a). If the verbal complex,\nhowever, consists of an inﬁnite full verb preceding a ﬁnite auxiliary, the modiﬁers of the virtual verb must be\ndistributed over two lexical items.5 An extension of the prediction protocol accounts for this case: A virtual\nword can be split if it may govern the lexical item and some modiﬁers can be transferred to the lexical item. In\nthis case, the lexical item is subordinated to a newly created virtual word (indicated by\nin Fig. 5b) governing\nthe remaining modiﬁers. Since order restrictions are available for virtual words, even non-projectivities can be\naccounted for by this scheme (cf. Fig. 5b).6\nAlthough prediction allows parsing to proceed incrementally and more informed (to the potential beneﬁt\nof increased eﬃciency), it engenders possible drawbacks: In underspeciﬁed contexts, a lot of false predictions\nmay arise and may dramatically increase the number of ambiguous analyses. Furthermore, the introduction of\nadditional operations (prediction, split, and merge) increases the search space of the parser. Part of the ﬁrst\nproblem is addressed by our extensive usage of the word class hierarchy. If a set of predictions contains all\nsubclasses of some word class W, only one virtual word of class W is created.\nText Phenomena. A particularly interesting feature of the performance grammar we propose is its capability\nto seamlessly integrate the sentence and text level of linguistic analysis.\nWe have already alluded to the\nnotoriously intricate interactions between syntactic criteria and semantic constraints at the phrasal and clausal\nlevel. The interaction is even more necessary at the text level of analysis as semantic interpretations have\nan immediate update eﬀect on the discourse representation structures to which text analysis procedures refer.\nTheir status and validity directly inﬂuence subsequent analyses at the sentence level, e.g., by supplying proper\nreferents for semantic checks when establishing new dependency relations. In addition, lacking recognition and\nreferential resolution of textual forms of pronominal or nominal anaphora (Strube & Hahn, 1995), textual ellipses\n(Hahn et al., 1996a) and metonymies (Markert & Hahn, 1997) leads to invalid or incohesive text knowledge\n5 We here assume the ﬁnite auxiliary to govern the subject (enforcing agreement), while the remaining complements are governed\nby the inﬁnite full verb.\n6 Non-projectivities often arise, e.g. due to the fronting of a non-subject relative pronoun. As indicated by the dashed line in\nFig. 5b and 5c, we employ additional projective relations to restrain ordering for discontinuities.\nFigure 5: Predicting and splitting a verb\nrepresentation structures. These not only yield invalid parsing results (at the methodological level) but also\npreclude proper text knowledge acquisition (at the level of system functionality). Hence, we stress the neat\nintegration of syntactic and semantic checks during the parsing process at the sentence and the text level. We\nnow turn to text grammar speciﬁcations concerned with anaphora resolution and their realization by a special\ntext parsing protocol.\nThe protocol which accounts for local text coherence analysis makes use of a special actor, the centering\nactor, which keeps a backward-looking center (Cb) and a preferentially ordered list of forward-looking centers\n(Cf) of the previous utterance (we here assume a functional approach (Strube & Hahn, 1996) to the well-known\ncentering model originating from Grosz et al. (1995)). These lists are accessed to establish proper referential\nlinks between an anaphoric expression in the current utterance and the valid antecedent in the preceding ones.\nNominal anaphora (cf. the occurrences of “the company” and “these printers” in Fig. 6) trigger a special\nsearchNomAntecedent message.\nWhen it reaches the Cf list, possible antecedents are accessed in the given\npreference order. If an antecedent and the anaphor fulﬁll certain grammatical and conceptual compatibility\nconstraints, an antecedentFound message is issued to the anaphor, and ﬁnally, the discourse referent of the\nantecedent replaces the one in the original anaphoric expression in order to establish local coherence. In case\nof successful anaphor resolution an anaphorSucceed message is sent from the resolved anaphor to the centering\nFigure 6: Anaphora Resolution Mode\nFigure 7: Sample Output of Text Parsing\nactor in order to remove the determined antecedent from the Cf list (this avoids illegal follow-up references).\nThe eﬀects of these changes at the level of text knowledge structures are depicted in Fig. 7, which contains the\nterminological representation structures for the sentences in Fig. 6.\n4\nPreliminary Evaluation\nAny text understanding system which is intended to meet the requirements discussed in Section 1 faces severe\nperformance problems. Given a set of strong heuristics, a computationally complete depth-ﬁrst parsing strategy\nusually will increase the parsing eﬃciency in the average case, i.e., for input that is in accordance with the\nparser’s preferences.\nFor the rest of the input further processing is necessary.\nThus, the worst case for a\ndepth-ﬁrst strategy applies to input which cannot be assigned any analysis at all (i.e., in cases of extra- or\nungrammaticality). Such a failure scenario leads to an exhaustive search of the parse space. Unfortunately, under\nrealistic conditions of real-world text input these cases occur quite often. Hence, by using a computationally\ncomplete depth-ﬁrst strategy one merely would trade space complexity for time complexity.\nTo maintain the potential for eﬃciency of depth-ﬁrst operation it is necessary to prevent the parser from\nexhaustive backtracking. In our approach this is achieved by two means. First, by restricting memoization of\nattachment candidates for backtracking (e.g., by retaining only the head portion of a newly built phrase, cf.\nfootnote 1). Second, by restricting the accessibility of attachment candidates for backtracking (e.g., by bounding\nthe forwarding of backtracking messages to linguistically plausible barriers such as punctuation actors). In\neﬀect, these restrictions render the parser computationally incomplete, since some input, though covered by the\ngrammar speciﬁcation, will not be correctly analyzed.\n4.1\nPerformance Aspects\nThe stipulated eﬃciency gain that results from deciding against completeness is empirically substantiated by a\ncomparison of our ParseTalk system, henceforth designated as PT, with a standard chart parser,7 abbreviated\nas CP. As the CP does not employ any robustness mechanisms (one might,e.g., incorporate those proposed by\nMellish (1989)) the current comparison had to be restricted to entirely grammatical sentences. We also do not\naccount for prediction mechanisms the necessity of which we argued for in Section 2. For the time being, an\nevaluation of the prediction mechanisms is still under way. Actually, the current comparison of the two parsers\nis based on a set of 41 sentences from our corpus (articles from computer magazines) that do not exhibit the\ntype of structure requiring prediction (cf. Fig. 5 and the example therein). For 40 of the test sentences8 the CP\nﬁnds all correct analyses but also those over-generated by the grammar. In combination, this leads to a ratio\nof 2.3 of found analyses to correct ones. The PT system (over-generating at a ratio of only 1.6) ﬁnds 36 correct\nanalyses, i.e., 90% of the analyses covered by the grammar (cf. the remark on ’near misses’ in Section 4.2). Our\npreliminary evaluation study rests on two measurements, viz. one considering concrete run-time data, the other\ncomparing the number of method calls.\nnumber of\nspeed-up factor\nsamples\nmin–max\naverage\n25\n1.1–4.2\n2.8\n10\n5.1–8.9\n6.9\n6\n10.9–54.8\n45.2\nTable 1: Ratio of run times of the CP and the PT system, chunked by speed-up.\nThe loss in completeness is compensated by a reduction in processing costs on the order of one magnitude\non the average. Since both systems use the identical dependency grammar and knowledge representation the\nimplementation of which rests on identical Smalltalk and LOOM/Common Lisp code, a run time comparison\nseems reasonable to some degree. For the test set the PT parser turned out to be about 17 times faster than the\nCP parser (per sentence speed-up averaged at over 10). Table 1 gives an overview of the speed-up distribution.\n25 short to medium long sentences were processed with a speed-up in a range from 1.1 to 4.2 times faster than\nthe chart parser averaging at 2.8. Another 10 longer and more complex sentences show the eﬀects of complexity\nreduction even more clearly, averaging at a speed-up of 6.9 (of a range from 5.1 to 8.9). One of the remaining\n6 very complex sentences is discussed below.\nAccordingly to these factors, the PT system spent nearly two hours (on a SPARCstation 10 with 64 MB of\nmain memory) processing the entire test set, while the CP parser took more than 24 hours. The exorbitant run\ntimes are largely a result of the (incremental) conceptual interpretation, though these computations are carried\nout by the LOOM system (Mac Gregor & Bates, 1987), still one of the fastest knowledge representation systems\ncurrently available (Heinsohn et al., 1994).\nWhile the chart parser is completely coded in Smalltalk, the PT system is implemented in Actalk (Briot,\n1989) — an extension of Smalltalk which simulates the asynchronous communication and concurrent execution of\nactors on sequential architectures. Thus, rather than exploiting parallelism, the PT parser currently suﬀers from\na scheduling overhead. A more thorough comparison abstracting from these implementational considerations can\n7 The active chart parser by Winograd (1983) was adapted to parsing a dependency grammar. No packing or structure sharing\ntechniques could be used since the analyses have continuously to be interpreted in conceptual terms. We just remark that the\npolynomial time complexity known from chart parsing of context-free grammars does not carry over to linguistically adequate\nversions of dependency grammars (Neuhaus & Br¨oker, 1997).\n8The problem caused by the single missing sentence is discussed in Section 4.2.\nFigure 8: Calls to syntaxCheck\nFigure 9: Calls to conceptCheck\nbe made at the level of method calls. We here consider the computationally expensive methods syntaxCheck\nand conceptCheck (cf. Section 2). Especially the latter consumes large computational resources, as mentioned\nabove, since for each syntactic interpretation variant a context has to be built in the KB system and its\nconceptual consistency must be checked continuously. The number of calls to these methods is given by the\nplots in Figs. 8 and 9. Sentences are ordered by increasing numbers of calls to syntaxCheck as executed by\nthe CP (this correlates fairly well with the syntactic complexity of the input). The values for sentences 39–41\nin Fig. 8 are left out in order to preserve a proper scaling of the ﬁgure for plotting (39: 14389, 40=41: 27089\nchecks). A reduction of the total numbers of syntactic as well as semantic checks by a factor of nine to ten\ncan be observed applying the strategies discussed for the PT system, i.e., the basic protocol plus skipping and\nbacktracking.\n4.2\nLinguistic Aspects\nThe well-known PP attachment ambiguities pose a high processing burden for any parsing system. At the\nsame time, PP adjuncts often convey crucial information from a conceptual point of view as in sentence 40:\nBei einer Blockgr¨oße, die kleiner als 32 KB ist, erreicht die Quantum-Festplatte beim sequentiellen Lesen einen\nDatendurchsatz von 1.100 KB/s bis 1.300 KB/s. [For a block size of less than 10 KB, the Quantum hard disk\ndrive reaches a data throughput of 1.100 KB/s to 1.300 KB/s for sequential reading]. Here, the chart parser\nconsiders all 16 globally ambiguous analyses stemming from ambiguous PP attachments.\nApart from the speed-up discussed above the PT parser behaves robust in the sense that it can gracefully\nhandle cases of underspeciﬁcation or ungrammaticality. For instance, sentence 36 (Im direkten Vergleich zur\nSeagate bietet sie f¨ur denselben Preis weniger Kapazit¨at. [In direct comparison to the Seagate drive, it (the\ntested drive) oﬀers less capacity for the same price.]) contains an unspeciﬁed word ’weniger’ (i.e. ’less’) such\nthat no complete and correct analysis could be produced. Still, the PT parser was able to ﬁnd a ’near miss’,\ni.e., a discontinuous analysis skipping just that word.\nA case where the PT parser failed to ﬁnd the correct analysis was sentence 39: Die Ger¨auschentwicklung der\nFestplatte ist deutlich h¨oher als die Ger¨auschentwicklung der Maxtor 7080A. [The drive’s noise level is clearly\nhigher than the noise level of the Maxtor 7080A]. When the adverb ’deutlich’ (i.e. ’clearly’) is processed it is\nimmediately attached to the matrix verb as an adjunct. Actually it should modify ’h¨oher’ (i.e. ’higher’), but\nas it is not mandatory no backtrack is initiated by the PT parser to ﬁnd the correct analysis.\n5\nConclusion\nThe incomplete depth-ﬁrst nature of our approach leads to a signiﬁcant speed-up of processing approximately\nin the order of one magnitude, which is gained at the risk of not ﬁnding a correct analysis at all. This lack of\ncompleteness resulted in the loss of about 10% of the parses in our experiments and correlates with fewer global\nambiguities. We expect to ﬁnd even more favorable results for the PT system when processing the complete\ncorpus, i.e., when processing material that requires prediction mechanisms.\nAcknowledgments. We would like to thank our colleagues in the CLIF group for fruitful discussions and instant sup-\nport. P. Neuhaus was supported by a grant from the interdisciplinary Graduate Program “Menschliche und maschinelle\nIntelligenz” (“Human und machine intelligence” at Freiburg University, N. Br¨oker was partially supported by a grants\nfrom DFG (Ha 2097/1-3).\nReferences\nAgha, G. (1990). The Structure and Semantics of Actor Languages. In J. W. de Bakker et al. (Eds.), Foundations of\nObject-Oriented Languages, pp. 1–59. Berlin: Springer.\nAllen, J. F. (1993). Natural language, knowledge representation, and logical form. In M. Bates & R. Weischedel (Eds.),\nChallenges in Natural Language Processing, Studies in Natural Language Processing, pp. 146–175. Cambridge, MA:\nCambridge University Press.\nBriot, J.-P. (1989). Actalk: A Testbed for Classifying and Designing Actor Languages in the Smalltalk-80 Environment.\nIn Proc. of the European Workshop on Object-Based Concurrent Computing. Nottingham, U.K., 10–14 Jul 1989,\npp. 109–129.\nCharniak, E. (1993). Statistical Language Learning. Cambridge, MA: MIT Press.\nChomsky, N. (1965). Aspects of the Theory of Syntax. Cambridge, MA: MIT Press.\nEarley, J. (1970). An Eﬃcient Context-Free Parsing Algorithm. Communications of the ACM, 13(2):94–102.\nGrosz, B. J., A. K. Joshi & S. Weinstein (1995). Centering: A Framework for Modeling the Local Coherence of Discourse.\nComputational Linguistics, 21(2):203–225.\nHahn, U., K. Markert & M. Strube (1996a). A conceptual reasoning approach to textual ellipsis. In ECAI ’96 — Proc.\nof the 12th European Conference on Artiﬁcial Intelligence, pp. 572–576, Budapest, Hungary, August 11-16, 1996.\nChichester etc.: J. Wiley.\nHahn, U., S. Schacht & N. Br¨oker (1994). Concurrent, Object-oriented Natural Language Parsing: The ParseTalk\nModel. International Journal of Human-Computer Studies, 41(1–2):179–222.\nHahn, U. & K. Schnattinger (1997). Deep knowledge discovery from natural language texts. In KDD – 97: Proc. of the\n3rd Intl. Conf. on Knowledge Discovery and Data Mining, Newport Beach, Calif., August 14–17, 1997.\nHahn, U., K. Schnattinger & M. Romacker (1996b). Automatic knowledge acquisition from medical texts. In Proc. of the\n1996 AMIA Annual Fall Symposium (formerly SCAMC). Beyond the Superhighway: Exploiting the Internet with\nMedical Informatics, pp. 383–387, Washington, D.C., October 26-30, 1996.\nHawkins, J. A. (1994).\nA Performance Theory of Order and Constituency.\nCambridge Studies in Linguistics\n73.\nCambridge: Cambridge University Press.\nHeinsohn, J., D. Kudenko, B. Nebel & H.-J. Proﬁtlich (1994). An Empirical Analysis of Terminological Representation\nSystems. Artiﬁcial Intelligence, 68(2):367–397.\nHuyck, C. R. & S. L. Lytinen (1993). Eﬃcient Heuristic Natural Language Parsing. In Proc. of the 11 th National Conf.\non Artiﬁcial Intelligence. Washington, D.C., 11–15 Jul 1993, pp. 386–391.\nJackendoﬀ, R. (1990). Semantic Structures. Cambridge, MA: MIT Press.\nKay, M. (1986). Algorithm Schemata and Data Structures in Syntactic Processing. In B. J. Grosz, K. Sparck Jones &\nB. L. Webber (Eds.), Readings in Natural Language Processing, pp. 35–70. Los Altos, CA: M. Kaufmann. Originally\npublished as a Xerox PARC technical report CSL-80-12, 1980.\nLavie, A. & M. Tomita (1993). GLR∗— An Eﬃcient Noise-Skipping Parsing Algorithm for Context Free Grammars. In\nProc. of the 3 rd Int. Workshop on Parsing Technology. Tilburg, NL & Durbuy, BE, 1993, pp. 123–134.\nMac Gregor, R. & R. Bates (1987). The LOOM Knowledge Representation Language. Technical Report RS-87-188:\nInformation Sciences Institute, University of Southern California.\nMarkert, K. & U. Hahn (1997). On the interaction of metonymies and anaphora. In IJCAI ’97 — Proc. of the 15th\nIntl. Joint Conference on Artiﬁcial Intelligence, Nagoya, Japan, August 23–29, 1997. San Francisco/CA: Morgan\nKaufmann.\nMellish, C. S. (1989). Some Chart-Based Techniques for Parsing Ill-Formed Input. In Proc. of the 27 th Annual Meeting\nof the Association for Computational Linguistics. 1989, pp. 102–109.\nNeuhaus, P. & N. Br¨oker (1997). The Complexity of Recognition of Linguistically Adequate Dependency Grammars.\nIn Proc. of the 35 th Annual Meeting of the Association for Computational Linguistics and the 8 th Conf. of the\nEuropean Chapter of the Association for Computational Linguistics. Madrid, ES, 7–12 Jul 1997.\nNeuhaus, P. & U. Hahn (1996). Restricted parallelism in object-oriented lexical parsing. In COLING ’96 — Proc. of the\n16th Intl. Conf. on Computational Linguistics, Vol. 1, pp. 502–507, Copenhagen, Denmark, August 5-9, 1996.\nShieber, S. M. (1986). An Introduction to Uniﬁcation-Based Approaches to Grammar. Lecture Notes 4. Stanford, CA:\nCSLI.\nStrube, M. & U. Hahn (1995). ParseTalk about Sentence- and Text-level Anaphora. In Proc. of the 7 th Conf. of the\nEuropean Chapter of the Association for Computational Linguistics. Dublin, Ireland, 27–31 Mar 1995, pp. 237–247.\nStrube, M. & U. Hahn (1996).\nFunctional centering.\nIn Proc. of the 34th Annual Meeting of the Association for\nComputational Linguistics, pp. 270–277, University of California at Santa Cruz, California, USA, 24-27 June 1996.\nSan Francisco/CA: Morgan Kaufmann.\nThibadeau, R., M. A. Just & P. A. Carpenter (1982). A Model of the Time Course and Content of Reading. Cognitive\nScience, 6:157–203.\nUszkoreit, H. (1991). Strategies for Adding Control Information to Declarative Grammars. In Proc. of the 29 th Annual\nMeeting of the Association for Computational Linguistics. Berkeley, CA, 18–21 Jun 1991, pp. 237–245.\nWinograd, T. (1983). Language as a Cognitive Process, Vol. 1, Syntax. Reading, MA: Addison-Wesley.\nWoods, W. A. & J. G. Schmolze (1992). The KL-ONE Family. Computers & Mathematics with Applications, 23(2–\n5):133–177.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1997-09-23",
  "updated": "1997-09-23"
}