{
  "id": "http://arxiv.org/abs/cmp-lg/9712001v1",
  "title": "Applying Explanation-based Learning to Control and Speeding-up Natural Language Generation",
  "authors": [
    "Guenter Neumann"
  ],
  "abstract": "This paper presents a method for the automatic extraction of subgrammars to\ncontrol and speeding-up natural language generation NLG. The method is based on\nexplanation-based learning (EBL). The main advantage for the proposed new\nmethod for NLG is that the complexity of the grammatical decision making\nprocess during NLG can be vastly reduced, because the EBL method supports the\nadaption of a NLG system to a particular use of a language.",
  "text": "arXiv:cmp-lg/9712001v1  8 Dec 1997\nApplying Explanation-based Learning to Control and Speeding-up\nNatural Language Generation\nG¨unter Neumann\nDFKI GmbH\nStuhlsatzenhausweg 3\n66123 Saarbr¨ucken, Germany\nneumann@dfki.uni-sb.de\nAbstract\nThis paper presents a method for the au-\ntomatic extraction of subgrammars to con-\ntrol and speeding-up natural language gen-\neration NLG. The method is based on\nexplanation-based learning EBL. The main\nadvantage for the proposed new method\nfor NLG is that the complexity of the\ngrammatical decision making process dur-\ning NLG can be vastly reduced, because\nthe EBL method supports the adaption of\na NLG system to a particular use of a lan-\nguage.\n1\nIntroduction\nIn\nrecent\nyears,\na\nMachine\nLearning\ntech-\nnique known as Explanation-based Learning EBL\n(Mitchell, Keller, and Kedar-Cabelli, 1986; van\nHarmelen and Bundy, 1988; Minton et al., 1989) has\nsuccessfully been applied to control and speeding-up\nnatural language parsing (Rayner, 1988; Samuelsson\nand Rayner, 1991; Neumann, 1994a; Samuelsson,\n1994; Srinivas and Joshi, 1995; Rayner and Carter,\n1996).\nThe core idea of EBL is to transform the\nderivations (or explanations) computed by a prob-\nlem solver (e.g., a parser) to some generalized and\ncompact forms, which can be used very eﬃciently\nfor solving similar problems in the future. EBL has\nprimarily been used for parsing to automatically spe-\ncialize a given source grammar to a speciﬁc domain.\nIn that case, EBL is used as a method for adapting a\ngeneral grammar and/or parser to the sub-language\ndeﬁned by a suitable training corpus (Rayner and\nCarter, 1996).\nA specialized grammar can be seen as describ-\ning a domain-speciﬁc set of prototypical construc-\ntions.\nTherefore, the EBL approach is also very\ninteresting for natural language generation (NLG).\nInformally, NLG is the production of a natural\nlanguage text from computer-internal representa-\ntion of information, where NLG can be seen as\na complex—potentially cascaded—decision making\nprocess. Commonly, a NLG system is decomposed\ninto two major components, viz. the strategic com-\nponent which decides ‘what to say’ and the tacti-\ncal component which decides ‘how to say’ the result\nof the strategic component. The input of the tacti-\ncal component is basically a semantic representation\ncomputed by the strategic component. Using a lexi-\ncon and a grammar, its main task is the computation\nof potentially all possible strings associated with a\nsemantic input. Now, in the same sense as EBL is\nused in parsing as a means to control the range of\npossible strings as well as their degree of ambigu-\nity, it can also be used for the tactical component\nto control the range of possible semantic input and\ntheir degree of paraphrases.\nIn this paper, we present a novel method for the\nautomatic extraction of subgrammars for the control\nand speeding-up of natural language generation. Its\nmain advantage for NLG is that the complexity of\nthe (linguistically oriented) decision making process\nduring natural language generation can be vastly re-\nduced, because the EBL method supports adaption\nof a NLG system to a particular language use. The\ncore properties of this new method are:\n• prototypical occuring grammatical construc-\ntions can automatically be extracted;\n• generation of these constructions is vastly sped\nup using simple but eﬃcient mechanisms;\n• the new method supports partial matching, in\nthe sense that new semantic input need not be\ncompletely covered by previously trained exam-\nples;\n• it can easily be integrated with recently de-\nveloped chart-based generators as described in,\ne.g., (Neumann, 1994b; Kay, 1996; Shemtov,\n1996).\nThe method has been completely implemented\nand tested with a broad-coverage HPSG-based\ngrammar for English (see sec. 5 for more details).\n2\nFoundations\nThe main focus of this paper is tactical generation,\ni.e., the mapping of structures (usually represent-\ning semantic information eventually decorated with\nsome functional features) to strings using a lexicon\nand a grammar. Thus stated, we view tactical gen-\neration as the inverse process of parsing. Informally,\nEBL can be considered as an intelligent storage unit\nof example-based generalized parts of the grammat-\nical search space determined via training by the tac-\ntical generator.1 Processing of similar new input is\nthen reduced to simple lookup and matching oper-\nations, which circumvent re-computation of this al-\nready known search space.\nWe concentrate on constraint-based grammar for-\nmalism following a sign-based approach consider-\ning linguistic objects (i.e., words and phrases) as\nutterance-meaning associations (Pollard and Sag,\n1994). Thus viewed, a grammar is a formal state-\nment of the relation between utterances in a natu-\nral language and representations of their meanings\nin some logical or other artiﬁcial language, where\nsuch representations are usually called logical forms\n(Shieber, 1993). The result of the tactical generator\nis a feature structure (or a set of such structures in\nthe case of multiple paraphrases) containing among\nothers the input logical form, the computed string,\nand a representation of the derivation.\nIn our current implementation we are using TDL,\na typed feature-based language and inference system\nfor constraint-based grammars (Krieger and Sch¨afer,\n1994). TDL allows the user to deﬁne hierarchically-\nordered types consisting of type and feature con-\nstraints. As shown later, a systematic use of type\ninformation leads to a very compact representation\nof the extracted data and supports an elegant but\neﬃcient generalization step.\nWe are adapting a “ﬂat” representation of log-\nical forms as described in (Kay, 1996; Copestake\net al., 1996). This is a minimally structured, but\ndescriptively adequate means to represent seman-\ntic information, which allows for various types of\nunder-/overspeciﬁcation, facilitates generation and\nthe speciﬁcation of semantic transfer equivalences\n1 In case a reversible grammar is used the parser can\neven be used for processing the training corpus.\nused for machine translation (Copestake et al., 1996;\nShemtov, 1996).2\nInformally, a ﬂat representation is obtained by\nthe use of extra variables which explicitly repre-\nsent the relationship between the entities of a logical\nform and scope information. In our current system\nwe are using the framework called minimal recur-\nsion semantics (MRS) described in (Copestake et\nal., 1996). Using their typed feature structure nota-\ntion ﬁgure 1 displays a possible MRS of the string\n“Sandy gives a chair to Kim” (abbreviated where\nconvenient).\nThe value of the feature liszt is actually treated\nlike a set, i.e., the relative order of the elements is\nimmaterial. The feature handel is used to repre-\nsent scope information, and index plays much the\nsame role as a lambda variable in conventional rep-\nresentations (for more details see (Copestake et al.,\n1996)).\n3\nOverview of the method\ngrammatical\nprocessor\nretrieve &\ninstantiate\nextract &\ngeneralize &\nindex\nsource\ngrammar\nsub\ngrammar\nsemantic\ninput\nresults\nLCB\nSGP\nAM\nTM\nFigure 3: A blueprint of the architecture.\nThe above ﬁgure displays the overall architecture\nof the EBL learning method. The right-hand part\nof the diagram shows the linguistic competence base\n(LCB) and the left the EBL-based subgrammar pro-\ncessing component (SGP).\nLCB corresponds to the tactical component of a\ngeneral natural language generation system NLG. In\nthis paper we assume that the strategic component\nof the NLG has already computed the MRS repre-\nsentation of the information of an underlying com-\nputer program. SGP consists of a training module\nTM, an application module AM, and the subgram-\n2 But note, our approach does not depend on a ﬂat\nrepresentation of logical forms.\nHowever, in the case\nof conventional representation form, the mechanisms for\nindexing the trained structures would require more com-\nplex abstract data types (see sec. 4 for more details).\n\n\nhandel h1\nindex\ne2\nliszt\n*\nSandyRel\nhhandel h4\ninst\nx5\ni\n,\nGiveRel\n\n\nhandel\nh1\nevent\ne2\nact\nx5\npreparg x6\nund\nx7\n\n, TempOver\nhhandel h1\nevent\ne2\ni\n,\nSome\n\n\nhandel h9\nbv\nx7\nrestr\nh10\nscope\nh11\n\n,\nChairRel\nhhandel h10\ninst\nx7\ni\n,\nTo\n\"handel h12\narg\nv13\nprep\nx6\n#\n, KimRel\nhhandel h14\ninst\nx6\ni+\n\n\nFigure 1: The MRS of the string “Sandy gives a chair to Kim”\n\"\nliszt\nD\nSandyRel[handel h4], GiveRel[handel h1], TempOver[handel h1], Some[handel h9],\nChairRel[handel h10], To[handel h12], KimRel[handel h14]\nE\n#\nFigure 2: The generalized MRS of the string “Sandy gives a chair to Kim”\nmar, automatically determined by TM and applied\nby AM.\nBrieﬂy, the ﬂow of control is as follows: During\nthe training phase of the system, a new logical form\nmrs is given as input to the LCB. After grammatical\nprocessing, the resulting feature structure fs(mrs)\n(i.e., a feature structure that contains among others\nthe input MRS, the computed string and a repre-\nsentation of the derivation tree) is passed to TM.\nTM extracts and generalizes the derivation tree of\nfs(mrs), which we call the template templ(mrs)\nof fs(mrs).\ntempl(mrs) is then stored in a deci-\nsion tree, where indices are computed from the MRS\nfound under the root of templ(mrs). During the ap-\nplication phase, a new semantic input mrs′ is used\nfor the retrieval of the decision tree. If a candidate\ntemplate can be found and successfully instantiated,\nthe resulting feature structure fs(mrs′) constitutes\nthe generation result of mrs′.\nThus described, the approach seems to facilitate\nonly exact retrieval and matching of a new seman-\ntic input. However, before we describe how partial\nmatching is realized, we will demonstrate in more de-\ntail the exact matching strategy using the example\nMRS shown in ﬁgure 1.\nTraining phase\nThe training module TM starts\nright after the resulting feature structure fs for the\ninput MRS mrs has been computed.\nIn the ﬁrst\nphase, TM extracts and generalizes the derivation\ntree of fs, called the template of fs. Each node of\nthe template contains the rule name used in the cor-\nresponding derivation step and a generalization of\nthe local MRS. A generalized MRS is the abstrac-\ntion of the liszt value of a MRS where each element\nonly contains the (lexical semantic) type and han-\ndel information (the handel information is used\nfor directing lexical choice (see below)).\nIn our example mrs, ﬁgure 2 displays the gener-\nalized MRS mrsg. For convenience, we will use the\nmore compact notation:\n{(SandyRel h4), (GiveRel h1),\n(TempOver h1), (Some h9),\n(ChairRel h10), (To h12), (KimRel h14)}\nUsing this notation, ﬁgure 4 (see next page) dis-\nplays the template templ(mrs) obtained from fs.\nNote that it memorizes not only the rule application\nstructure of a successful process but also the way the\ngrammar mutually relates the compositional parts of\nthe input MRS.\nIn the next step of the training module TM, the\ngeneralized MRS mrsg information of the root node\nof templ(mrs) is used for building up an index in\na decision tree. Remember that the relative order\nof the elements of a MRS is immaterial. For that\nreason, the elements of mrsg are alphabetically or-\ndered, so that we can treat it as a sequence when\nused as a new index in the decision tree.\nThe alphabetic ordering has two advantages.\nFirstly, we can store diﬀerent templates under a\ncommon preﬁx, which allows for eﬃcient storage and\nretrieval. Secondly, it allows for a simple eﬃcient\ntreatment of MRS as sets during the retrieval phase\nof the application phase.\nDetN\n{(To h12), (KimRel h14)}\nPrepNoModLe\n{(To h12)}\nProperLe\n{(KimRel h14)}\nSubjhD\nProperLe\n{(SandyRel h4)}\nHCompNc\nHCompNc\n{(GiveRel h1), (TempOver h1),\n(Some h9), (ChairRel h10)}\nMvTo+DitransLe\nDetN\n(ChairRel h10)}\n{(GiveRel h1),\n(TempOver h1)}\n{(Some h9),\nDetSgLe\n{(Some h9)}\nIntrNLe\n{(ChairRel h10)}\n(Some h9), (ChairRel h10), (To h12), (KimRel h14)}\n{(GiveRel h1), (TempOver h1)\n(Some h9), (ChairRel h10), (To h12), (KimRel h14)}\n{(SandyRel h4), (GiveRel h1), (TempOver h1),\nFigure 4: The template templ(mrs). Rule names\nare in bold.\nApplication phase\nThe application module AM\nbasically performs the following steps:\n1. Retrieval: For a new MRS mrs′ we ﬁrst con-\nstruct the alphabetically sorted generalized MRS\nmrs\n′\ng. mrs\n′\ng is then used as a path description\nfor traversing the decision tree. For reasons we\nwill explain soon, traversal is directed by type\nsubsumption. Traversal is successful if mrs\n′\ng has\nbeen completely processed and if the end node\nin the decision tree contains a template. Note\nthat because of the alphabetic ordering, the rel-\native order of the elements of new input mrs′ is\nimmaterial.\n2. Expansion: A successfully retrieved template\ntempl is expanded by deterministically applying\nthe rules denoted by the non-terminal elements\nfrom the top downwards in the order speciﬁed\nby templ. In some sense, expansion just re-plays\nthe derivation obtained in the past. This will\nresult in a grammatically fully expanded fea-\nture structure, where only lexical speciﬁc infor-\nmation is still missing. But note that through\nstructure sharing the terminal elements will al-\nready be constrained by syntactic information.3\n3 It is possible to perform the expansion step oﬀ-line\nas early as the training phase, in which case the applica-\ntion phase can be sped up, however at the price of more\nmemory being taken up.\n3. Lexical lookup: From each terminal element of\nthe unexpanded template templ the type and\nhandel information is used to select the cor-\nresponding element from the input MRS mrs′\n(note that in general the MRS elements of the\nmrs′ are much more constrained than their cor-\nresponding elements in the generalized MRS\nmrs\n′\ng). The chosen input MRS element is then\nused for performing lexical lookup, where lexi-\ncal elements are indexed by their relation name.\nIn general this will lead to a set of lexical can-\ndidates.\n4. Lexical instantiation: In the last step of the ap-\nplication phase, the set of selected lexical el-\nements is uniﬁed with the constraints of the\nterminal elements in the order speciﬁed by the\nterminal yield. We also call this step terminal-\nmatching.\nIn our current system terminal-\nmatching is performed from left to right. Since\nthe ordering of the terminal yield is given by the\ntemplate, it is also possible to follow other se-\nlection strategies, e.g., a semantic head-driven\nstrategy, which could lead to more eﬃcient\nterminal-matching, because the head element is\nsupposed to provide selectional restriction in-\nformation for its dependents.\nA template together with its corresponding index\ndescribes all sentences of the language that share\nthe same derivation and whose MRS are consistent\nwith that of the index. Furthermore, the index and\nthe MRS of a template together deﬁne a normaliza-\ntion for the permutation of the elements of a new\ninput MRS. The proposed EBL method guarantees\nsoundness because retaining and applying the orig-\ninal derivation in a template enforces the full con-\nstraints of the original grammar.\nAchieving more generality\nSo far, the applica-\ntion phase will only be able to re-use templates for\na semantic input which has the same semantic type\ninformation. However, it is possible to achieve more\ngenerality, if we apply a further abstraction step on\na generalized MRS. This is simply achieved by se-\nlecting a supertype of a MRS element instead of the\ngiven specialized type.\nThe type abstraction step is based on the stan-\ndard assumption that the word-speciﬁc lexical se-\nmantic types can be grouped into classes represent-\ning morpho-syntactic paradigms. These classes de-\nﬁne the upper bounds for the abstraction process. In\nour current system, these upper bounds are directly\nused as the supertypes to be considered during the\ntype abstraction step. More precisely, for each el-\nement x of a generalized MRS mrsg it is checked\nwhether its type Tx is subsumed by an upper bound\nTs (we assume disjoint sets). Only if this is the case,\nTs replaces Tx in mrsg.4 Applying this type abstrac-\ntion strategy on the MRS of ﬁgure 1, we obtain:\n{(Named h4), (ActUndPrep h1),\n(TempOver h1), (Some h9),\n(RegNom h10), (To h12), (Named h14)}\nDetN\n{(To h12), (Name h14)}\nPrepNoModLe\nProperLe\n{(To h12)}\n{(Name h14)}\nSubjhD\nProperLe\n{(Named h4)}\nHCompNc\nHCompNc\n{(ActUndPrep h1), (TempOver h1),\n(Some h9), (RegNom h10)}\nMvTo+DitransLe\nDetN\n(RegNom h10)}\n{(ActUndPrep h1),\n(TempOver h1)}\n{(Some h9),\nDetSgLe\n{(Some h9)}\nIntrNLe\n{(RegNom h10)}\n(Some h9), (RegNom h10), (To h12), (Named h14)}\n{(ActUndPrep h1), (TempOver h1)\n(Some h9), (RegNom h10), (To h12), (Named h14)}\n{(Named h4), (ActUndPrep h1), (TempOver h1),\nFigure 5: The more generalized derivation tree dtg\nof dt.\nwhere e.g., Named is the common supertype of\nSandyRel and KimRel, and ActUndPrep is the\nsupertype of GiveRel.\nFigure 5 shows the tem-\nplate templg obtained from fs using the more gen-\neral MRS information. Note, that the MRS of the\nroot node is used for building up an index in the\ndecision tree.\nNow, if retrieval of the decision tree is directed\nby type subsumption, the same template can be re-\ntrieved and potentially instantiated for a wider range\nof new MRS input, namely for those which are type\ncompatible wrt.\nsubsumption relation.\nThus, the\ntemplate templg can now be used to generate, e.g.,\nthe string “Kim gives a table to Peter”, as well as\nthe string “Noam donates a book to Peter”.\n4 Of course, if a very ﬁne-grained lexical seman-\ntic type hierarchy is deﬁned then a more careful selec-\ntion would be possible to obtained diﬀerent degrees of\ntype abstraction and to achieve a more domain-sensitive\ndetermination of the subgrammars.\nHowever, more\ncomplex type abstraction strategies are then needed\nwhich would be able to ﬁnd appropriate supertypes\nautomatically.\nHowever, it will not be able to generate a sentence\nlike “A man gives a book to Kim”, since the retrieval\nphase will already fail. In the next section, we will\nshow how to overcome even this kind of restriction.\n4\nPartial Matching\nThe core idea behind partial matching is that in case\nan exact match of an input MRS fails we want at\nleast as many subparts as possible to be instantiated.\nSince the instantiated template of a MRS subpart\ncorresponds to a phrasal sign, we also call it a phrasal\ntemplate. For example, assuming that the training\nphase has only to be performed for the example in\nﬁgure 1, then for the MRS of “A man gives a book to\nKim”, a partial match would generate the strings “a\nman” and “gives a book to Kim”.5 The instantiated\nphrasal templates are then combined by the tactical\ncomponent to produce larger units (if possible, see\nbelow).\nExtended training phase\nThe training module\nis adapted as follows:\nStarting from a template\ntempl obtained for the training example in the man-\nner described above, we extract recursively all pos-\nsible subtrees templs also called phrasal templates.\nNext, each phrasal template is inserted in the deci-\nsion tree in the way described above.\nIt is possible to direct the subtree extraction pro-\ncess with the application of ﬁlters, which are ap-\nplied to the whole remaining subtree in each recur-\nsive step. By using these ﬁlters it is possible to re-\nstrict the range of structural properties of candidate\nphrasal templates (e.g., extract only saturated NPs,\nor subtrees having at least two daughters, or sub-\ntrees which have no immediate recursive structures).\nThese ﬁlters serve the same means as the “chunking\ncriteria” described in (Rayner and Carter, 1996).\nDuring the training phase it is recognized for each\nphrasal template templs whether the decision tree\nalready contains a path pointing to a previously ex-\ntracted and already stored phrasal template templ\n′\ns,\nsuch that templs = templ\n′\ns. In that case, templs is\nnot inserted and the recursion stops at that branch.\nExtended application phase\nFor the applica-\ntion module, only the retrieval operation of the de-\ncision tree need be adapted.\nRemember that the input of the retrieval opera-\ntion is the sorted generalized MRS mrsg of the input\nMRS mrs. Therefore, mrsg can be handled like a\nsequence. The task of the retrieval operation in the\n5 If we would allow for an exhaustive partial match\n(see below) then the strings “a book” and “Kim” would\nadditionally be generated.\ncase of a partial match is now to potentially ﬁnd all\nsubsequences of mrsg which lead to a template.\nIn case of exact matching strategy, the decision\ntree must be visited only once for a new input. In\nthe case of partial matching, however, the decision\ntree describes only possible preﬁxes for a new input.\nHence, we have to recursively repeat retrieval of the\ndecision tree as long as the remaining suﬃx is not\nempty. In other words, the decision tree is now a\nﬁnite representation of an inﬁnite structure, because\nimplicitly, each endpoint of an index bears a pointer\nto the root of the decision tree.\nAssuming that the following template/index pairs\nhave been inserted into the decision tree: ⟨ab, t1⟩,\n⟨abcd, t2⟩, ⟨bcd, t3⟩.\nThen retrieval using the path\nabcd will return all three templates, retrieval using\naabbcd will return template t1 and t3, and abc will\nonly return t1.6\nInterleaving\nwith\nnormal\nprocessing\nOur\nEBL method can easily be integrated with normal\nprocessing, because each instantiated template can\nbe used directly as an already found sub-solution.\nIn case of an agenda-driven chart generator of the\nkind described in (Neumann, 1994a; Kay, 1996), an\ninstantiated template can be directly added as a\npassive edge to the generator’s agenda.\nIf passive\nedges with a wider span are given higher priority\nthan those with a smaller span, the tactical gener-\nator would try to combine the largest derivations\nbefore smaller ones, i.e., it would prefer those struc-\ntures determined by EBL.\n5\nImplementation\nThe EBL method just described has been fully im-\nplemented and tested with a broad coverage HPSG-\nbased English grammar including more than 2000\nfully speciﬁed lexical entries.7 The TDL grammar\nformalism is very powerful, supporting distributed\ndisjunction, full negation, as well as full boolean type\nlogic.\nIn our current system, an eﬃcient chart-based\nbidirectional parser is used for performing the train-\ning phase.\nDuring training, the user can interac-\ntively select which of the parser’s readings should\nbe considered by the EBL module. In this way the\nuser can control which sort of structural ambigui-\nties should be avoided because they are known to\n6 It is possible to parameterize our system to per-\nform an exhaustive or a non-exhaustive strategy. In the\nnon-exhaustive mode, the longest matching preﬁxes are\npreferred.\n7This grammar has been developed at CSLI, Stan-\nford, and kindly be provided to the author.\ncause misunderstandings. For interleaving the EBL\napplication phase with normal processing a ﬁrst pro-\ntotype of a chart generator has been implemented\nusing the same grammar as used for parsing.\nFirst tests has been carried out using a small test\nset of 179 sentences. Currently, a parser is used for\nprocessing the test set during training. Generation\nof the extracted templates is performed solely by\nthe EBL application phase (i.e., we did not consid-\nered integration of EBL and chart generation). The\napplication phase is very eﬃcient. The average pro-\ncessing time for indexing and instantiation of a sen-\ntence level template (determined through parsing) of\nan input MRS is approximately one second.8 Com-\npared to parsing the corresponding string the factor\nof speed up is between 10 to 20. A closer look to\nthe four basic EBL-generation steps: indexing, in-\nstantiation, lexical lookup, and terminal matching\nshowed that the latter is the most expensive one (up\nto 70% of computing time). The main reasons are\nthat 1.) lexical lookup often returns several lexical\nreadings for an MRS element (which introduces lex-\nical non-determinism) and 2.) the lexical elements\nintroduce most of the disjunctive constraints which\nmakes uniﬁcation very complex. Currently, termi-\nnal matching is performed left to right. However,\nwe hope to increase the eﬃciency of this step by us-\ning head-oriented strategies, since this might help to\nre-solve disjunctive constraints as early as possible.\n6\nDiscussion\nThe only other approach I am aware of which\nalso considers EBL for NLG is (Samuelsson, 1995a;\nSamuelsson, 1995b).\nHowever, he focuses on the\ncompilation of a logic grammar using LR-compiling\ntechniques, where EBL-related methods are used to\noptimize the compiled LR tables, in order to avoid\nspurious non-determinisms during normal genera-\ntion. He considers neither the extraction of a spe-\ncialized grammar for supporting controlled language\ngeneration, nor strong integration with the normal\ngenerator.\nHowever, these properties are very important for\nachieving high applicability.\nAutomatic grammar\nextraction is worthwhile because it can be used to\nsupport the deﬁnition of a controlled domain-speciﬁc\nlanguage use on the basis of training with a gen-\neral source grammar.\nFurthermore, in case exact\nmatching is requested only the application module\nis needed for processing the subgrammar. In case\n8 EBL-based generation of all possible templates of\nan input MRS is less than 2 seconds.\nThe tests have\nbeen performed using a Sun UltraSparc.\nof normal processing, our EBL method serves as a\nspeed-up mechanism for those structures which have\n“actually been used or uttered”. However, complete-\nness is preserved.\nWe view generation systems which are based on\n“canned text” and linguistically-based systems sim-\nply as two endpoints of a contiguous scale of possible\nsystem architectures (see also (Dale et al., 1994)).\nThus viewed, our approach is directed towards the\nautomatic creation of application-speciﬁc generation\nsystems.\n7\nConclusion and Future Directions\nWe have presented a method of automatic extrac-\ntion of subgrammars for controlling and speeding up\nnatural language generation (NLG). The method is\nbased on explanation-based learning (EBL), which\nhas already been successfully applied for parsing.\nWe showed how the method can be used to train\na system to a speciﬁc use of grammatical and lexical\nusage.\nWe already have implemented a similar EBL\nmethod for parsing, which supports on-line learn-\ning as well as statistical-based management of ex-\ntracted data. In the future we plan to combine EBL-\nbased generation and parsing to one uniform EBL\napproach usable for high-level performance strate-\ngies which are based on a strict interleaving of pars-\ning and generation (cf. (Neumann and van Noord,\n1994; Neumann, 1994a)).\n8\nAcknowledgement\nThe research underlying this paper was supported\nby a research grant from the German Bundesmin-\nisterium\nf¨ur\nBildung,\nWissenschaft,\nForschung\nund Technologie (BMB+F) to the DFKI project\nparadime FKZ ITW 9704.\nI would like to thank the HPSG people from CSLI,\nStanford for their kind support and for providing the\nHPSG-based English grammar. In particular I want\nto thank Dan Flickinger and Ivan Sag. Many thanks\nalso to Walter Kasper for fruitful discussions.\nReferences\n[Copestake et al.1996] Copestake, A., D. Flickinger,\nR. Malouf, S. Riehemann, and I. Sag.\n1996.\nTranslation using minimal recursion semantics.\nIn Proceedings, 6th International Conference on\nTheoretical and Methodological Issues in Machine\nTranslation.\n[Dale et al.1994] Dale, R., W. Finkler, R. Kittredge,\nN. Lenke, G. Neumann, C. Peters, and M. Stede.\n1994.\nReport from working group 2:\nLexi-\ncalization and architecture.\nIn W. Hoeppner,\nH. Horacek, and J. Moore, editors, Principles of\nNatural Language Generation, Dagstuhl-Seminar-\nReport; 93. Schloß Dagstuhl, Saarland, Germany,\nEurope, pages 30–39.\n[Kay1996] Kay, M. 1996. Chart generation. In 34th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, Santa Cruz, Ca.\n[Krieger and Sch¨afer1994] Krieger, Hans-Ulrich and\nUlrich Sch¨afer.\n1994. TDL—a type description\nlanguage for constraint-based grammars. In Pro-\nceedings of the 15th International Conference on\nComputational Linguistics, COLING-94, pages\n893–899.\n[Minton et al.1989] Minton,\nS.,\nJ. G. Carbonell,\nC. A. Knoblock, D. R.Kuokka, O. Etzioni, and\nY.Gi. 1989. Explanation-based learning: A prob-\nlem solving perspective.\nArtiﬁcial Intelligence,\n40:63–115.\n[Mitchell, Keller, and Kedar-Cabelli1986]\nMitchell, T., R. Keller, and S. Kedar-Cabelli.\n1986.\nExplanation-based generalization: a uni-\nfying view. Machine Learning, 1:47–80.\n[Neumann1994a] Neumann, G. 1994a. Application\nof explanation-based learning for eﬃcient process-\ning of constraint based grammars. In Proceedings\nof the Tenth IEEE Conference on Artiﬁcial Intel-\nligence for Applications, pages 208–215, San An-\ntonio, Texas, March.\n[Neumann1994b] Neumann, G. 1994b. A Uniform\nComputational Model for Natural Language Pars-\ning and Generation. Ph.D. thesis, Universit¨at des\nSaarlandes, Germany, Europe, November.\n[Neumann and van Noord1994] Neumann,\nG.\nand\nG. van Noord.\n1994.\nReversibility and self-\nmonitoring in natural language generation.\nIn\nTomek Strzalkowski, editor, Reversible Grammar\nin Natural Language Processing. Kluwer, pages\n59–96.\n[Pollard and Sag1994] Pollard, C. and I. M. Sag.\n1994.\nHead-Driven Phrase Structure Grammar.\nCenter for the Study of Language and Informa-\ntion Stanford.\n[Rayner1988] Rayner,\nM.\n1988.\nApplying\nexplanation-based generalization to natural lan-\nguage processing. In Proceedings of the Interna-\ntional Conference on Fifth Generation Computer\nSystems, Tokyo.\n[Rayner and Carter1996] Rayner, M. and D. Carter.\n1996. Fast parsing using pruning and grammar\nspecialization. In 34th Annual Meeting of the As-\nsociation for Computational Linguistics, Morris-\ntown, New Jersey.\n[Samuelsson1994] Samuelsson,\nC.\n1994.\nFast\nNatural-Language\nParsing\nUsing\nExplanation-\nBased Learning. Ph.D. thesis, Swedish Institute\nof Computer Science, Kista, Sweden, Europe.\n[Samuelsson1995a] Samuelsson, C.\n1995a. An eﬃ-\ncient algorithm for surface generation. In Proceed-\nings of the 14th International Joint Conference on\nArtiﬁcial Intelligence, pages 1414–1419, Montreal,\nCanada.\n[Samuelsson1995b] Samuelsson, C. 1995b. Example-\nbased optimization of surface-generation tables.\nIn Proceedings of Recent Advances in Natural Lan-\nguage Processing, Velingrad, Bulgaria, Europe.\n[Samuelsson and Rayner1991] Samuelsson,\nC.\nand\nM. Rayner.\n1991.\nQuantitative evaluation of\nexplanation-based learning as an optimization\ntool for a large-scale natural language system. In\nIJCAI-91, pages 609–615, Sydney, Australia.\n[Shemtov1996] Shemtov, H.\n1996.\nGeneration of\nParaphrases from Ambiguous Logical Forms. In\nProceedings of the 16th International Conference\non Computational Linguistics (COLING), pages\n919–924, Kopenhagen, Denmark, Europe.\n[Shieber1993] Shieber, S. M. 1993. The problem of\nlogical-form equivalence. Computational Linguis-\ntics, 19:179–190.\n[Srinivas and Joshi1995] Srinivas, B. and A. Joshi.\n1995.\nSome novel applications of explanation-\nbased\nlearning\nto\nparsing\nlexicalized\ntree-\nadjoining grammars.\nIn 33th Annual Meeting\nof the Association for Computational Linguistics,\nCambridge, MA.\n[van Harmelen and Bundy1988] van\nHarmelen,\nF.\nand A. Bundy. 1988. Explanation-based general-\nization=partial evaluation. Artiﬁcial Intelligence,\n36:401–412.\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1997-12-08",
  "updated": "1997-12-08"
}