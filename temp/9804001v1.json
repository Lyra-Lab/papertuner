{
  "id": "http://arxiv.org/abs/cmp-lg/9804001v1",
  "title": "Graph Interpolation Grammars: a Rule-based Approach to the Incremental Parsing of Natural Languages",
  "authors": [
    "John Larcheveque"
  ],
  "abstract": "Graph Interpolation Grammars are a declarative formalism with an operational\nsemantics. Their goal is to emulate salient features of the human parser, and\nnotably incrementality. The parsing process defined by GIGs incrementally\nbuilds a syntactic representation of a sentence as each successive lexeme is\nread. A GIG rule specifies a set of parse configurations that trigger its\napplication and an operation to perform on a matching configuration. Rules are\npartly context-sensitive; furthermore, they are reversible, meaning that their\noperations can be undone, which allows the parsing process to be\nnondeterministic. These two factors confer enough expressive power to the\nformalism for parsing natural languages.",
  "text": "arXiv:cmp-lg/9804001v1  2 Apr 1998\nGraph Interpolation Grammars:\na rule-based approach to the incremental parsing of\nnatural languages\nJohn Larchevˆeque\nMarch 1998\nAbstract\nGraph Interpolation Grammars are a declarative formalism with an operational\nsemantics. Their goal is to emulate salient features of the human parser, and notably\nincrementality. The parsing process deﬁned by GIGs incrementally builds a syntactic\nrepresentation of a sentence as each successive lexeme is read. A GIG rule speciﬁes\na set of parse conﬁgurations that trigger its application and an operation to perform\non a matching conﬁguration.\nRules are partly context-sensitive; furthermore, they\nare reversible, meaning that their operations can be undone, which allows the parsing\nprocess to be nondeterministic. These two factors confer enough expressive power to\nthe formalism for parsing natural languages.\n1\nIntroduction\n1.1\nCharacteristics and rationale\nA graph interpolation grammar is a grammar formalism with an operational semantics.\nA rule in a graph interpolation grammar speciﬁes not only syntactic relations but also\nan elementary parsing operation.\nRules are lexicalized in the sense that each rule describes the combinatory properties\nof a lexical item. Parsing a sentence consists in matching each lexeme in the input string\nwith a rule in the grammar and applying this rule to the current parse representation.\nGIG-driven parsing was designed with incrementality and ﬂexibility in mind, so as\nto emulate some features of the human parsing capability, in particular incremental\nprocessing, error tolerance, and handling of complex word orders.\nIn a complete model of discourse understanding, incremental parsing would be\nshown to work in tandem with some form of composition between partial semantic\nrepresentations. It is to be thought that the collaboration between syntax and seman-\ntics in natural discourse understanding is fairly close, and that backtracking in the\nparser is frequently initiated by a clash found between semantic features. This report,\nhowever, will not attempt to give even the roughest idea of what semantic represen-\ntations should look like and will therefore exclusively focus on syntactic phenomena,\nsometimes at the cost of simplifying the phenomena at hand.\n1\n1.2\nPlan of the report\nThe ﬁrst two sections give a fairly complete presentation of the concepts and processes\ninvolved in parsing with a Graph Interpolation Grammar. The syntactic structures\ngenerated when parsing with a GIG are described in Section 2, while the grammar\nrules and the parsing process are described in Section 3.\nThe next few sections apply the formalism to selected problems. Section 4 com-\npares the handling of a simple expression language using a Context Free Grammar and\na Graph Interpolation Grammar, and Section 5 analyzes Dutch Cross-Serial Depen-\ndencies.\nFinally, the conclusion indicates topics for further research and related works.\n2\nSyntactic structures\n2.1\nPhrases\nA phrase is made up of a head and its complements. It will be represented by a graph\nwith a root and one labelled edge from the root to each nonroot node.\nThe root represents the head of the phrase, the other nodes represent its comple-\nments; and the edge labels represent grammatical functions.\n2.1.1\nOrdering relations\nIn addition, nodes in a phrase graph are connected through ordering edges. In the\nsimplest case, the eﬀect of ordering edges is to align graph nodes from left to right.\nFor example, the sentence\nShe gave me an apple.\ncontains a phrase the head of which is a ditransitive verb with three complements, the\nsubject, the indirect object, and the direct object. This phrase can be represented by\nthe following graph (in which V for verb and NP for noun phrase).\nV\nNP\nNP\nobject\nindirectObject\nprecedes\nNP\nprecedes\nsubject\nprecedes\nFigure 1: A phrase graph\nSince this is a case in which a total left-to-right alignment of nodes can be deﬁned,\nordering edges can be omitted and the ordering represented by the relative positions\nof the nodes in the diagram, as in Figure 2. Explicit orderings will not be used in this\nreport, for a strict total ordering of phrase constituents exists in all examples given.\nAt any rate, the possibility exists of specifying sophisticated order constraints as long\nas these constraints are local to a phrase.\n2\nV\nNP\nNP\nobject\nindirectObject\nNP\nsubject\nFigure 2: The same phrase graph with implicit ordering\n2.1.2\nPhrase head\nThe criteria for distinguishing a head in a phrase are both syntactic and semantic.\nFrom a syntactic point of view, the head determines the presence and functions of\nits complements.\nFrom a semantic point of view, the head acts as a predicate of\nits complements, adding semantic features to them, while the complements generally\ncontribute to rooting these features into a speciﬁc situation.\nOn the basis of these criteria, an attributive adjective forms the head of a phrase\nmade up of an adjective and a noun, such as red leaf. Indeed, the occurrence of the\nadjective implies the occurrence of a single noun, whereas the presence of the noun\ndoes not require or limit the number of attributive adjectives. On the other hand,\nthough evidence for this type of criterion is not so easy to adduce, the adjective adds\nsemantic features to the noun, as is shown in particular by its capacity to function as a\npredicate in a clause. Figure 3 illustrates a phrase headed by an attributive adjective.\n(Adj stands for adjective.)\nAdj\nN\nsubject\nFigure 3: A modifying phrase\nConsidering the adjective as the head of such a phrase can be counterintuitive\ninsofar as the phrase in question functions as a noun. It is, however, necessary to\ndistinguish the internal structure of a phrase —as described by its phrase graph—\nfrom its function with respect to other phrases. In order to bring out this distinction,\na structure that interrelates phrase graphs must be deﬁned. The next section deﬁnes\na structure of this type, the parse graph.\n2.2\nParse graphs\nA parse graph is a structure in which phrase graphs are related through parent-of edges.\nConsider for example the phrase good old days. It contains the phrase old days,\nwhich has old as its head, but functions as a noun and is itself modiﬁed by the adjective\ngood in the larger phrase good old days. The ’containment’ relation just outlined can be\nrendered by drawing a parent-of edge between the noun node of the enclosing phrase\nand the head of the embedded phrase.\nGraphically, this can be represented as in\nFigure 4.\nFrom now on, in parse graph representations, vertical edges will be assumed to be\ndirected downward and represent parent-of edges.\n3\nAdj\nN\nsubject\nAdj\nN\nsubject\nparent−of\nFigure 4: A parse graph\nOn the other hand, a parent-of edge can also connect a phrase node to a lexical\nitem. A complete parse graph for a phrase or sentence can thus be deﬁned as a parse\ngraph with a root node and such that every path leads to a lexical item. These facts\ngive rise to the following principle and deﬁnition.\nPrinciple 1 A lexical node can occur in a parse graph as the destination of a parent-of\nedge.\nDeﬁnition 1 A complete parse graph is a parse graph with a root node, i.e. a node\nfrom which every node is reachable, and such that every path leads to a lexical node.\nFigure 5 shows the complete parse graph for the phrase the good old days. (Det\nstands for determiner.)\nAdj\nN\nsubject\nAdj\nN\nsubject\nDet\nthe\ngood\nold\ndays\ndeterminer\nNP\nN\nFigure 5: A complete parse graph\nSuch a graph can be mapped to a tree in such a way that members of a phrase graph,\nincluding the head, are mapped to sibling nodes. Although the graph contains more\ninformation and is better adapted to incremental construction that its tree counterpart,\nit will often prove useful to apply tree terminology to parse graphs. For example, the\nterms descendant, ancestor, or frontier are to be understood with respect to the tree\ncounterpart of a parse graph. Likewise, a dangling node is to be understood as a non-\nlexical node that occupies a terminal position in the tree counterpart of an incomplete\nparse graph. For further reference, here is the deﬁnition that justiﬁes this terminology.\n4\nDeﬁnition 2 The summary tree of a parse graph is a connected graph that contains\nthe same set of nodes as the parse graph and such that each path of the parse graph\nconsisting of either a parent-of edge or a parent-of edge followed by a functional edge\nis mapped to a parent-of edge of the summary tree.\nAdj\nN\nAdj\nN\nDet\nthe\ngood\nold\ndays\nNP\nN\nFigure 6: Summary tree for the parse graph on Figure 5\n2.3\nSubtyping in node labels\nThe classiﬁcation of lexical items can be made according to several possible criteria.\nFor example, the can be categorized as an article and thus be distinguished from this.\nOn the other hand, both of the and this are deﬁnite determiners and, as such, can be\nusefully grouped under one category. This suggests that lexical categories are usefully\nviewed as sets of syntactic properties, such as “determiner”, “article”, “deﬁnite”, etc.\nFurthermore, some of these properties are visible in ancestors of lexical items in a parse\ngraph. For example, the presence of a deﬁnite determiner confers to a Noun Phrase\nthe status of a deﬁnite description. A simple way of modeling this consists in labelling\neach node of a parse graph with a set of property names rather than an atomic name.\nThus, the label of a node dominating the is a set containing the names “determiner”,\n“article”, and “deﬁnite”, and the label of an NP determined by the, this, a genitive\nNP, or any other deﬁnite determiner, is a set containing the name “deﬁnite”.\nFor simplicity, most of the graphs and rules given as examples have atomic node\nlabels, even when some node types could be shown to subsume or share properties\nwith other symbols. However, Section 4.2 shows how multi-valued node labels can\ncontribute to the expressive power of a grammar.\n3\nBuilding parse graphs\nParse graphs, as just deﬁned, can be incrementally built by scanning an input ﬂow\nof lexemes and performing a building step as each lexeme is read. A building step is\nformally described by a composition rule, which adds to the graph under construction\na subgraph associated with the current lexeme.\n5\n3.1\nForm of a composition rule\nSuppose that (i) the phrase being parsed is a robin, (ii) somehow (see Section 3.5),\nthe parse graph on Figure 7 was built on encountering the determiner a, and (iii) the\ncurrent lexeme is robin.\nN\nNP\nDet\ndeterminer\na\nFigure 7: An NP context\nThe presence of the dangling —i.e. childless— noun node to the right of the previous\nlexeme, namely a, materializes the anticipation of a noun. Singular common nouns are\ngenerally inserted in just such a context, and so we can deﬁne a rule for the lexeme\nrobin that stipulates (i) that the linear successor of the previous lexeme (i.e. a) in the\nparse graph should be a dangling noun node, (ii) that the new subgraph to integrate\nconsists of a noun node dominating the lexeme robin, and (iii) that this new subgraph\nis to be substituted for the dangling noun node. (A graphic representation of this rule\nis given on Figure 8, in section 3.2.)\nMore generally, let the context be the parse graph obtained by parsing the input\nstring up to the current lexeme, a composition rule consists of:\n1. a pattern to match against the context, known as the context pattern, and\n2. a subgraph to integrate into the context, known as the addendum.\nOr, more formally:\nDeﬁnition 3 A composition rule is a pair made up of a context pattern and an ad-\ndendum.\nThis deﬁnition in turn relies on the deﬁnitions of a context pattern and an adden-\ndum. The following deﬁnition of a context pattern is somewhat simpliﬁed and will be\nenriched later (Deﬁnition 11, Section 3.10).\nDeﬁnition 4 A context pattern is a parse graph with one distinguished node called\nthe context anchor and an indication of whether the context anchor is an ancestor or\nimmediate linear successor of the previous lexeme.\nTo proceed in depth-ﬁrst order, here is the deﬁnition that explains the term linear\nsuccessor.\nDeﬁnition 5 When all phrase constituents in a parse graph are strictly ordered from\nleft to right, a linear order over the parse graph is given by the left-to-right ordering of\nnodes in the frontier of the summary tree (Deﬁnition 2, Section 2.2). It is therefore a\npartial ordering of parse graph nodes, under which only terminal nodes of the summary\ntree are comparable.\n6\nThe context pattern indicates what the parse graph obtained so far must look like\nin order for the rule to be applicable. A context matches a context pattern if it is a\nsupergraph of the pattern and it contains a node whose location and label match the\nanchor speciﬁcations.\nSection 3.7.2, on free word order, examines the consequences of having several\npossible linearizations of the parse graph. This has computational implications but\ndoes not impact the above deﬁnitions in any essential way.\nDeﬁnition 6 An addendum is a parse graph with one or two distinguished nodes called\naddendum anchors and exactly one lexical node.\nA rule is found applicable when the current lexeme matches the addendum lexeme\nand the context matches the context pattern. The conjunction of context and current\nlexeme forms a parse conﬁguration.\nDeﬁnition 7 A parse conﬁguration is a pair made up of a context and a current\nlexeme.\nBeside a parse conﬁguration, what a rule speciﬁes is an operation to modify this\nconﬁguration.\nThis operation, called an interpolation, is speciﬁed by means of the\ncontext anchor and the addendum.\nWhen the addendum contains a single anchor, such as the noun node in our ex-\nample, the interpolation is performed by substituting the addendum anchor for the\ncontext anchor. This particular type of interpolation will sometimes be called an in-\nsertion, by opposition to a proper interpolation, which is characterized by the presence\nof two distinct anchors in the addendum.\nWhen the addendum contains two anchors, they delimit a path that is substituted\nfor the context anchor. Since a single anchor can be viewed as a path of length 0, this\ncase subsumes insertion.\nNo formal deﬁnition of interpolation will be attempted at present. Let it simply be\nclear that interpolation involves in fact two operations:\n1. the substitution of a path of the addendum for the context anchor,\n2. the disjoint union of the addendum with the context.\nThese notions will be treated in more detail in Section 3.7.1.\n3.2\nInsertion example\nAn example of insertion is the example given informally above. The rule can be repre-\nsented as on Figure 8.\nIn a graphic representation of a composition rule, the context pattern and the\naddendum appear on either side of an implies ( ⇒) sign, with the addendum on the\nright; and the anchoring speciﬁcations are materialized by placing a mark next to each\nanchor. This mark is an asterisk (∗) next to an addendum anchor. Next to the context\nanchor, it is a double down arrow (⇓) or a left arrow (←) according as the context\nanchor is an ancestor or an immediate successor of the previous lexeme. The notion\nof immediate successor is deﬁned with respect to the linear order indicated through\n7\nN\nrobin\n* N\nFigure 8: An insertion rule\nthe ordering edges present in addenda. Its applicability to free word-order languages\nis discussed in Section 3.7.2.\nApplying the insertion rule on Figure 8 to the parse graph on Figure 7 results in\nthe parse graph on Figure 9.\nrobin\nN\nNP\nDet\ndeterminer\na\nFigure 9: Result of applying the above insertion rule\n3.3\nProper interpolation example\nFigure 10 illustrates path-to-node anchoring.\nThe composition rule interpolates a\nprepositional phrase into a noun phrase to produce a qualiﬁed noun phrase as in a\nbird on a tree. (PP stands for Prepositional Phrase and Prep for preposition.)\n*\nNP\nPrep\nPP\nobject\nsubject\nNP\non\nNP\n* NP\nFigure 10: A proper interpolation rule\nThe sign next to the context anchor indicates that it is to be found above the\nprevious lexeme rather than immediately after it. The diagram on Figure 11 shows the\ncontext to which this rule could apply and the resulting context after its application.\nAn application diagram follows the following pattern.\ncontext + addendum ⇒new context\n8\n*\nNP\nPrep\nPP\nsubject\non\nNP\n* NP\nDet\nN\na\nbird\n+\nPrep\nPP\nNP\non\nNP\nNP\nN\nbird\nobject\nNP\ndet\nDet\na\ndet\nsubject\nobject\nFigure 11: An application of the preceding rule\n3.4\nAnchor matching\nThe preceding examples have shown that the context anchor can be an ancestor or an\nimmediate linear successor of the previous lexeme. The ﬁrst case occurs typically with\na proper interpolation, and the second with an insertion. This is, of course, just the\ntypical case; Section 3.7.2 gives general criteria for locating the context anchor.\n3.4.1\nMatching an immediate successor anchor\nNo attempt to deal with free word order will be made until Section 3.7.2. Until then,\nwe can rely on a strict left-to-right ordering of all phrase constituents. Therefore, the\nprevious lexeme always has a unique immediate linear successor, which is its immediate\nsuccessor in the frontier of the summary tree (Deﬁnition 2, Section 2.2).\n3.4.2\nMatching an ancestor anchor\nAncestor positions are also deﬁned with respect to the summary tree of the parse graph\n(Deﬁnition 2, Section 2.2). On the other hand, for a node to match an ancestor anchor\nthere is an additional condition beside location and label match. It can be stated as\nfollows.\nPrinciple 2 A context anchor in ancestor position can only be matched by the root of\na complete parse graph (see Deﬁnition 1, Section 2.2).\nFor example, an NP cannot be postmodiﬁed if it is incomplete, i.e. if the frontier\nof its summary tree contains nonlexical nodes.\n3.4.3\nIncidence of node subtyping on anchor matching\nIf node labels are sets of symbols, label matching requires that all symbols attached to\nthe context anchor be present in the node to match. For example, a context pattern\ncould contain the information that the anchor is a deﬁnite determiner, and any node\nthat has at least the type atoms “deﬁnite” and “determiner” will satisfy label matching.\nIn other words, for a node to match a context anchor, its type must be a subtype\nof the type of the context anchor. On the other hand, this means that the formulation\nof a rule, and notably the addendum, must involve a special device to mention the\n9\nfull type of the node that matched the anchor, so as not to lose information. (Indeed,\na rule is allowed to change the type of the anchor, as explained in Section 3.5 and\nillustrated on Figure 13.) Accordingly, the special symbol & can be used to assign to\nan addendum anchor the exact type of the node matching the context anchor.\nAn example of a grammar that uses subtyping is given in Section 4.2.\n3.5\nParser initialization\nTo begin with, there is no context or, rather, the context consists of a single node with\na generic label that subsumes all phrases with which an utterance is likely to start1. No\nlexeme has been integrated to the context yet; but, for technical purposes, the initial\nnode can be considered to “follow the previous lexeme”. In other words, a rule with\nan immediate-successor (←) mark next to the context anchor will match the initial\ncontext.\nSuppose a sentence begins with the phrase a robin; then, the initial lexeme, a, can\nbe integrated using the following insertion rule.\n*\nNP\nN\nNP\nDet\ndeterminer\na\nFigure 12: Insertion rule for a\nThis rule simply means that a can be considered as the beginning of a Noun Phrase.\nIt does not say that a could be the beginning of a sentence. Yet, most utterances are\nsentences; so it would seem that this rule is generally inappropriate. It is not, however,\nif one considers that the deﬁnition of an interpolation that was (informally) given in\nSection 3.1 does not require the two ends of an interpolation path to bear the same\nlabel. And, indeed, should a phrase like a robin be followed with a verb, such as ﬂew,\nthe rule shown on Figure 13 can apply to make the root of the parse graph an S-node.\n(Dir stands for “directional phrase”.)\nNP\nsubject\n* NP\n* S\nV\nflew\nobject\nDir\nFigure 13: Interpolation rule for ﬂew\n1Label subsumption is discussed in Section 2.3.\n10\n3.6\nBacktracking\nMatching a rule often involves making a prediction on the continuation of the utterance.\nFor example, the rule on Figure 13 predicts that a directional phrase follows the verb\nﬂew. When several predictions are possible, as will often be the case, a trial-and-error\napproach is adopted.\nA prediction conﬂict appears when several rules are available for a given context\nand current lexeme (i.e. a given parse conﬁguration). It is supposed that the rules\nassociated with each lexeme are totally ordered by priority, and each rule is tried in\npriority order until a successful parse is obtained2.\nFor example, when the current lexeme is ﬂew and an NP node dominates the\nprevious lexeme, there could be two rules, with higher priority on the rule on Figure 13,\nand lower priority on the following rule, which makes ﬂew an intransitive verb.\nNP\nsubject\n* NP\n* S\nV\nflew\nFigure 14: An alternative rule for ﬂew\nGiven these two rules, the sentence a robin ﬂew swiftly would ﬁrst be mapped to the\nparse graph on Figure 15; then, on realizing that, by the end of the sentence, the parse\ngraph is still incomplete, one would have to reconsider the choices made. Supposing\nthere is a single analysis available for swiftly, the parser would then backtrack on ﬂew\nand try the second rule, which would eventually lead to a successful parse.\nS\nV\nflew\nrobin\nN\nNP\nDet\na\ndet\nV\nAdv\nswiftly\nsubj\nsubj\nobject\nDir\nFigure 15: Graph built before backtracking\nBacktracking involves undoing some of the rules applied so far.\nThis supposes\nthat track is kept of the list of rules applied, and, more importantly, that each rule is\nreversible. As a matter of fact, rules, as deﬁned, are reversible (owing to Principle 4,\n2If the selection process parallels a similar process in the human mind, a simple hypothesis would correlate\nrule priority with statistical eﬃciency; alternately, or complementarily, there could be structural criteria that\ncorrelated priority with simplicity or canonicity.\n11\nSection 3.7.1). In order to undo the eﬀect of a rule, the addendum is to be removed and\nthe interpolation path replaced back by the context anchor. If rules were not reversible,\nthen, it would be necessary to keep track of at least the last few parse conﬁgurations.\nOf course, backtracking does not always take place after reading the whole input\nstring. It takes place whenever no further rule can be triggered and the parse graph is\nincomplete.\nFor another example, consider the sentence he gave trouble to us and suppose that\nthe highest-priority rule for gave is the following.\n(The edge label iObj stands for\n“indirect object”.)\nNP\n* NP\n* S\nV\ngave\nsubj\niObj\nNP\nNP\nobj\nFigure 16: Default rule for gave\nThen, on encountering to, no rule will match the context (in which an object NP\nis expected), thus leading the parser to backtrack on trouble, then gave, and select the\nfollowing rule, which will eventually lead to a successful parse. (A to-Prep is a category\nthat contains only the preposition to —and unto in archaic dialects.)\nNP\n* NP\n* S\nV\ngave\nsubj\niObj\nNP\nobj\nto−PP\nto−Prep\nNP\nobj\nFigure 17: Alternate rule for gave\n3.7\nInterpolation and anchor matching\nThis section revisits concepts introduced in Section 3.1, giving more precise formula-\ntions and adding details that were initially overlooked.\n3.7.1\nAddendum incorporation\nIn all the examples given so far, an insertion consists in replacing a dangling node\nwith a node with a child and a proper interpolation consists in interpolating a path\nwhose ﬁrst edge is a parent-of edge, so that the destination of the interpolation path\ndominates its source.\n12\nWith interpolations of this type, merging an addendum into the context is a fairly\nstraightforward process, which has not been explicated so far, but only illustrated\nthrough examples.\nHowever, the concept of interpolation allows more ﬂexibility than has been hitherto\nsuggested, and, in order to consider further varieties of interpolation, it is important\nto bring out the graph invariants implicitly used and preserved during addendum in-\ncorporation.\nInvariant 1 In a phrase graph, there cannot be distinct edges with the same label.\nInvariant 2 In a parse graph, any node except the root has exactly one incoming edge.\nInvariant 3 In a parse graph, there cannot be more than one parent-of edge from any\nnode.\nAccording to invariant 2, an interpolation path can be deﬁned as the unique oriented\npath that connects two addendum anchors. With respect to this path, there is a source\nanchor and a destination anchor.\nInvariant 2 also implies that the source anchor\ninherits the incoming edge of the context anchor as a result of an interpolation.\nFurthermore, when one looks at the way the outgoing edges of the context anchor\nare distributed between the addendum anchors of an interpolation, invariant 3 can be\nseen to prevail in that the parent-of edge, in the examples seen so far, is transmitted\nto the destination anchor. The underlying principle can be stated as follows.\nPrinciple 3 All edges of the context anchor are inherited by the source of the in-\nterpolation path except for edges present in the addendum, which are shifted to the\ndestination of the interpolation path.\nPrinciple 3 itself presupposes that the addendum and the context form disjoint\ngraphs. This principle is well worth spelling out, for it allows rules to be easily reversible\nin the sense of Section 3.6.\nPrinciple 4 In a composition rule, the sets of nodes of the addendum and the context\nhave an empty intersection.\nIndeed, if the graph union that is speciﬁed by a rule is a disjoint union, then, no\ninformation beyond the list of rules applied is necessary to undo the eﬀect of these\nrules.\nOn the other hand, Principle 3 presupposes that, if the context anchor is a head,\nthen, the addendum anchors must both be capable of inheriting functional edges, i.e.\nbe phrase heads as well. Conversely, if the context anchor is not a head, substituting\na head for it would create a second head within one phrase, which is ruled out by\ndeﬁnition. These considerations can be summarized by the following principle.\nPrinciple 5 The head status of the context anchor must be matched by the addendum\nanchors.\nThis principle does not directly govern addendum incorporation, for it has to be\nbuilt into the grammar rules. It is especially useful as a prerequisite of Principle 3.\n13\nNonstandard interpolation\nTo see how Principle 3 applies to a nonstandard\ncase of proper interpolation, i.e. a case in which the interpolation path does not start\nwith a parent-of edge, consider the rule on Figure 18. (The box represents an empty\ncategory, or gap; the symbol that-Cnj informally denotes a subcategory whose main\nrepresentative is the conjunction that.)\n*\n*\nS\nV\nNP\nobj\nNP\nV\nthought\nthat−Cl\nthat−Cnj\nobj\nV\nsubj\ndet\nFigure 18: A nonstandard case of proper interpolation\nAlthough the context anchor, which is a verb, is a dangling node, an empty ob-\nject NP (i.e. an object gap) has been predicted, typically owing to the presence of\na preceding object pronoun, i.e. a relative or interrogative pronoun, or a topicalized\nobject NP. The occurrence of a verb that governs a sentence rather an object NP\npushes the predicted object gap further. Since the object gap does not appear in the\naddendum (owing to Principle 4), its position in the graph after the rule has applied is\nnot speciﬁed explicitly, but can be inferred from Principle 3. Indeed, since the object\nedge from the context anchor is redundant with the object edge that emanates from the\nsource anchor of the addendum, it is inherited by the destination anchor. Therefore,\nafter addendum incorporation, the context will contain the following subgraph.\nS\nNP\nobj\nNP\nV\nthought\nthat−Cl\nthat−Cnj\nobj\nV\nsubj\ndet\nFigure 19: Subgraph obtained after addendum incorporation\nNonstandard insertion\nA standard insertion substitutes a node with an outgoing\nparent-of edge for a dangling node. But it is also conceivable to substitute a node with\narbitrary nonredundant outgoing edges for a node with descendants.\nA practical use of nonstandard interpolation could be the handling of optional\ncomplements. For example, supposing the rule that handles the by-complement of a\n14\npassive were the one represented on FigurepassiveAgent.ﬁg, there would be no necessity\nto anticipate the presence of a by-complement on encountering a passive, and so no\nbacktracking would be involved to handle its optionality3.\nobj\n*\nNP\npassive−V\npassive−V\nagent\nby−PP\nby−Prep\nby\nFigure 20: Rule for the by-complement of a passive\n3.7.2\nAnchor matching\nThe principles assumed so far for matching the context anchor have to be stated ex-\nplicitly and possibly extended; this is necessary, on the one hand, to predict the anchor\nposition for any arbitrary interpolation, including a nonstandard one, and, on the other\nhand, to cater for variations in word order.\nSection 3.4 speciﬁed two possible positions for the context anchor, as ancestor or\nimmediate linear successor of the previous lexeme. But no principle to ﬁnd out whether\na given interpolation should take place at an ancestor or successor position has yet been\nstated. As a matter of fact, a simple principle underlies the examples given so far.\nPrinciple 6 If the addendum lexeme linearly follows either endpoint of the interpola-\ntion path, the context anchor is an ancestor of the previous lexeme. In all other cases,\nthe context anchor is the immediate linear successor of the previous lexeme.\nThis principle is analogous to Principle 2 in that it prevents the occurrence of\ndangling nodes among the linear predecessors of the current lexeme. So it is a practical\nconsequence of a more general principle.\nPrinciple 7 A valid interpolation cannot leave or create dangling nodes that linearly\nprecede the lexeme it integrates.\nAnother consequence of Principle 7 on the form of rules is the following.\nPrinciple 8 An addendum cannot contain dangling nodes that linearly precede the\nanchor that inherits the parent-of edge of the context anchor.\n3The directional complement of the verb ﬂy, used to illustrate backtracking in Section 3.6, could be\nhandled in this way, unless there were good reasons to distinguish the two constructions from the point of\nview of lexical semantics.\n15\nUnder strict word order, the form of rules can be controlled through Principles 8\nand 6 to enforce Principle 7. When alternative orderings are allowed, the matching\nprocess itself may have to take this principle into account, for rules may have to admit\naddenda that may violate Principle 7 or 8 under conditions that can only be checked\ndynamically.\nTo see Principle 6 in application, compare the rule on Figure 21, which represents\na premodiﬁcation, to the rule on Figure 22, which represents a postmodiﬁcation.\nsubj\nN\n* N\n* N\nred\nAdj\nFigure 21: Premodiﬁcation by the adjective red\n*\n*\nV\nV\nAdv\nswiftly\nsubj\nV\nFigure 22: Postmodiﬁcation by the adverb swiftly\nIt is now apparent that word order plays a key role in anchor choice during rule\ndesign and in anchor matching during rule application. So it is interesting to see what\nextensions to the principles just brought out are necessary in order to accommodate\ngreater freedom in word order than has been presupposed up till now.\nComplex phrase ordering\nUnder a strict ordering of phrase nodes, a parse graph\ncan be linearized by considering the frontier of the summary tree (Deﬁnition 2, Sec-\ntion 2.2).\nThen, in a given parse conﬁguration, the previous lexeme has a unique\nimmediate successor. However, when several possible orders are allowed, there may be\nmore than one linear successor to the previous lexeme.\nConsequently, several context nodes may be found to match the context anchor of\nan applicable rule. This introduces a degree of indeterminacy which is compounded\nwith the indeterminacy resulting from rule conﬂicts (i.e.\nthe availability of several\nmatching rules for a given lexeme).\nNot only does this type of indeterminacy arise in the presence of free word order, but\nit is also liable to arise in a ﬁxed-order framework if several nodes along a parent-of path\nare allowed to bear the same label. Indeed, this situation could result in several nodes\n16\nmatching an ancestor anchor; and a principle that ruled it out would be no more than\nan ad hoc feature that reduced the formalism’s expressive power without reducing its\nindeterminacy in any essential way. Therefore, indeterminacy due to anchor matching\nhas to be handled somehow.\nOnce again, a trial-and-error approach seems to be the natural path to follow. The\nmechanism used to solve rule conﬂicts can be applied here too, provided competing\nanchor matches are somehow ordered by priority.\nAs far as ancestor conﬂicts are\nconcerned, proximity to the previous lexeme (i.e. depth) seems to correlate simply and\nnaturally with priority. As far as successor conﬂict is concerned, however, it would\nseem that no natural solution is available unless, even when the placement of nodes in\na phrase graph is relatively free, there is an underlying preferred order.\nIn addition to the possibility of ﬁnding multiple anchor matches, free word order also\ncomplicates the process of identifying context anchors, for context nodes do not stand in\na simple topological relation with respect to the previous lexeme. From an algorithmic\npoint of view, the solution seems to consist in updating the set of successors of the\nprevious lexeme after each interpolation, as described in Procedure 1, Appendix A.\n3.8\nCoreference links\nIncremental left-to-right parsing, as outlined, forms phrases exclusively from adjacent\nlexemes. However, immediate syntactic relations between distant constituents do exist\nin natural languages. Interrogative and relative clauses in English provide such exam-\nples. For example, in the following sentence, the pronoun who somehow functions as\nthe subject of to win, although these two constituents occupy distant positions in the\ninput stream.\nWho do you want to win?\nThis diﬃculty will be solved through a now classic device in syntactic theory, namely\ncoindexing of phrases. This consists in linking coreferential phrases together, typically\nby assigning to them equal referent indices. In the above example, the interrogative\npronoun who and the empty subject of to win (whose presence is concretized by the\nunacceptability of Who d’you wanna win? as a spoken realization) are coindexed.\nIn fact, in order to reconcile the fact that the parsing model under discussion can\nonly establish immediate syntactic relations between neighbouring constituents and\nthe fact that various ordering constraints or rhetorical phenomena may separate phrase\nconstituents, we will rely on coreference links. A coreference link is somewhat more\ngeneral than NP coindexing is that it may apply to any part of speech rather than just\nnoun phrases.\nNodes related by a coreference links have just one counterpart in the semantic\ninterpretation to be derived from the parse graph. This means that all members of a\ncoreference chain denote the same object in the universe of discourse. Syntactically,\nthis entails that members of a coreference chain of NPs share gender and number\ncharacteristics, and members of a coreference chain of verbs share complements. (For\nexample, one member will have a subject, another member no subject but a direct\nobject, and a third an indirect object, thus forming three instances of a single verb\nwith subject, object, and indirect object.)\n17\nMany phenomena in natural languages suggest that the human parser prohibits\nto a large extent the splitting of a phrase across distant lexemes but uses coindexing\nto handle the sharing of constituents between phrases and rhetorical disruptions to\nstandard phrasal order.\nSuch disruptions are typically attenuated by the use of resumptive pronouns in\nspoken French, as in the following example.\nIl est bizarre, ton chapeau.\nIt is weird, [is] your hat.\nHere, the main verb ﬁnds its subject where the parser expects it, but this subject\nis only a pronoun coreferenced with the “real” subject, which, for intonational eﬀect,\nis relegated to a secondary tone unit so as to allow full emphasis on the predicate.\nWhen no overt pronoun shows the existence of a coreference chain, it is sometimes\nnecessary to postulate the existence of empty lexemes, as in the interrogative clause\ncited above.\nIn Section 5, an analysis of Dutch cross serial dependencies will be attempted in\nterms of covert coreference chains.\nThe introduction of coreference links gives rise to the following deﬁnitions.\nDeﬁnition 8 An augmented parse graph is a parse graph augmented with coreference\nlinks.\nDeﬁnition 9 A coreference link is an undirected edge that relates two nodes standing\nin an equivalence relation. Links that can be inferred, such as the link from any node\nto itself, do not need to appear in the representation of an augmented parse graph.\nThe equivalence relations represented in an augmented parse graph are coreferentiality\nrelations.\nDeﬁnition 10 A coreferentiality is an equivalence relation over parse graph nodes\nbearing a speciﬁed label.\nCoreferentialities found useful are NP coreferentiality and\nverb coreferentiality.\nThis deﬁnes a graph hierarchy, with phrase graphs at the bottom, and augmented\nparse graphs at the top.\n3.9\nMultiple interpolation\nCoreference links make it meaningful to deﬁne parallel interpolations operating simul-\ntaneously on several context anchors. Multiple interpolation seems useful only when\nthe context anchors are located with respect to a coreference chain. Figure 23 shows\nan example of a double interpolation.\nThis example is purely formal, but concrete examples will be given in Section 5\non Dutch Cross-Serial Dependencies. Nonetheless, it illustrates an important feature\nof multiple interpolation; namely, the fact that the rule, albeit multiple, adds a single\nlexeme to the parse graph. Typically, it anticipates a Y -gap coreferential with this\n18\n*\nX\nY\nY\ny\nX\nf\nZ\n*\nY\nZ\nFigure 23: A double interpolation\nlexeme. Gaps do not count as lexemes in so far as they are not physically present in\nthe input string.\nFurthermore, the formulation of multiple interpolation makes it necessary to deﬁne\na third possible location for the context anchor, namely linear successor, denoted by a\ndouble left arrow (⇐), as opposed to immediate linear successor, denoted by a simple\nleft arrow (←).\n3.10\nImplicit reﬂexive-transitive closure in patterns\nThe deﬁnition of coreferentiality as an equivalence relation (Deﬁnition 10) yields some-\nwhat unsuspected power to the occurrence of coreferential links in context patterns.\nTo see that, consider the example of adjectival deﬁnite descriptions in English, as\nfound in the wealthy, the inﬂuential, etc. Leaving semantic features aside, one impor-\ntant syntactic requirement for forming such NPs is that the determiner be deﬁnite.\nThis is captured by the rule on Figure 24. (Def stands for “deﬁnite determiner”.)\nsubj\nN\n* N\n* N\nAdj\ndet\nDef\nwealthy\nFigure 24: Restrictive rule for adjectival deﬁnite descriptions\nCoreference links are created because the structure being built is a modiﬁcation\nstructure, not because of the presence of a noun gap4. The modiﬁcation examples\ngiven so far did not contain coreference links simply because extended parse graphs\nhad not been deﬁned.\nThis rule requires that the noun node that is to be replaced by an adjective and\nan empty noun be immediately preceded by a deﬁnite determiner. Therefore, it will\n4This “gap” does not play a pronominal role, but rather represents a semantically empty noun. A precise\nnotation would include the name Pro in the label of “real” gaps, such as those used in Section 5.\n19\nnot apply to a noun which is itself premodiﬁed by an adjective, as in the idle rich.\nOn the other hand, one cannot devise a rule covering speciﬁcally a string of the form\nDef Adj Adj, for the premodiﬁcation can be iterated, as in the outrageous idle rich.\nConsider therefore the rule on Figure 25.\nsubj\n* N\n* N\nAdj\ndet\nDef\nwealthy\nN\nN\nFigure 25: Comprehensive rule for adjectival deﬁnite descriptions\nThis rule allows the context anchor to be separated from the NP head by any\nnumber of premodiﬁcation stages, including zero. For example, the last interpolation\nto be applied when parsing the outrageous idle rich can be represented by the diagram\non 265.\nAdj\nN\nAdj\nN\nthe\nNP\nN\ndet\nsubj\nsubj\noutrageous\nidle\nDef\nAdj\nN\nAdj\nthe\nNP\nN\ndet\nsubj\nsubj\noutrageous\nidle\nDef\nN\nN\nAdj\nsubj\nrich\nFigure 26: Last interpolation performed when parsing the outrageous idle rich\nThe existence of rules in which an anchor is related to the rest of the context by a\ncoreference link leads to the following deﬁnition of a context pattern.\nDeﬁnition 11 A context pattern is a subgraph of an augmented parse graph with one\nor several distinguished nodes called anchors. Anchor location with respect to the previ-\nous lexeme takes one of the three values ancestor, immediate successor, and successor.\n5This diagram represents a derivation step, as explained in Section 4.1.2.\n20\n4\nA simple expression grammar\nTo show the speciﬁcities of Graph Interpolation Grammars, even in the absence of\ncontext-sensitivity in rules, this section applies them to the classical problem of a simple\narithmetic expression language with two precedence levels and left associativity.\n4.1\nDeriving an GIG from a CFG\nThis section will derive as simply as possible a Graph Interpolation Grammar from the\nContext Free Grammar given in Figure 27. This kind of derivation could conceivably\nbe performed automatically in order to beneﬁt from the incrementality of GIG-driven\nparsing for existing context-free grammars. For example, in a language-based editor,\nthe incremental building of parse representations as each token is read could be used\nto implement syntax-sensitive editing functions.\nE\n→\nE + T\nE\n→\nT\nT\n→\nT * F\nT\n→\nF\nF\n→\n( E )\nF\n→\n0\nF\n→\n1\nFigure 27: Context-Free Grammar for a simple expression language\n4.1.1\nNumbers\nMoving from a CFG to a GIG involves lexicalization. This means that every rule is\nto be matched by a lexeme and contains a representation of the contexts in which this\nlexeme is likely to occur. As far as numbers are concerned, they can function either as\nfactors, terms, or expressions. Therefore, each number will have three associated rules.\nFigure 28 shows the three rules associated with the number 0.\nT\nF\n0\nF\n0\n0\nE\nT\nF\nE\n*\nT\n*\nF\n*\nFigure 28: GIG rules associated with the lexeme 0\n21\n4.1.2\nSum\nA sum is identiﬁed on scanning the lexeme +. By representing a sum as an interpolation\ninto an E, the fact that + is the lowest-priority operator is captured. Accordingly, the\nrule for + is as represented on Figure 29.\nT\nE\nE\n*\nSum\n+\nop1\nop2\n* E\nFigure 29: Rule to be matched on encountering a +\nSince the addendum lexeme follows the destination of the interpolation path, i.e.\nthe ﬁrst operand of the sum, the context anchor is stipulated to be an ancestor of the\nprevious lexeme (Principle 6).\nThe fact that the second operand is a term (T) rather than an expression (E)\nforces left associativity. This can be veriﬁed by unwinding the GIG derivation for the\nstring 0 + 0 + 0. This derivation, up to the insertion of the last zero, is represented on\nFigure 30.\nE\nE\nT\nF\n0\nE\nSum\n+\nop1\nop2\nE\nT\nF\n0\nT\nSum\n+\nop1\nop2\nE\nT\nF\n0\nT\nE\nSum\n+\nop1\nop2\nE\nT\nF\n0\nT\nSum\n+\nop1\nop2\nE\nT\nE\nF\n0\nF\n0\nFigure 30: The derivation for 0 + 0+\nA step in a GIG derivation consists in the marking of a context anchor in the\nresult of the preceding step, and a representation of the parse graph as modiﬁed by\nthe interpolation applied.\n4.1.3\nProduct\nThe rule for a product is entirely analogous to the rule for a sum, except that it is\nallowed to apply to a term rather than an expression. This fact gives it precedence\n22\nover sum, for essentially the same reason as in the original context-free grammar.\nFigure 31 shows the rule for the product operator, and Figure 32 shows the deriva-\ntion for 1+1∗0 up to the insertion of the lexeme 0. It illustrates the relative priorities\nof sum and product.\nop1\nop2\nT\n* T\n* T\nProduct\nF\n*\nFigure 31: The rule for ∗\nE\nE\nT\nF\nE\nSum\n+\nop1\nop2\nE\nT\nF\nT\nSum\n+\nop1\nop2\nE\n1\n1\nT\nF\n1\nE\nT\nop1\nop2\nProduct\n*\nSum\n+\nop1\nop2\nE\nT\nF\n1\nE\nT\nT\nF\nF\n1\nF\n1\nFigure 32: The derivation for 1 + 1∗\n4.1.4\nParentheses\nA rule for a left parenthesis is an insertion that announces an expression and a right\nparenthesis in a context where an expression, a term, or a factor could be expected.\nAs a right parenthesis is not supposed to occur unless it has been announced by a left\nparenthesis, its rule is a simple lexical insertion. Accordingly, only the rules for the left\nparenthesis are shown on Figure 33.\nNote that nonterminals RPar and Rpar are necessary according to principle 1,\nSection 2.2.\nApplying these rules to the parsing of (1 + 1) ∗0, one obtains the derivation whose\nmain steps are shown on Figure 34.\n23\nE\nT\n* F\nF\nE\nT\nF\nE\n*\nT\n*\nRPar\nLPar\n(\nstart\nend\nE\nRPar\nLPar\n(\nstart\nend\nF\nE\nRPar\nLPar\n(\nstart\nend\nFigure 33: The rules for the left parenthesis\n4.2\nA more idiomatic approach\nThe existence of three rules to insert a number or a left parenthesis is the translation\nof a CFG idiom for handling precedence. The native GIG device for solving this type\nof problem relies on node subtyping.\nFor the expression language considered, we can have tree type atoms, N (for num-\nber), P (for product), and S (for sum). They form two hierarchies that meet at the\nuniversal type N,P,S, as shown on Figure 35.\nIf we ignore preterminal labels (such as Sum or LPar), an N,P,S matches any\nanchor; so it is an expression in a broad sense. It occurs typically as the initial parse\ngraph and between parentheses. An N,P is any expression except a sum, so it occurs\nas the right operand of a sum to enforce left association. Likewise, an N is exclusively\na number (or a parenthesized expression), such as occurs as the right operand of a\nproduct. As far as the right-hand branch of the hierarchy is concerned, a P,S can\noccur as the left operand of a product or sum, while an S can occur only as the left\noperand of a sum. These enforce the relative precedence of sum and product.\n4.2.1\nNumbers\nThere is a single rule for inserting a number. It requires the atom N in the context\nanchor and does not modify its type. The symbol & that is used as the label of the\naddendum anchor indicates that this label is inherited as is from the node matching\nthe context anchor. The rule for zero is represented on ﬁgure 36.\n4.2.2\nSum\nThe rule for the operator + is represented on Figure 37.\nA sum can be interpolated only at a node whose type contains the type atom S. To\nguard against the interpolation of a product on top of a sum, the type S is assigned\nto the root of the addendum. To guard against right association of additions, the type\nN,P is assigned to the right operand. The left operand cannot be the object of further\ninterpolations and simply inherits the type of the node matching the context anchor.\nFigure 38 derives 0 + 0 + 0 up to the last insertion to show that left associativity is\nenforced.\n24\nT\nF\nE\nE\nRPar\nLPar\n(\nstart\nend\nE\nT\nF\nE\nT\nF\n1\nE\nSum\n+\nop1\nop2\nE\nT\nF\nT\nRPar\nLPar\n(\nstart\nend\nE\nT\nF\nE\nRPar\nLPar\n(\nend\nstart\n1\nE\nSum\n+\nop1\nop2\nE\nT\nF\nF\nE\nRPar\nLPar\n(\nstart\n1\nT\nF\n1\nT\nend\n)\n*\nT\nE\nop1\nop2\nProduct\n*\nT\nF\nF\nE\nSum\n+\nop1\nop2\nE\nT\nF\nRPar\nLPar\n(\nstart\n1\nF\n1\nT\nend\n)\nFigure 34: Main steps in the derivation for (1 + 1) ∗0\n25\nS\nP,S\nN,P,S\nN,P\nN\nFigure 35: Type hierarchy for the expression grammar\n*\nN\n&\n0\nFigure 36: The rule for zero\n*\nSum\n+\nop1\nop2\nS\nS\n* &\nN,P\nFigure 37: The rule for +\nSum\n+\nop1\nop2\nN,P,S\n0\nN,P,S\nS\nN,P,S\n0\nN,P\nS\nSum\n+\nop1\nop2\nN,P,S\n0\nN,P\n0\nSum\n+\nop1\nop2\nS\nN,P\nS\nSum\n+\nop1\nop2\nS\nN,P\n0\n0\nFigure 38: The derivation for 0 + 0+\n26\nWhen the second occurrence of + is encountered, only one of the ancestors of the\nprevious lexeme has a type which contains S.\n4.2.3\nProduct\nThe rule for the operator ∗exactly parallels the rule for the sum operator. It requires\nthe anchor type to contain the atom P, assigns the type P,S to the addendum root,\nand assigns the type N to the right operand. It is represented on Figure 39.\nop1\nop2\n*\nProduct\n*\nP\nP,S\n* &\nN\nFigure 39: The rule for the product operator\nThe fact that the root node of the addendum is a P,S rather than an S gives priority\nto multiplication over addition. Figure 40 outlines the derivations for 1 + 1 and 0 ∗1.\nN,P,S\nS\nSum\n+\nop1\nop2\nN,P,S\nN,P\n*\n1\n1\nN,P,S\nop1\nop2\nN,P,S\n*\n1\nProduct\n*\n0\nN\nP,S\nFigure 40: Derivation outlines for 1 + 1 and 0 ∗1\nIf the current lexeme after parsing 1 + 1 is ∗, only the closest ancestor of the\nprevious lexeme will have a matching type. On the other hand, if the current lexeme\nafter parsing 0∗1 is a plus, only the root of the parse graph will have a matching type.\n4.2.4\nParentheses\nThe rule for the left parenthesis is given on Figure 41.\nOnce it has applied, only an insertion at the linear successor of the left parenthesis\nis possible (due to Principle 2).\nThis could be the insertion of a number or a left\nparenthesis. So, the supertype N as phrase head would also work, but no particular\nconstraint is necessary here, which is why the most permissive type is indicated. The\nsame remark holds for the initial context. In fact, the type N,P,S makes the grammar\nmore open to future evolutions, but could be replaced by N without impairing its\noperation.\n27\nRPar\nLPar\n(\nstart\nend\nN\n* &\nN,P,S\nFigure 41: The rule for the left parenthesis\n5\nDutch Cross-Serial Dependencies\nIn Dutch, a few conjunctions, such as dat (that) or omdat (because), introduce a clause\nin which the verb comes after its object NP. Now, with perceptual and causative verbs,\ni.e. verbs which syntactically take both an NP object and an inﬁnitive clause object, a\ndouble order prevails: all object NPs occur before the ﬁnite verb, in order of increasing\nnesting level, as in German for instance; and inﬁnitive verbs occur after the ﬁnite verb\nin order of increasing nesting level, which reﬂects a right-branching construction for\nclausal objects, quite unlike German, in which the verbs occur in order of decreasing\nnesting level, thus inviting a recursive analysis.\nIn several papers on DCSDs, examples revolve around hippopotami. The example\ngiven below, taken from [Ren94], does not depart from this tradition.\nThe indices\nrelate each verb to its preceding NP object.\n...\ndat ik Henk1 haar2 de\nnijlpaarden3\nzag1 helpen2 voeren3\n...\nthat I Henk1\nher2 the hippopotamus3 saw1 help2\nfeed3\n“that I saw Henk help her feed the hippopotamus”\nThe analysis that will be attempted is based on the principle that, when a verb in a\ndat-clause has an NP object and a clause object, the NP object is constrained to occur\nbefore the verb, and the clause object is constrained to occur immediately after it.\nIn itself, however, the existence of diﬀerent positional requirements on the NP object\nand the clausal indirect object would not seem to require a special syntactic device\nwhen the ditransitive verb governs an ordinary transitive verb. Thus, in the light of\nthe principle just outlined, one could expect the following incorrect construction to\noccur.\n* ... dat ik haar zag de nilparden voeren.\nIn fact, there seems to be an additional constraint that amounts to forbidding any\nobject in a dat-clause to occur before the ﬁnite verb. So the correct construction is\ninstead as follows.\n... dat ik haar1 de nilparden2 zag1 voeren2.\nTo recapitulate, the constraints at work to produce this construction are the fol-\nlowing.\n1. The NP object of any verb in a dat-clause or a clause nested therein occurs before\nit to form an ’inverted’ verb phrase.\n28\n2. The clausal indirect object of a perceptual or causative verb, which is an inﬁnitive\nclause with a subject gap, occurs after it.\n3. In a dat-clause, no direct NP object, whatever the nesting level at which it occurs,\noccurs before the ﬁnite verb.\nNow, the conjunction of the third constraint with the existence of a double order on\nthe construction of perceptual and causative verbs in dat-clauses produces a rift in\nthe construction, as a result of which a ditransitive verb may be separated from its\npreceding direct object by a theoretically arbitrary distance. In order to govern two\ncomplements across a rift, a verb, in the general framework adopted here, is in fact\nrealized as two graph nodes, one of which is a verb gap. To see that, the representation\nbelow shows the phenomenon with one level of nesting. Verb gaps are represented by\nsquare boxes.\n... dat ik haar ✷1 de nijlpaarden ✷2 zag1 voeren2\nThis representation postulates that haar and de nijlpaarden occur in inverted VPs\nheaded by verb gaps and that the verb gaps are coindexed with lexical verbs occurring\nin a right-branching construction governed by the ﬁnite verb.\nIf extended one additional nesting level, the representation that is obtained is the\nfollowing.\n... dat ik Henk ✷1 haar ✷2 de nijlpaarden ✷3 zag1 helpen2 voeren3\nFurthermore, each non-ﬁnite lexical verb heads a clause whose subject is a gap.\nEach such NP gap is coindexed with the NP object of the superordinate verb. The\ncoreference pattern for NPs is the following.\n... dat ik Henk1 haar2 de nijlpaarden zag ✷1 helpen ✷2 voeren\nIn order to capture these hypotheses as GIG rules, one will have to consider three\ncases: (i) a dat-clause headed by a perception or causative verb, (ii) a non-ﬁnite clause\nwith an inverted VP whose head is a perception or causative verb, and (iii) a non-ﬁnite\nclause with an inverted VP whose head is a monotransitive verb.\n5.1\nComplex dat-clause\nWhen the conjunction dat is encountered, one generally has no way of guessing that\nits verb will impose a cross-serial construction. So the rule on Figure 42 will be used\nonly on backtracking after realizing that no rule can integrate the second object NP\nhaar if the verb that is expected is an ordinary transitive verb.\nThe inverted verb phrase (split-inv-VP) that heads the sentence is split into an\ninstance that governs the direct object (inv-VP) and an instance that governs the\nclausal indirect object (iObj-VP). Since neither of these half-instances qualiﬁes as a\nhead, an empty coordination conjunction is postulated to relate them. This empty\nconjunction represents the barrier between the left-branching construction for NPs\nand the right-branching construction for verbs.\nGiven our example, the lexemes whose occurrence is anticipated by this rule are\n(i) the subject of the split-inv-VP, namely ik, (ii) the direct object of its inv-VP\n29\n*\ndat−Cl\ninv−S\ndat−Cl\ndet\ndat−Cnj\ndat\nsplit−inv−VP\nsubj\nNP\nCnj\ninv−VP\nop1\nop2\niObj−VP\nV\ncomplex−V\nobj\nNP\niObj\nnonfin−S\nnonfin−V\nsubj\nNP\nFigure 42: The rule for a dat-clause with a complex ditransitive\ninstance, namely Henk, (iii) the complex-V, namely zag, and (iv) the verb of its clausal\nobject, namely helpen, which is labelled as nonﬁn-V.\nThe labels given to nodes here are more precise than required by the operation of\nthe rules. But this precision hopefully helps legibility.\n5.2\nComplex nonﬁnite clause\nJust as in the case of the previous rule, the decision to trigger the rule for an embedded\ncomplex verb cannot be taken the ﬁrst time its object is encountered, but only after\nbacktracking on the following NP.\nThe rule represented on Figure 43 is a double interpolation which acts simultane-\nously on both instances of a split VP. A relation of coordination is supposed between\nthe inverted verb phrases on the left (top interpolation), for there is no clause hierar-\nchy among them. The use of double interpolation on two graph areas connected by a\ncoreference link allows these areas to lie at an arbitrary distance from each other.\nSupposing the lexemes ik and Henk were inserted into the output of the rule rep-\nresented on Figure 42, the eﬀect of the last rule would be to produce the graph on\nFigure 44.\n5.3\nSimple nonﬁnite clause\nWith a simple transitive verb, the verb gap that is created is simply coreferenced to\nits counterpart in the context; so the bottom interpolation does not create any node.\nThe presence of an article in the context speciﬁed on Figure 45 announces a transitive\nverb; and so no rollbacking need be involved to trigger the rule that is represented.\n30\ninv−VP\nV\ninv−VP\nCnj\ninv−VP\nop1\nop2\ninv−VP\n*\n*\nV\nobj\nNP\nhaar\ncomplex−V\niObj\nnonfin−S\nnonfin−V\niObj\nnonfin−S\nnonfin−V\nsubj\nNP\n* complex−nonfin−V\nFigure 43: The rule for a nested verb of the helpen type\nThis rule adds an NP node to the left of the “syntactic rift”; this node is the object\nof an inv-VP containing an anticipatory verb gap coindexed with the second context\nanchor, which itself anticipates the occurrence of the verb voeren to the right of the\nrift. In short, the top interpolation adds a direct object, and the bottom interpolation\nconnects it to the cascade of clausal objects after zag.\n6\nConclusion\nThis report has deﬁned a form of grammar, called Graph Interpolation Grammar,\nwhich produces parse graphs by simple derivation from an initial graph consisting of\none node. Since derivation integrates input lexemes left-to-right, a GIG derivation is\nan incremental parse of an input sentence.\nThe basic operation, graph interpolation, which combines path substitution and\ndisjoint graph union, has been described at some length. Two classic problems, one\nfrom the ﬁeld of parsing (arithmetic expressions), and one from the ﬁeld of linguis-\ntics (Dutch cross-serial dependencies), have been selected to illustrate problem-solving\nthrough GIG design.\nThe essential, hopefully, has been stated, but much remains to be done to improve\nthe formalism and explore its potentialities. Areas for further research are presented\nin Section 6.2.\nOn the other hand, the GIG model shares intuitions with several other syntactic\nformalisms, which are brieﬂy reviewed in Section 6.1.\n31\ninv−S\ndat−Cl\ndet\ndat−Cnj\ndat\nsplit−inv−VP\nsubj\nNP\nCnj\ninv−VP\nop1\nop2\niObj−VP\nV\ncomplex−V\nobj\nNP\niObj\nnonfin−S\nsubj\nNP\nik\nCnj\ninv−VP\nop1\nop2\ninv−VP\nV\nobj\nNP\nhaar\nHenk\niObj\nnonfin−S\nnonfin−V\nsubj\nNP\ncomplex−nonfin−V\nFigure 44: The output of the rule on Figure 43\n32\ninv−VP\nV\ninv−VP\nCnj\ninv−VP\nop1\nop2\ninv−VP\n*\n*\nV\nobj\nNP\ncomplex−V\niObj\nnonfin−S\nnonfin−V\n* nonfin−V\nN\nDet\ndet\nde\nFigure 45: The rule for a nested monotransitive verb\n6.1\nRelated works\nGraph interpolation can be viewed as an extension of tree adjunction to parse graphs.\nAnd, indeed, TAGs [JLT75], by introducing a 2-dimensional formalism into computa-\ntional linguistics, have made a decisive step towards designing a syntactic theory that\nis both computationally tractable and linguistically realistic. In this respect, it is an\nobligatory reference for any syntactic theory intent on satisfying these criteria.\nThe basic intuition in GIGs, however, that of incrementally connecting lexemes by\nlooking up their combinatory properties, can be found in link grammars [ST95]. Link\ngrammars are mathematically interesting, but do not easily interface with a semantic\ncomponent.\nCategorial grammars [Ste86, Ste88] have a similar rationale. They use binary com-\nbinators on function types that represent grammatical categories. The parse structures\ngenerated, which are binary trees reﬂecting the order of combinator application, can\nbe somewhat counterintuitive from a semantic point of view.\nIn Lexical Functional Grammars [Bre85], grammatical functions are loosely coupled\nwith phrase structure, which seems to be just the opposite of what is done in a GIG,\nin which functional edges are part of the phrase structure.\nNonetheless, these two\napproaches share the concern of bringing out a functional structure, even if much of\nwhat enters into an f-structure (i.e. a functional structure) in LFG is to be addressed\nby the semantic component —a topic for further research— in GIG.\n6.2\nFurther research\n33\n6.2.1\nLinguistic phenomena\nImportant categories of phenomena to analyze include ellipsis, notably under coordi-\nnation, and word scrambling.\nFurthermore, the discussion of free word order in this report has hovered at an\nuncomfortable level of abstraction. It is necessary to return to this issue with concrete\nexamples.\nOn the other hand, the interface with a scanner and morphological analyzer has\nyet to be stated explicitly. Phenomena to account for include idioms, phrasal lexical\nitems, agglutinative morphology, and scanning ambiguities.\n6.2.2\nPsycholinguistic considerations\nThe main natural feature embodied in Graph Interpolation Grammars is incremen-\ntal left-to-right parsing, which is expected to provide a basis for incremental semantic\nevaluation of discourse, a feature in favour of which there seems to be conclusive ex-\nperimental evidence.\nProcessing time\nThe formalism would be particularly satisfactory as a model of\nhuman parsing if the computational complexity it predicts correlated with observed\nprocessing time in humans. What is mainly at stake here is the plausibility of the\nbacktracking mechanism that has been described.\nGarden-path sentences\nGIG parsing involves backtracking for sentences that hu-\nmans do not identify as garden-path sentences. A hypothesis to test against further\nevidence is that the human analyzer is garden-pathed only when semantic interpreta-\ntion has to be revised, not when purely structural adjustments are taking place. A\nthorough discussion of this question requires that a working model be proposed for the\nsemantic component.\nError tolerance\nThe capacity to pick up meaning from ill-formed utterances is a\ncapability of the human language processor that does not seem beyond the reach of\nGIG-based modelling. The model could involve some form of partial match of context\npatterns to resort to when the parser is stymied.\nEvolutionary aspect\nLanguages undergo idiolectal variations and historical changes.\nA constant adjustment of rules takes place when language is used. If GIGs were en-\ndowed with the error recovery device delineated in the previous paragraph, a model of\nevolution could then be worked out to promote heavily activated near-matches to full\nmatches, possibly at the detriment of competing weakly activated full matches, which\ncould disappear in the process.\n6.2.3\nMathematical properties\nExpressive power\nGIGs are at least as expressive as TAGs, for graph interpolation\ncan be used to express tree adjunction or tree substitution.\nThis means that the\nformalism is at least mildly context-sensitive.\nFurthermore, multiple interpolation\nadds expressive power comparable to that found in Multi-Component TAGs [KJ87].\n34\nGIGs do seem to provide the “right” amount of context-sensitiveness, but this remains\nto be precisely quantiﬁed.\nComplexity\nThe backtracking mechanism induces a worst-case exponential com-\nplexity. On realistic grammars, however, both the base of the exponential —i.e. the\nnumber of lexemes past which it makes sense to backtrack— and the exponent —i.e. the\nnumber of competing rules for a given lexeme— seem to have very low upper bounds.\nWhat is a “realistic” grammar could be characterized precisely by determining such\nbounds by collecting linguistic facts. Beside quantiﬁed bounds, it is to be thought that\nsome barrier eﬀect makes it pointless to backtrack beyond certain syntactic barriers.\nOn the other hand, the backtracking mechanism itself could be reconsidered to reduce\nits computational complexity, for example by allowing backtracking to skip back to\na privileged place either through links built while parsing or using a form of pattern\nmatching.\nAutomatic generation of GIG rules\nSection 4 suggests that a Graph Interpo-\nlation Grammar could be automatically derived from a Context Free Grammar, mostly\nfor the purpose of using existing LR grammars.\nThe feasibility of converting automatically from other formalisms, and particularly\nLexical Functional Grammars, seems worth exploring as well.\nLinearized syntax for rules\nFrom a practical point of view, however, GIGs are\nnot to be seen as a low-level formalism to be generated automatically from higher-\nlevel formalisms. But, for GIG writing to be practical, what could be needed is (i) a\nlinearized syntax to type rules quickly rather than draw graphs, and (ii) a WYSIWYG\ntool that showed incrementally the graphic aspect of what is being typed. (This tool\ncould of course be implemented on top of a GIG engine.)\nA linearized syntax could involve a frame-like notation in which an augmented parse\ngraph were a set of slots with a phrases slot that pointed (via slot values) to phrases\nwith an optional pre-deﬁned parent slot and user-deﬁned functional slots.\nFor example, a linearized version of the addendum in Figure 25 could look like the\nframe expression on Figure 46. (Predeﬁned slot names are boldfaced. Slot references\nare slash-separated paths starting at the parent of the slot being deﬁned. A double\ndot (..) moves up one level in the slot hierarchy.)\nOn the other hand, many grammar designers would probably not feel frustrated by\nthe lack of a linear syntax given an eﬃcient graphic tool.\nReferences\n[Bre85] Joan Bresnan.\nThe mental representation of grammatical relations.\nMIT\nPress series on cognitive theory and mental representation. MIT Press, 1985.\nCollected articles.\n[JLT75] A. K. Joshi, L. S. Levy, and M. Takahashi. Tree adjunct grammars. Journal\nof Computer and System Sciences, 10:136–63, 1975.\n35\naddendum {\nphrases {\n0 { head N; };\n1 {\nparent ../0/head;\nhead Adj;\nsubj N;\n};\n2 {\nparent ../1/subj;\nhead Gap;\n};\n};\nlexeme {\nparent ../phrases/1/head;\nspelling ”wealthy”;\n};\nanchors {\nsource ../phrases/0;\ndestination source;\n};\ncoreferences {\n0 {\n0 ../../phrases/0/head;\n1 ../../phrases/1/subj;\n};\n};\n}\nFigure 46: A linearized addendum\n36\n[KJ87]\nAnthony Kroch and Aravind K. Joshi.\nAnalyzing extraposition in a tree\nadjoining grammar. In G. Huck and A. Ojeda, editors, Discontinuous con-\nstituents: syntax and semantics. Academic Press, 1987.\n[Ren94] Gerrit Rentier. Dutch cross serial dependencies in HPSG. Computation and\nLanguage archive cmp-lg/9410016, U.S. National Science Foundation, ITK,\nTilburg University, October 1994.\n[ST95]\nDaniel D. K. Sleator and Davy Temperley.\nParsing english with a link\ngrammar. Technical Report CMU-CS-TR-91-126, Carnegie Mellon Univer-\nsity, 1995. Available in the Computation and Language archive under cmp-\nlg/9508004.\n[Ste86]\nMark Steedman.\nCombinatory grammars and human language processing.\nTechnical Report DAI-RP-279, University of Edinburgh, 1986.\n[Ste88]\nMark Steedman. Combinators and grammars. In Richard T. Oehrle, E. Bach,\nand D. Wheeler, editors, Categorial Grammars and Natural Language Struc-\ntures, pages 417–442. Redel, Dordrecht, 1988. paper to the Conference on\nCategorial Grammar, Tucson, AR, June 1985.\nA\nAnchor location procedures\nThe following procedure can be used to update the set of possible linear successors of\nthe current lexeme after an interpolation. It is operational even when several linear\norders are allowed.\nProcedure 1\n1. Based on ordering relations in the addendum, one identiﬁes the set of possible\nlinear successors of the current lexeme, which is here the lexeme just inserted.\n2. The possible linear successors of the previous lexeme, as recorded when it was\ninserted, minus the node that matched the context anchor, are added to the set\nfound in step 1.\nNote that, since the addendum and the context are disjoint, the second step cannot\nadd potential successors already added by the ﬁrst step. This observation is important\nto determine whether rule application remains reversible in the presence of free word\norder. Indeed, undoing a rule with respect to the set of potential successors of the\ncurrent lexeme can be done as follows.\nProcedure 2\n1. Based on ordering relations in the addendum, one identiﬁes a subset of possible\nlinear successors to the current lexeme, i.e. the lexeme on which to backtrack,\nand subtracts this subset from the set of possible successors.\n2. Once the insertion itself has been undone, the context anchor for this insertion\nis added to the set obtained in the previous step, and this set becomes the set of\npotential successors of the previous lexeme.\n37\n",
  "categories": [
    "cmp-lg",
    "cs.CL"
  ],
  "published": "1998-04-02",
  "updated": "1998-04-02"
}